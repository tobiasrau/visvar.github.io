<!DOCTYPE html>
    <html lang="en">
    <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Non-verbal Communication and Joint Attention Between People with and Without Visual Impairments: Deriving Guidelines for Inclusive Conversations in Virtual Realities | VISVAR Research Group, University of Stuttgart</title>
      <link rel="stylesheet" href="../style.css">
      <script src="../script.js"></script>
      <link rel="shortcut icon" href="../img/favicon.png">
      <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
      <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
    </head>
    <body>
      <a class="anchor" name="top"></a>
      <main>
        
<div>
<header>
<div>
<a href="https://visvar.github.io/">
<h1 class="h1desktop"><div>VISVAR</div><div>Research</div><div>Group</div></h1>
<h1 class="h1mobile">VISVAR</h1>
</a>
</div>
<div>
<nav>
<ul>
<li><a href="https://visvar.github.io/#aboutus">about VISVAR</a></li>
<li><a href="https://visvar.github.io/#publications">publications</a></li>
<li class="memberNav"><a href="https://visvar.github.io/#members">members</a></li>
<ul class="memberNav">

<li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>

<li><a href="https://visvar.github.io/members/quynh_quang_ngo.html">Quynh Quang Ngo</a></li>

<li><a href="https://visvar.github.io/members/benjamin_lee.html">Benjamin Lee</a></li>

<li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>

<li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>

<li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>

<li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>

<li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>

<li><a href="https://visvar.github.io/members/markus_wieland.html">Markus Wieland</a></li>

<li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>

<li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>

<li><a href="https://visvar.github.io/members/nina_doerr.html">Nina DÃ¶rr</a></li>

<li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>

<li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>

<li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>

<li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>

<li><a href="https://visvar.github.io/members/tobias_rau.html">Tobias Rau</a></li>

<li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>

</ul>
</ul>
</nav>
</div>
</header>
</div>
        <div>
          <article><a class="anchor" name="publications"></a>
            <h1>Non-verbal Communication and Joint Attention Between People with and Without Visual Impairments: Deriving Guidelines for Inclusive Conversations in Virtual Realities</h1>
            <div class="pubPageContent">
              <img id="imagewieland2022nonverbal" src="../img/wieland2022nonverbal.png"/>
              <div>
                <b>Venue.</b> Lecture Notes in Computer Science (2022) Full Paper
              </div>
              <div>
                <b>Authors.</b> Markus Wieland, Lauren Thevin, Albrecht Schmidt, Tonja Machulla
              </div>
              <div>
                <b>Materials.</b>
                <a href="https://doi.org/10.1007/978-3-031-08648-9_34" target="_blank">website</a>
                <a href="../pdf/wieland2022nonverbal.pdf" target="_blank">PDF</a>
                
                
              </div>
              <div class="abstract"><b>Abstract.</b> With the emergence of mainstream virtual reality (VR) platforms for social interactions, non-verbal communicative cues are increasingly being transmitted into the virtual environment. Since VR is primarily a visual medium, accessible VR solutions are required for people with visual impairments (PVI). However, existing propositions do not take into account social interactions, and therefore PVI are excluded from this type of experience. To address this issue, we conducted semi-structured interviews with eleven participants, seven of whom were PVI and four of whom were partners or close friends without visual impairments, to explore how non-verbal cues and joint attention are used and perceived in everyday social situations and conversations. Our goal was to provide guidelines for inclusive conversations in virtual environments for PVI. Our findings suggest that gaze, head direction, head movements, and facial expressions are important for both groups in conversations but often difficult to identify visually for PVI. From our findings, we provide concrete suggestions for the design of social VR spaces, inclusive to PVI.</div>
              <div class="bibtex"><textarea>@inproceedings{10.1007/978-3-031-08648-9_34,
    title        = {Non-verbal Communication and Joint Attention Between People with and Without Visual Impairments: Deriving Guidelines for Inclusive Conversations in Virtual Realities},
    author       = {Wieland, Markus and Thevin, Lauren and Schmidt, Albrecht and Machulla, Tonja},
    year         = {2022},
    booktitle    = {Computers Helping People with Special Needs},
    publisher    = {Springer International Publishing},
    pages        = {295--304},
    abstract     = {With the emergence of mainstream virtual reality (VR) platforms for social interactions, non-verbal communicative cues are increasingly being transmitted into the virtual environment. Since VR is primarily a visual medium, accessible VR solutions are required for people with visual impairments (PVI). However, existing propositions do not take into account social interactions, and therefore PVI are excluded from this type of experience. To address this issue, we conducted semi-structured interviews with eleven participants, seven of whom were PVI and four of whom were partners or close friends without visual impairments, to explore how non-verbal cues and joint attention are used and perceived in everyday social situations and conversations. Our goal was to provide guidelines for inclusive conversations in virtual environments for PVI. Our findings suggest that gaze, head direction, head movements, and facial expressions are important for both groups in conversations but often difficult to identify visually for PVI. From our findings, we provide concrete suggestions for the design of social VR spaces, inclusive to PVI.}
}
</textarea></div>
              
              
              <img class="qr" src="../qr/wieland2022nonverbal.png"/>
            </div>
          </article>
        </div>
      </main>
    </body>
    </html>