<!DOCTYPE html>
    <html lang="en">
    <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study | VISVAR Research Group, University of Stuttgart</title>
      <link rel="stylesheet" href="../style.css">
      <script src="../script.js"></script>
      <link rel="shortcut icon" href="../img/favicon.png">
      <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
      <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
    </head>
    <body>
      <a class="anchor" name="top"></a>
      <main>
        
<div>
<header>
<div>
<a href="https://visvar.github.io/">
<h1 class="h1desktop"><div>VISVAR</div><div>Research</div><div>Group</div></h1>
<h1 class="h1mobile">VISVAR</h1>
</a>
</div>
<div>
<nav>
<ul>
<li><a href="https://visvar.github.io/#aboutus">about VISVAR</a></li>
<li><a href="https://visvar.github.io/#publications">publications</a></li>
<li class="memberNav"><a href="https://visvar.github.io/#members">members</a></li>
<ul class="memberNav">

<li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>

<li><a href="https://visvar.github.io/members/quynh_quang_ngo.html">Quynh Quang Ngo</a></li>

<li><a href="https://visvar.github.io/members/benjamin_lee.html">Benjamin Lee</a></li>

<li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>

<li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>

<li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>

<li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>

<li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>

<li><a href="https://visvar.github.io/members/markus_wieland.html">Markus Wieland</a></li>

<li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>

<li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>

<li><a href="https://visvar.github.io/members/nina_doerr.html">Nina DÃ¶rr</a></li>

<li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>

<li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>

<li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>

<li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>

<li><a href="https://visvar.github.io/members/tobias_rau.html">Tobias Rau</a></li>

<li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>

</ul>
</ul>
</nav>
</div>
</header>
</div>
        <div>
          <article><a class="anchor" name="publications"></a>
            <h1>Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study</h1>
            <div class="pubPageContent">
              <img id="imagehube2022using" src="../img/hube2022using.png"/>
              <div>
                <b>Venue.</b> CHI (2022) Late-Breaking Work
              </div>
              <div>
                <b>Authors.</b> Natalie Hube,  Kresimir Vidackovic, Michael Sedlmair
              </div>
              <div>
                <b>Materials.</b>
                <a href="https://doi.org/10.1145/3491101.3519822" target="_blank">website</a>
                <a href="../pdf/hube2022using.pdf" target="_blank">PDF</a>
                
                
              </div>
              <div class="abstract"><b>Abstract.</b> Virtual avatars are widely used for collaborating in virtual environments. Yet, often these avatars lack expressiveness to determine a state of mind. Prior work has demonstrated efective usage of determining emotions and animated lip movement through analyzing mere audio tracks of spoken words. To provide this information on a virtual avatar, we created a natural audio data set consisting of 17 audio fles from which we then extracted the underlying emotion and lip movement. To conduct a pilot study, we developed a prototypical system that displays the extracted visual parameters and then maps them on a virtual avatar while playing the corresponding audio fle. We tested the system with 5 participants in two conditions: (i) while seeing the virtual avatar only an audio fle was played. (ii) In addition to the audio fle, the extracted facial visual parameters were displayed on the virtual avatar. Our results suggest the validity of using additional visual parameters in the avatars face as it helps to determine emotions. We conclude with a brief discussion on the outcomes and their implications on future work.</div>
              <div class="bibtex"><textarea>@inproceedings{10.1145/3491101.3519822,
    title        = {Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study},
    author       = {Hube, Natalie and Vidackovic, Kresimir and Sedlmair, Michael},
    year         = {2022},
    booktitle    = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},
    publisher    = {Association for Computing Machinery},
    series       = {CHI EA '22},
    doi          = {10.1145/3491101.3519822},
    url          = {https://doi.org/10.1145/3491101.3519822},
    abstract     = {Virtual avatars are widely used for collaborating in virtual environments. Yet, often these avatars lack expressiveness to determine a state of mind. Prior work has demonstrated effective usage of determining emotions and animated lip movement through analyzing mere audio tracks of spoken words. To provide this information on a virtual avatar, we created a natural audio data set consisting of 17 audio files from which we then extracted the underlying emotion and lip movement. To conduct a pilot study, we developed a prototypical system that displays the extracted visual parameters and then maps them on a virtual avatar while playing the corresponding audio file. We tested the system with 5 participants in two conditions: (i) while seeing the virtual avatar only an audio file was played. (ii) In addition to the audio file, the extracted facial visual parameters were displayed on the virtual avatar. Our results suggest the validity of using additional visual parameters in the avatars' face as it helps to determine emotions. We conclude with a brief discussion on the outcomes and their implications on future work.},
    articleno    = {260},
    numpages     = {7},
    keywords     = {emotion, avatars, lip synchronization, virtual reality}
}
</textarea></div>
              
              
              <img class="qr" src="../qr/hube2022using.png"/>
            </div>
          </article>
        </div>
      </main>
    </body>
    </html>