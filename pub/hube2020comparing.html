<!DOCTYPE html>
    <html lang="en">
    <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion | VISVAR Research Group, University of Stuttgart</title>
      <link rel="stylesheet" href="../style.css">
      <script src="../script.js"></script>
      <link rel="shortcut icon" href="../img/favicon.png">
      <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
      <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
    </head>
    <body>
      <a class="anchor" name="top"></a>
      <main>
        <div>
          
<header>
  <div>
    <a href="https://visvar.github.io/">
      <h1 class="h1desktop"><div>VISVAR</div><div>Research</div><div>Group</div></h1>
      <h1 class="h1mobile">VISVAR</h1>
    </a>
  </div>
  <div>
    <nav>
      <ul>
        <li>
          <a href="https://visvar.github.io/#aboutus">about VISVAR</a>
        </li>
        <li>
          <a href="https://visvar.github.io/#publications">publications</a>
        </li>
        <li class="memberNav">
          <a href="https://visvar.github.io/#members">members</a>
        </li>
        <ul class="memberNav">
          
            <li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>
          
            <li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>
          
            <li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>
          
            <li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>
          
            <li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>
          
            <li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>
          
            <li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>
          
            <li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>
          
            <li><a href="https://visvar.github.io/members/nina_doerr.html">Nina DÃ¶rr</a></li>
          
            <li><a href="https://visvar.github.io/members/quynh_quang_ngo.html">Quynh Quang Ngo</a></li>
          
            <li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>
          
            <li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>
          
            <li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>
          
            <li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>
          
            <li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>
          
        </ul>
      </ul>
    </nav>
  </div>
</header>
        </div>
        <div>
          <article> <a class="anchor" name="publications"></a>
            <h1>Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion</h1>
            <div class="pubPageContent">
              <div class="pubImage">
              <img id="imagehube2020comparing" src="../img/hube2020comparing.png"/>
              </div>
              <div>
              ISMAR (2020) Poster / Short Paper
              </div>
              <h4>Authors</h4>
              <div>
              Natalie Hube, Oliver Lenz, Lars Engeln, Rainer Groh, Michael Sedlmair
              </div>
              <h4>Materials</h4>
              <div>
                <a href="https://doi.org/10.1109/ISMAR-Adjunct51615.2020.00023" target="_blank">website</a>
                <a href="../pdf/hube2020comparing.pdf" target="_blank">PDF</a>
                
                
              </div>
              <h4>Abstract</h4><div class="abstract">We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.</div>
              <h4>BibTex</h4><div class="bibtex"><textarea>@INPROCEEDINGS{9288476,
  author={Hube, Natalie and Lenz, Oliver and Engeln, Lars and Groh, Rainer and Sedlmair, Michael},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion}, 
  year={2020},
  pages={30-35},
  doi={10.1109/ISMAR-Adjunct51615.2020.00023}}</textarea></div>
              
            </div>
          </div>
          </article>
        </div>
      </main>
    </body>
    </html>