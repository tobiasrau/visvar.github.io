<!DOCTYPE html>
    <html lang="en">
    <head>
      <meta charset="UTF-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <title>Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion | VISVAR Research Group, University of Stuttgart</title>
      <link rel="stylesheet" href="../style.css">
      <link rel="shortcut icon" href="../img/favicon.png">
      <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
      <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
    </head>
    <body>
      <a class="anchor" name="top"></a>
      <main>
        
<div>
  <header>
    <div>
      <a href="https://visvar.github.io/">
        <img class="logo" src="img/visvar_logo.svg" />
      </a>
    </div>
    <div>
      <nav>
      <ul>
        <li><a href="https://visvar.github.io/#aboutus">about VISVAR</a></li>
        <li><a href="https://visvar.github.io/#publications">publications</a></li>
        <li><a href="https://visvar.github.io/#members">members</a></li>
        <ul class="memberNav">
          
          <li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>
          
          <li><a href="https://visvar.github.io/members/quynh_quang_ngo.html">Quynh Quang Ngo</a></li>
          
          <li><a href="https://visvar.github.io/members/benjamin_lee.html">Benjamin Lee</a></li>
          
          <li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>
          
          <li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>
          
          <li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>
          
          <li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>
          
          <li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>
          
          <li><a href="https://visvar.github.io/members/markus_wieland.html">Markus Wieland</a></li>
          
          <li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>
          
          <li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>
          
          <li><a href="https://visvar.github.io/members/nina_doerr.html">Nina Doerr</a></li>
          
          <li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>
          
          <li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>
          
          <li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>
          
          <li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>
          
          <li><a href="https://visvar.github.io/members/tobias_rau.html">Tobias Rau</a></li>
          
          <li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>
          
        </ul>
      </ul>
      </nav>
    </div>
  </header>
</div>

        <div>
          <article><a class="anchor" name="publications"></a>
            <h1>Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion</h1>
            <div class="pubPageContent">
              <img id="imagehube2020comparing" src="../img/hube2020comparing.png"/>
              <div>
                <b>Venue.</b> ISMAR (2020) Poster / Short Paper
              </div>
              <div>
                <b>Authors.</b> Natalie Hube, Oliver Lenz, Lars Engeln, Rainer Groh, Michael Sedlmair
              </div>
              <div>
                <b>Materials.</b>
                <a href="https://doi.org/10.1109/ISMAR-Adjunct51615.2020.00023" target="_blank">website</a>
                <a href="../pdf/hube2020comparing.pdf" target="_blank">PDF</a>
                
                
              </div>
              <div class="abstract"><b>Abstract.</b> We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.</div>
              <div class="bibtex"><textarea>@inproceedings{9288476,
    title        = {Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion},
    author       = {Hube, Natalie and Lenz, Oliver and Engeln, Lars and Groh, Rainer and Sedlmair, Michael},
    year         = {2020},
    booktitle    = {2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
    pages        = {30--35},
    doi          = {10.1109/ISMAR-Adjunct51615.2020.00023}
}
</textarea></div>
              
              
              <img class="qr" src="../qr/hube2020comparing.png"/>
            </div>
          </article>
        </div>
      </main>
    </body>
    </html>