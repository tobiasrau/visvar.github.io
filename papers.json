[{"Title":"An Image Quality Dataset with Triplet Comparisons for Multi-dimensional Scaling","Submission Target":"QoMEX","Date":"2024-06-22","Type":"Short Paper","First Author":"Mohsen Jenadeleh","Other Authors":"Frederik L. Dennig, Rene Cutura, Quynh Quang Ngo, Daniel A. Keim, Michael Sedlmair, Dietmar Saupe","Key (e.g. for file names)":"jenadeleh2024an","Publisher URL (official)":"https://doi.org/10.1109/QoMEX61742.2024.10598258","url2":"https://ieeexplore.ieee.org/document/10598258","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In the early days of perceptual image quality research more than 30 years ago, the multidimensionality of distortions in perceptual space was considered important. However, research focused on scalar quality as measured by mean opinion scores. With our work, we intend to revive interest in this relevant area by presenting a first pilot dataset of annotated triplet comparisons for image quality assessment. It contains one source stimulus together with distorted versions derived from 7 distortion types at 12 levels each. Our crowdsourced and curated dataset contains roughly 50,000 responses to 7,000 triplet comparisons. We show that the multidimensional embedding of the dataset poses a challenge for many established triplet embedding algorithms. Finally, we propose a new reconstruction algorithm, dubbed logistic triplet embedding (LTE) with Tikhonov regularization. It shows promising performance. This study helps researchers to create larger datasets and better embedding techniques for multidimensional image quality. The dataset includes images and ratings and can be accessed at https://github.com/jenadeleh/multidimensionalIQA-dataset/tree/main.","bibtex":"@INPROCEEDINGS{10598258,\r\n  author={Jenadeleh, Mohsen and Dennig, Frederik L. and Cutura, Rene and Ngo, Quynh Quang and Keim, Daniel A. and Sedlmair, Michael and Saupe, Dietmar},\r\n  booktitle={2024 16th International Conference on Quality of Multimedia Experience (QoMEX)}, \r\n  title={An Image Quality Dataset with Triplet Comparisons for Multi-dimensional Scaling}, \r\n  year={2024},\r\n  pages={278-281},\r\n  keywords={Image quality;Layout;Reconstruction algorithms;Distortion;Topology;Distortion measurement;Logistics;multidimensional image quality assessment;triplet comparison;image quality dataset},\r\n  doi={10.1109/QoMEX61742.2024.10598258}}\r\n","notes":"","funding":""},{"Title":"NMF-Based Analysis of Mobile Eye-Tracking Data","Submission Target":"ETRA","Date":"2024-06-04","Type":"Workshop Paper","First Author":"Daniel Klötzl","Other Authors":"Tim Krake, Frank Heyen, Michael Becher, Maurice Koch, Daniel Weiskopf, Kuno Kurzhals","Key (e.g. for file names)":"kloetzl2024nmfbased","Publisher URL (official)":"https://doi.org/10.1145/3649902.3653518","url2":"https://dl.acm.org/doi/10.1145/3649902.3653518","PDF URL (public)":"https://arxiv.org/pdf/2404.03417v1","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The depiction of scanpaths from mobile eye-tracking recordings by thumbnails from the stimulus allows the application of visual computing to detect areas of interest in an unsupervised way. We suggest using nonnegative matrix factorization (NMF) to identify such areas in stimuli. For a user-defined integer k, NMF produces an explainable decomposition into k components, each consisting of a spatial representation associated with a temporal indicator. In the context of multiple eye-tracking recordings, this leads to k spatial representations, where the temporal indicator highlights the appearance within recordings. The choice of k provides an opportunity to control the refinement of the decomposition, i.e., the number of areas to detect. We combine our NMF-based approach with visualization techniques to enable an exploratory analysis of multiple recordings. Finally, we demonstrate the usefulness of our approach with mobile eye-tracking data of an art gallery. ","bibtex":"@inproceedings{10.1145/3649902.3653518,\r\nauthor = {Kl\\\"{o}tzl, Daniel and Krake, Tim and Heyen, Frank and Becher, Michael and Koch, Maurice and Weiskopf, Daniel and Kurzhals, Kuno},\r\ntitle = {NMF-Based Analysis of Mobile Eye-Tracking Data},\r\nyear = {2024},\r\nisbn = {9798400706073},\r\npublisher = {ACM},\r\nurl = {https://doi.org/10.1145/3649902.3653518},\r\ndoi = {10.1145/3649902.3653518},\r\nabstract = {The depiction of scanpaths from mobile eye-tracking recordings by thumbnails from the stimulus allows the application of visual computing to detect areas of interest in an unsupervised way. We suggest using nonnegative matrix factorization (NMF) to identify such areas in stimuli. For a user-defined integer k, NMF produces an explainable decomposition into k components, each consisting of a spatial representation associated with a temporal indicator. In the context of multiple eye-tracking recordings, this leads to k spatial representations, where the temporal indicator highlights the appearance within recordings. The choice of k provides an opportunity to control the refinement of the decomposition, i.e., the number of areas to detect. We combine our NMF-based approach with visualization techniques to enable an exploratory analysis of multiple recordings. Finally, we demonstrate the usefulness of our approach with mobile eye-tracking data of an art gallery.},\r\nbooktitle = {Proceedings of the 2024 Symposium on Eye Tracking Research and Applications},\r\narticleno = {76},\r\nnumpages = {9},\r\nkeywords = {Clustering, Eye Tracking, Matrix Factorization, NMF, Visualization},\r\nseries = {ETRA '24}\r\n}","notes":"","funding":""},{"Title":"chARpack: The Chemistry Augmented Reality Package","Submission Target":"JCIM","Date":"2024-05-30","Type":"Full Paper","First Author":"Tobias Rau","Other Authors":"Michael Sedlmair, Andreas Köhn","Key (e.g. for file names)":"rau2024charpack","Publisher URL (official)":"https://doi.org/10.1021/acs.jcim.4c00462","url2":"https://pubs.acs.org/doi/full/10.1021/acs.jcim.4c00462","PDF URL (public)":"https://pubs.acs.org/doi/epdf/10.1021/acs.jcim.4c00462","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Off-loading visualization and interaction into virtual reality (VR) using head-mounted displays (HMDs) has gained considerable popularity in simulation sciences, particularly in chemical modeling. Because of its unique way of soft immersion, augmented reality (AR) HMD technology has even more potential to be integrated into the everyday workflow of computational chemists. In this work, we present our environment to explore the prospects of AR in chemistry and general molecular sciences: The chemistry in Augmented Reality package (chARpack). Besides providing an extensible framework, our software focuses on a seamless transition between a 3D stereoscopic view with true 3D interactions and the traditional desktop PC setup to provide users with the best setup for all tasks in their workflow. Using feedback from domain experts, we discuss our design requirements for this kind of hybrid working environment (AR + PC), regarding input, features, degree of immersion, and collaboration.","bibtex":"@article{doi:10.1021/acs.jcim.4c00462,\r\nauthor = {Rau, Tobias and Sedlmair, Michael and Köhn, Andreas},\r\ntitle = {chARpack: The Chemistry Augmented Reality Package},\r\njournal = {Journal of Chemical Information and Modeling},\r\nvolume = {0},\r\nnumber = {0},\r\npages = {null},\r\nyear = {0},\r\ndoi = {10.1021/acs.jcim.4c00462},\r\nURL = {https://doi.org/10.1021/acs.jcim.4c00462}\r\n}\r\n","notes":"","funding":""},{"Title":"Visual Highlighting for Situated Brushing and Linking","Submission Target":"EuroVis","Date":"2024-05-27","Type":"Full Paper","First Author":"Nina Doerr","Other Authors":"Benjamin Lee, Katarina Baricova, Dieter Schmalstieg, Michael Sedlmair","Key (e.g. for file names)":"doerr2024visual","Publisher URL (official)":"https://doi.org/10.1111/cgf.15105","url2":"https://onlinelibrary.wiley.com/doi/10.1111/cgf.15105","PDF URL (public)":"https://arxiv.org/pdf/2403.15321","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Brushing and linking is widely used for visual analytics in desktop environments. However, using this approach to link many data items between situated (e.g., a virtual screen with data) and embedded views (e.g., highlighted objects in the physical environment) is largely unexplored. To this end, we study the effectiveness of visual highlighting techniques in helping users identify and link physical referents to brushed data marks in a situated scatterplot. In an exploratory virtual reality user study (N=20), we evaluated four highlighting techniques under different physical layouts and tasks. We discuss the effectiveness of these techniques, as well as implications for the design of brushing and linking operations in situated analytics. ","bibtex":"@article{https://doi.org/10.1111/cgf.15105,\r\nauthor = {Doerr, Nina and Lee, Benjamin and Baricova, Katarina and Schmalstieg, Dieter and Sedlmair, Michael},\r\ntitle = {Visual Highlighting for Situated Brushing and Linking},\r\njournal = {Computer Graphics Forum},\r\nvolume = {43},\r\nnumber = {3},\r\npages = {e15105},\r\nkeywords = {CCS Concepts, • Human-centered computing → Empirical studies in visualization, Empirical studies in HCI, Information visualization},\r\ndoi = {https://doi.org/10.1111/cgf.15105},\r\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.15105},\r\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.15105},\r\nyear = {2024}\r\n}\r\n","notes":"","funding":""},{"Title":"ChoreoVis: Planning and Assessing Formations in Dance Choreographies","Submission Target":"EuroVis","Date":"2024-05-27","Type":"Full Paper","First Author":"Samuel Beck","Other Authors":"Nina Doerr, Kuno Kurzhals, Alexander Riedlinger, Fabian Schmierer, Michael Sedlmair, Steffen Koch","Key (e.g. for file names)":"beck2024choreovis","Publisher URL (official)":"https://doi.org/10.1111/cgf.15104","url2":"https://onlinelibrary.wiley.com/doi/10.1111/cgf.15104","PDF URL (public)":"https://arxiv.org/pdf/2404.04100","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Sports visualization has developed into an active research field over the last decades. Many approaches focus on analyzing movement data recorded from unstructured situations, such as soccer. For the analysis of choreographed activities like formation dancing, however, the goal differs, as dancers follow specific formations in coordinated movement trajectories. To date, little work exists on how visual analytics methods can support such choreographed performances. To fill this gap, we introduce a new visual approach for planning and assessing dance choreographies. In terms of planning choreographies, we contribute a web application with interactive authoring tools and views for the dancers' positions and orientations, movement trajectories, poses, dance floor utilization, and movement distances. For assessing dancers' real-world movement trajectories, extracted by manual bounding box annotations, we developed a timeline showing aggregated trajectory deviations and a dance floor view for detailed trajectory comparison. Our approach was developed and evaluated in collaboration with dance instructors, showing that introducing visual analytics into this domain promises improvements in training efficiency for the future. ","bibtex":"@article{https://doi.org/10.1111/cgf.15104,\r\nauthor = {Beck, Samuel and Doerr, Nina and Kurzhals, Kuno and Riedlinger, Alexander and Schmierer, Fabian and Sedlmair, Michael and Koch, Steffen},\r\ntitle = {ChoreoVis: Planning and Assessing Formations in Dance Choreographies},\r\njournal = {Computer Graphics Forum},\r\nvolume = {43},\r\nnumber = {3},\r\npages = {e15104},\r\nkeywords = {CCS Concepts, • Human-centered computing → Visual analytics},\r\ndoi = {https://doi.org/10.1111/cgf.15104},\r\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.15104},\r\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.15104},\r\nyear = {2024}\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Design Space of Visual Feedforward And Corrective Feedback in XR-Based Motion Guidance Systems","Submission Target":"CHI","Date":"2024-05-11","Type":"Full Paper","First Author":"Xingyao Yu","Other Authors":"Benjamin Lee, Michael Sedlmair","Key (e.g. for file names)":"yu2024design","Publisher URL (official)":"https://doi.org/10.1145/3613904.3642143","url2":"https://dl.acm.org/doi/10.1145/3613904.3642143","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3613904.3642143","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Extended reality (XR) technologies are highly suited in assisting individuals in learning motor skills and movements – referred to as motion guidance. In motion guidance, the \"feedforward\" provides instructional cues of the motions that are to be performed, whereas the \"feedback\" provides cues which help correct mistakes and minimize errors. Designing synergistic feedforward and feedback is vital to providing an effective learning experience, but this interplay between the two has not yet been adequately explored. Based on a survey of the literature, we propose design space for both motion feedforward and corrective feedback in XR, and describe the interaction effects between them. We identify common design approaches of XR-based motion guidance found in our literature corpus, and discuss them through the lens of our design dimensions. We then discuss additional contextual factors and considerations that influence this design, together with future research opportunities for motion guidance in XR.","bibtex":"@inproceedings{10.1145/3613904.3642143,\r\nauthor = {Yu, Xingyao and Lee, Benjamin and Sedlmair, Michael},\r\ntitle = {Design Space of Visual Feedforward And Corrective Feedback in XR-Based Motion Guidance Systems},\r\nyear = {2024},\r\nisbn = {9798400703300},\r\npublisher = {ACM},R226\r\nurl = {https://doi.org/10.1145/3613904.3642143},\r\ndoi = {10.1145/3613904.3642143},\r\nabstract = {Extended reality (XR) technologies are highly suited in assisting individuals in learning motor skills and movements—referred to as motion guidance. In motion guidance, the “feedforward’’ provides instructional cues of the motions that are to be performed, whereas the “feedback’’ provides cues which help correct mistakes and minimize errors. Designing synergistic feedforward and feedback is vital to providing an effective learning experience, but this interplay between the two has not yet been adequately explored. Based on a survey of the literature, we propose design space for both motion feedforward and corrective feedback in XR, and describe the interaction effects between them. We identify common design approaches of XR-based motion guidance found in our literature corpus, and discuss them through the lens of our design dimensions. We then discuss additional contextual factors and considerations that influence this design, together with future research opportunities for motion guidance in XR.},\r\nbooktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},\r\narticleno = {723},\r\nnumpages = {15},\r\nkeywords = {Design Space, Extended Reality, Motion Guidance, Visualization},\r\nseries = {CHI '24}\r\n}","notes":"","funding":""},{"Title":"Sitting Posture Recognition and Feedback: A Literature Review","Submission Target":"CHI","Date":"2024-05-11","Type":"Full Paper","First Author":"Christian Krauter","Other Authors":"Katrin Angerbauer, Aimée Sousa Calepso, Alexander Achberger, Sven Mayer, Michael Sedlmair","Key (e.g. for file names)":"krauter2024sitting","Publisher URL (official)":"https://doi.org/10.1145/3613904.3642657","url2":"https://dl.acm.org/doi/10.1145/3613904.3642657","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3613904.3642657","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Extensive sitting is unhealthy; thus, countermeasures are needed to react to the ongoing trend toward more prolonged sitting. A variety of studies and guidelines have long addressed the question of how we can improve our sitting habits. Nevertheless, sitting time is still increasing. Here, smart devices can provide a general overview of sitting habits for more nuanced feedback on the user's sitting posture. Based on a literature review (N=223), including publications from engineering, computer science, medical sciences, electronics, and more, our work guides developers of posture systems. There is a large variety of approaches, with pressure-sensing hardware and visual feedback being the most prominent. We found factors like environment, cost, privacy concerns, portability, and accuracy important for deciding hardware and feedback types. Further, one should consider the user's capabilities, preferences, and tasks. Regarding user studies for sitting posture feedback, there is a need for better comparability and for investigating long-term effects.","bibtex":"@inproceedings{10.1145/3613904.3642657,\r\nauthor = {Krauter, Christian and Angerbauer, Katrin and Sousa Calepso, Aim\\'{e}e and Achberger, Alexander and Mayer, Sven and Sedlmair, Michael},\r\ntitle = {Sitting Posture Recognition and Feedback: A Literature Review},\r\nyear = {2024},\r\nisbn = {9798400703300},\r\npublisher = {ACM},\r\nurl = {https://doi.org/10.1145/3613904.3642657},\r\ndoi = {10.1145/3613904.3642657},\r\nabstract = {Extensive sitting is unhealthy; thus, countermeasures are needed to react to the ongoing trend toward more prolonged sitting. A variety of studies and guidelines have long addressed the question of how we can improve our sitting habits. Nevertheless, sitting time is still increasing. Here, smart devices can provide a general overview of sitting habits for more nuanced feedback on the user’s sitting posture. Based on a literature review (N=223), including publications from engineering, computer science, medical sciences, electronics, and more, our work guides developers of posture systems. There is a large variety of approaches, with pressure-sensing hardware and visual feedback being the most prominent. We found factors like environment, cost, privacy concerns, portability, and accuracy important for deciding hardware and feedback types. Further, one should consider the user’s capabilities, preferences, and tasks. Regarding user studies for sitting posture feedback, there is a need for better comparability and for investigating long-term effects.},\r\nbooktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},\r\narticleno = {943},\r\nnumpages = {20},\r\nkeywords = {Literature review, chair, posture, sitting},\r\nseries = {CHI '24}\r\n}","notes":"","funding":""},{"Title":"A Systematic Review of Ability-diverse Collaboration through Ability-based Lens in HCI","Submission Target":"CHI","Date":"2024-05-11","Type":"Full Paper","First Author":"Lan Xiao","Other Authors":"Maryam Bandukda, Katrin Angerbauer, Weiyue Lin, Tigmanshu Bhatnagar, Michael Sedlmair, Catherine Holloway","Key (e.g. for file names)":"xiao2024systematic","Publisher URL (official)":"https://doi.org/10.1145/3613904.3641930","url2":"https://dl.acm.org/doi/10.1145/3613904.3641930","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3613904.3641930","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In a world where diversity is increasingly recognised and celebrated, it is important for HCI to embrace the evolving methods and theories for technologies to reflect the diversity of its users and be ability-centric. Interdependence Theory, an example of this evolution, highlights the interpersonal relationships between humans and technologies and how technologies should be designed to meet shared goals and outcomes for people, regardless of their abilities. This necessitates a contemporary understanding of \"ability-diverse collaboration,\" which motivated this review. In this review, we offer an analysis of 117 papers sourced from the ACM Digital Library spanning the last two decades. We contribute (1) a unified taxonomy and the Ability-Diverse Collaboration Framework, (2) a reflective discussion and mapping of the current design space, and (3) future research opportunities and challenges. Finally, we have released our data and analysis tool to encourage the HCI research community to contribute to this ongoing effort.","bibtex":"@inproceedings{10.1145/3613904.3641930,\r\nauthor = {Xiao, Lan and Bandukda, Maryam and Angerbauer, Katrin and Lin, Weiyue and Bhatnagar, Tigmanshu and Sedlmair, Michael and Holloway, Catherine},\r\ntitle = {A Systematic Review of Ability-diverse Collaboration through Ability-based Lens in HCI},\r\nyear = {2024},\r\nisbn = {9798400703300},\r\npublisher = {ACM},\r\nurl = {https://doi.org/10.1145/3613904.3641930},\r\ndoi = {10.1145/3613904.3641930},\r\nabstract = {In a world where diversity is increasingly recognised and celebrated, it is important for HCI to embrace the evolving methods and theories for technologies to reflect the diversity of its users and be ability-centric. Interdependence Theory, an example of this evolution, highlights the interpersonal relationships between humans and technologies and how technologies should be designed to meet shared goals and outcomes for people, regardless of their abilities. This necessitates a contemporary understanding of \"ability-diverse collaboration,\" which motivated this review. In this review, we offer an analysis of 117 papers sourced from the ACM Digital Library spanning the last two decades. We contribute (1) a unified taxonomy and the Ability-Diverse Collaboration Framework, (2) a reflective discussion and mapping of the current design space, and (3) future research opportunities and challenges. Finally, we have released our data and analysis tool to encourage the HCI research community to contribute to this ongoing effort.},\r\nbooktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},\r\narticleno = {961},\r\nnumpages = {21},\r\nkeywords = {Interdependence, ability-based method, accessibility, collaboration},\r\nseries = {CHI '24}\r\n}","notes":"","funding":""},{"Title":"Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks","Submission Target":"VR","Date":"2024-04-15","Type":"Full Paper","First Author":"Nelusa Pathmanathan","Other Authors":"Tobias Rau, Xiliu Yang, Aimée Sousa Calepso, Felix Amtsberg, Achim Menges, Michael Sedlmair, Kuno Kurzhals","Key (e.g. for file names)":"pathmanathan2024eyes","Publisher URL (official)":"https://doi.org/10.1109/VR58804.2024.00098","url2":"https://ieeexplore.ieee.org/abstract/document/10494083","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The use of augmented reality technology to support humans with situated visualization in complex tasks such as navigation or assembly has gained increasing importance in research and industrial applications. One important line of research regards supporting and understanding collaborative tasks. Analyzing collaboration patterns is usually done by conducting observations and interviews. To expand these methods, we argue that eye tracking can be used to extract further insights and quantify behavior. To this end, we contribute a study that uses eye tracking to investigate participant strategies for solving collaborative sorting and assembly tasks. We compare participants’ visual attention during situated instructions in AR and traditional paper-based instructions as a baseline. By investigating the performance and gaze behavior of the participants, different strategies for solving the provided tasks are revealed. Our results show that with situated visualization, participants focus more on task-relevant areas and require less discussion between collaboration partners to solve the task at hand.","bibtex":"@INPROCEEDINGS {10494083,\r\nauthor = {N. Pathmanathan and T. Rau and X. Yang and A. Calepso and F. Amtsberg and A. Menges and M. Sedlmair and K. Kurzhals},\r\nbooktitle = {2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},\r\ntitle = {Eyes on the Task: Gaze Analysis of Situated Visualization for Collaborative Tasks},\r\nyear = {2024},\r\npages = {785-795},\r\nabstract = {The use of augmented reality technology to support humans with situated visualization in complex tasks such as navigation or assembly has gained increasing importance in research and industrial applications. One important line of research regards supporting and understanding collaborative tasks. Analyzing collaboration patterns is usually done by conducting observations and interviews. To expand these methods, we argue that eye tracking can be used to extract further insights and quantify behavior. To this end, we contribute a study that uses eye tracking to investigate participant strategies for solving collaborative sorting and assembly tasks. We compare participants’ visual attention during situated instructions in AR and traditional paper-based instructions as a baseline. By investigating the performance and gaze behavior of the participants, different strategies for solving the provided tasks are revealed. Our results show that with situated visualization, participants focus more on task-relevant areas and require less discussion between collaboration partners to solve the task at hand.},\r\nkeywords = {visualization;three-dimensional displays;collaboration;gaze tracking;user interfaces;user experience;task analysis},\r\ndoi = {10.1109/VR58804.2024.00098},\r\nurl = {https://doi.ieeecomputersociety.org/10.1109/VR58804.2024.00098},\r\npublisher = {IEEE Computer Society},\r\naddress = {Los Alamitos, CA, USA},\r\nmonth = {mar}\r\n}","notes":"","funding":""},{"Title":"Putting Our Minds Together: Iterative Exploration for Collaborative Mind Mapping","Submission Target":"AHs","Date":"2024-04-04","Type":"","First Author":"Ying Yang","Other Authors":"Tim Dwyer, Zachari Swiecki, Benjamin Lee, Michael Wybrow, Maxime Cordeil, Teresa Wulandari, Bruce H. Thomas, Mark Billinghurst","Key (e.g. for file names)":"yang2024putting","Publisher URL (official)":"https://doi.org/10.1145/3652920.3653043","url2":"https://arxiv.org/abs/2403.13517","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We delineate the development of a mind-mapping system designed concurrently for both VR and desktop platforms. Employing an iterative methodology with groups of users, we systematically examined and improved various facets of our system, including interactions, communication mechanisms and gamification elements, to streamline the mind-mapping process while augmenting situational awareness and promoting active engagement among collaborators. We also report our observational findings on these facets from this iterative design process. ","bibtex":"@inproceedings{10.1145/3652920.3653043,\r\nauthor = {Yang, Ying and Dwyer, Tim and Swiecki, Zachari and Lee, Benjamin and Wybrow, Michael and Cordeil, Maxime and Wulandari, Teresa and Thomas, Bruce H and Billinghurst, Mark},\r\ntitle = {Putting Our Minds Together: Iterative Exploration for Collaborative Mind Mapping},\r\nyear = {2024},\r\nisbn = {9798400709807},\r\npublisher = {ACM},\r\nurl = {https://doi.org/10.1145/3652920.3653043},\r\ndoi = {10.1145/3652920.3653043},\r\nabstract = {We delineate the development of a mind-mapping system designed concurrently for both VR and desktop platforms. Employing an iterative methodology with groups of users, we systematically examined and improved various facets of our system, including interactions, communication mechanisms and gamification elements, to streamline the mind-mapping process while augmenting situational awareness and promoting active engagement among collaborators. We also report our observational findings on these facets from this iterative design process.},\r\nbooktitle = {Proceedings of the Augmented Humans International Conference 2024},\r\npages = {255–258},\r\nnumpages = {4},\r\nkeywords = {Collaborative Sensemaking, Gamification, Hand Gestures, Virtual Reality},\r\nseries = {AHs '24}\r\n}","notes":"","funding":""},{"Title":"Challenges and potential for human–robot collaboration in timber prefabrication","Submission Target":"Automation in Construction","Date":"2024-02-07","Type":"Full Paper","First Author":"Xiliu Yang","Other Authors":"Felix Amtsberg, Michael Sedlmair, Achim Menges","Key (e.g. for file names)":"yang2024challenges","Publisher URL (official)":"https://doi.org/10.1016/j.autcon.2024.105333","url2":"https://www.sciencedirect.com/science/article/pii/S0926580524000694","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Recent advancements in robotics and human–machine interfaces enable new collaborative procedures that combine the strengths of machines and humans. Compared to existing automation technologies in the timber prefabrication industry, human–robot collaboration (HRC) offers new possibilities for increased flexibility and productivity. This paper aims to map out the challenges and opportunities for HRC within the context of timber prefabrication by constructing a conceptual framework. The framework is based on three pillars: (1) existing HRC theories and frameworks, (2) a literature review of HRC research in robotic timber fabrication, and (3) perspectives of human labour in the timber construction industry. The relevant topics among these three areas are triangulated to construct a conceptual framework that bridges the system-, design-, and human-centred considerations. The framework serves as an organising device to support future explorations and research on human–robot collaboration in robotic timber construction.","bibtex":"@article{YANG2024105333,\r\ntitle = {Challenges and potential for human–robot collaboration in timber prefabrication},\r\njournal = {Automation in Construction},\r\nvolume = {160},\r\npages = {105333},\r\nyear = {2024},\r\nissn = {0926-5805},\r\ndoi = {https://doi.org/10.1016/j.autcon.2024.105333},\r\nurl = {https://www.sciencedirect.com/science/article/pii/S0926580524000694},\r\nauthor = {Xiliu Yang and Felix Amtsberg and Michael Sedlmair and Achim Menges},\r\nkeywords = {Human–robot collaboration, Robotic fabrication, Timber prefabrication, Human-centred design, Co-design},\r\nabstract = {Recent advancements in robotics and human–machine interfaces enable new collaborative procedures that combine the strengths of machines and humans. Compared to existing automation technologies in the timber prefabrication industry, human–robot collaboration (HRC) offers new possibilities for increased flexibility and productivity. This paper aims to map out the challenges and opportunities for HRC within the context of timber prefabrication by constructing a conceptual framework. The framework is based on three pillars: (1) existing HRC theories and frameworks, (2) a literature review of HRC research in robotic timber fabrication, and (3) perspectives of human labour in the timber construction industry. The relevant topics among these three areas are triangulated to construct a conceptual framework that bridges the system-, design-, and human-centred considerations. The framework serves as an organising device to support future explorations and research on human–robot collaboration in robotic timber construction.}\r\n}","notes":"","funding":""},{"Title":"ClustML: A Measure of Cluster Pattern Complexity in Scatterplots Learnt from Human-labeled Groupings","Submission Target":"","Date":"2024-01-01","Type":"Full Paper","First Author":"Mostafa Abbas","Other Authors":"Ehsan Ullah, Abdelkader Baggag, Halima Bensmail, Michael Sedlmair, Michael Aupetit","Key (e.g. for file names)":"abbas2021arxiv","Publisher URL (official)":"https://doi.org/10.1177/14738716231220536","url2":"https://journals.sagepub.com/doi/abs/10.1177/14738716231220536","PDF URL (public)":"https://arxiv.org/pdf/2106.00599.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Visual quality measures (VQMs) are designed to support analysts by automatically detecting and quantifying patterns in visualizations. We propose a new data-driven technique called ClustRank that allows to rank scatterplots according to visible grouping patterns. Our model first encodes scatterplots in the parametric space of a Gaussian Mixture Model, and then uses a classifier trained on human judgment data to estimate the perceptual complexity of grouping patterns. The numbers of initial mixture components and final combined groups determine the rank of the scatterplot. ClustRank improves on existing VQM techniques by mimicking human judgments on two-Gaussian cluster patterns and gives more accuracy when ranking general cluster patterns in scatterplots. We demonstrate its benefit by analyzing kinship data for genome-wide association studies, a domain in which experts rely on the visual analysis of large sets of scatterplots. We make the three benchmark datasets and the ClustRank VQM available for practical use and further improvements. ","bibtex":"@article{abbas2024clustml,\r\nauthor = {Mostafa M. Abbas and Ehsan Ullah and Abdelkader Baggag and Halima Bensmail and Michael Sedlmair and Michael Aupetit},\r\ntitle = {{ClustML}: A Measure of Cluster Pattern Complexity in Scatterplots Learnt from Human-labeled Groupings},\r\njournal = {Information Visualization},\r\npublisher = {{SAGE} Publications},\r\nvolume = {0},\r\nnumber = {0},\r\npages = {14738716231220536},\r\nyear = {2024},\r\ndoi = {10.1177/14738716231220536},\r\nURL = {\r\n        https://doi.org/10.1177/14738716231220536\r\n},\r\neprint = {\r\n        https://doi.org/10.1177/14738716231220536\r\n},\r\n    abstract = {Visual quality measures (VQMs) are designed to support analysts by automatically detecting and quantifying patterns in visualizations. We propose a new VQM for visual grouping patterns in scatterplots, called ClustML, which is trained on previously collected human subject judgments. Our model encodes scatterplots in the parametric space of a Gaussian Mixture Model and uses a classifier trained on human judgment data to estimate the perceptual complexity of grouping patterns. The numbers of initial mixture components and final combined groups quantify visual cluster patterns in scatterplots. It improves on existing VQMs, first, by better estimating human judgments on two-Gaussian cluster patterns and, second, by giving higher accuracy when ranking general cluster patterns in scatterplots. We use it to analyze kinship data for genome-wide association studies, in which experts rely on the visual analysis of large sets of scatterplots. We make the benchmark datasets and the new VQM available for practical use and further improvements.}\r\n}","notes":"","funding":""},{"Title":"Visual Ensemble Analysis of Fluid Flow in Porous Media across Simulation Codes and Experiment","Submission Target":"Transport in Porous Media","Date":"2023-12-01","Type":"Full Paper","First Author":"Ruben Bauer","Other Authors":"Quynh Quang Ngo, Guido Reina, Steffen Frey, Bernd Flemisch, Helwig Hauser, Thomas Ertl, Michael Sedlmair","Key (e.g. for file names)":"bauer2023visual","Publisher URL (official)":"https://doi.org/10.1007/s11242-023-02019-y","url2":"https://link.springer.com/article/10.1007/s11242-023-02019-y","PDF URL (public)":"https://link.springer.com/content/pdf/10.1007/s11242-023-02019-y.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We study the question of how visual analysis can support the comparison of spatio-temporal ensemble data of liquid and gas flow in porous media. To this end, we focus on a case study, in which nine different research groups concurrently simulated the process of injecting CO2 into the subsurface. We explore different data aggregation and interactive visualization approaches to compare and analyze these nine simulations. In terms of data aggregation, one key component is the choice of similarity metrics that define the relationship between different simulations. We test different metrics and find that using the machine-learning model “S4” (tailored to the present study) as metric provides the best visualization results. Based on that, we propose different visualization methods. For overviewing the data, we use dimensionality reduction methods that allow us to plot and compare the different simulations in a scatterplot. To show details about the spatio-temporal data of each individual simulation, we employ a space-time cube volume rendering. All views support linking and brushing interaction to allow users to select and highlight subsets of the data simultaneously across multiple views. We use the resulting interactive, multi-view visual analysis tool to explore the nine simulations and also to compare them to data from experimental setups. Our main findings include new insights into ranking of simulation results with respect to experimental data, and the development of gravity fingers in simulations.","bibtex":"@article{bauer2023visual,\r\n  title={Visual Ensemble Analysis of Fluid Flow in Porous Media Across Simulation Codes and Experiment},\r\n  author={Bauer, Ruben and Ngo, Quynh Quang and Reina, Guido and Frey, Steffen and Flemisch, Bernd and Hauser, Helwig and Ertl, Thomas and Sedlmair, Michael},\r\n  journal={Transport in Porous Media},\r\n  pages={1--29},\r\n  year={2023},\r\n  publisher={Springer},\r\n  doi={10.1007/s11242-023-02019-y},\r\n  url={https://doi.org/10.1007/s11242-023-02019-y} \r\n}","notes":"","funding":""},{"Title":"Visual Guitar Tab Comparison","Submission Target":"ISMIR","Date":"2023-11-09","Type":"Late-Breaking Demo Poster","First Author":"Frank Heyen","Other Authors":"Alejandro Gabino Diaz Mendoza, Quynh Quang Ngo, Michael Sedlmair ","Key (e.g. for file names)":"heyen2023tabcomp","Publisher URL (official)":"https://ismir2023program.ismir.net/lbd_357.html","url2":"","PDF URL (public)":"https://arxiv.org/pdf/2311.14726.pdf","Video":"","Video2":"","Supplemental":"https://github.com/visvar/visual-guitar-tab-comparison","Acknowledgements":"","Abstract":"We designed a visual interface for comparing different guitar tablature (tab) versions of the same piece. By automatically aligning the bars of these versions and visually encoding different metrics, our interface helps determine similarity, difficulty, and correctness. During our design, we collected and integrated feedback from musicians and finally conducted a qualitative evaluation with five guitarists. Results confirm that our interface effectively supports comparison and helps musicians choose a version appropriate for their personal skills and tastes. ","bibtex":"@misc{heyen2023vistabcomp,\r\n      title={Visual Guitar Tab Comparison}, \r\n      author={Frank Heyen and Alejandro Gabino Diaz Mendoza and Quynh Quang Ngo and Michael Sedlmair},\r\n      howpublished={International Conference on Music Information Retrieval ({ISMIR}) Late-breaking Demo},\r\n      year={2023},\r\n      eprint={2311.14726},\r\n      doi={https://doi.org/10.48550/arXiv.2311.14726}\r\n}","notes":"","funding":""},{"Title":"Visual Overviews for Sheet Music Structure","Submission Target":"ISMIR","Date":"2023-11-05","Type":"Full Paper","First Author":"Frank Heyen","Other Authors":"Quynh Quang Ngo, Michael Sedlmair","Key (e.g. for file names)":"heyen2023visual","Publisher URL (official)":"https://doi.org/10.5281/zenodo.10265383","url2":"https://ismir2023program.ismir.net/poster_216.html","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=zvok5W4p8oo","Video2":"","Supplemental":"https://github.com/visvar/sheetmusic-overviews","Acknowledgements":"","Abstract":"We propose different methods for alternative represen-\r\ntation and visual augmentation of sheet music that help\r\nusers gain an overview of general structure, repeating pat-\r\nterns, and the similarity of segments. To this end, we ex-\r\nplored mapping the overall similarity between sections or\r\nbars to colors. For these mappings, we use dimensionality\r\nreduction or clustering to assign similar segments to simi-\r\nlar colors and vice versa. To provide a better overview, we\r\nfurther designed simplified music notation representations,\r\nincluding hierarchical and compressed encodings. These\r\noverviews allow users to display whole pieces more com-\r\npactly on a single screen without clutter and to find and\r\nnavigate to distant segments more quickly. Our prelimi-\r\nnary evaluation with guitarists and tablature shows that our\r\ndesign supports users in tasks such as analyzing structure,\r\nfinding repetitions, and determining the similarity of spe-\r\ncific segments to others","bibtex":"@inproceedings{frank_heyen_2023_10265383,\r\n  author       = {Frank Heyen and\r\n                  Quynh Quang Ngo and\r\n                  Michael Sedlmair},\r\n  title        = {Visual Overviews for Sheet Music Structure},\r\n  booktitle    = {{Proceedings of the 24th International Society for \r\n                   Music Information Retrieval Conference}},\r\n  year         = 2023,\r\n  pages        = {692-699},\r\n  publisher    = {ISMIR},\r\n  month        = dec,\r\n  venue        = {Milan, Italy},\r\n  doi          = {10.5281/zenodo.10265383},\r\n  url          = {https://doi.org/10.5281/zenodo.10265383}\r\n}","notes":"","funding":""},{"Title":"SeatmateVR: Proxemic Cues for Close Bystander-Awareness in Virtual Reality","Submission Target":"ISS","Date":"2023-11-05","Type":"Full Paper","First Author":"Jingyi Li","Other Authors":"Hyerim Park, Robin Welsch, Sven Mayer, Andreas Butz","Key (e.g. for file names)":"li2023seatmatevr","Publisher URL (official)":"https://doi.org/10.1145/3626474","url2":"https://dl.acm.org/doi/abs/10.1145/3626474","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Prior research explored ways to alert virtual reality users of bystanders entering the play area from afar. However, in confined social settings like sharing a couch with seatmates, bystanders’ proxemic cues, such as distance, are limited during interruptions, posing challenges for proxemic-aware systems. To address this, we investigated three visualizations, using a 2D animoji, a fully-rendered avatar, and their combination, to gradually share bystanders’ orientation and location during interruptions. In a user study (N=22), participants played virtual reality games while responding to questions from their seatmates. We found that the avatar preserved game experiences yet did not support the fast identification of seatmates as the animoji did. Instead, users preferred the mixed visualization, where they found the seatmate’s orientation cues instantly in their view and were gradually guided to the person’s actual location. We discuss implications for fine-grained proxemic-aware virtual reality systems to support interaction in constrained social spaces.\r\n","bibtex":"@article{10.1145/3626474,\r\nauthor = {Li, Jingyi and Park, Hyerim and Welsch, Robin and Mayer, Sven and Butz, Andreas},\r\ntitle = {{SeatmateVR}: Proxemic Cues for Close Bystander-Awareness in Virtual Reality},\r\nyear = {2023},\r\nissue_date = {December 2023},\r\npublisher = {ACM},\r\naddress = {New York, NY, USA},\r\nvolume = {7},\r\nnumber = {ISS},\r\nurl = {https://doi.org/10.1145/3626474},\r\ndoi = {10.1145/3626474},\r\nabstract = {Prior research explored ways to alert virtual reality users of bystanders entering the play area from afar. However, in confined social settings like sharing a couch with seatmates, bystanders' proxemic cues, such as distance, are limited during interruptions, posing challenges for proxemic-aware systems. To address this, we investigated three visualizations, using a 2D animoji, a fully-rendered avatar, and their combination, to gradually share bystanders' orientation and location during interruptions. In a user study (N=22), participants played virtual reality games while responding to questions from their seatmates. We found that the avatar preserved game experiences yet did not support the fast identification of seatmates as the animoji did. Instead, users preferred the mixed visualization, where they found the seatmate's orientation cues instantly in their view and were gradually guided to the person's actual location. We discuss implications for fine-grained proxemic-aware virtual reality systems to support interaction in constrained social spaces.},\r\njournal = {Proc. ACM Hum.-Comput. Interact.},\r\nmonth = {nov},\r\narticleno = {438},\r\nnumpages = {20},\r\nkeywords = {bystander awareness, constrained interaction space, proxemic-aware virtual reality}\r\n}","notes":"","funding":""},{"Title":"Design Patterns for Situated Visualization in Augmented Reality","Submission Target":"VIS","Date":"2023-10-26","Type":"Full Paper","First Author":"Benjamin Lee","Other Authors":"Michael Sedlmair, Dieter Schmalstieg","Key (e.g. for file names)":"lee2023design","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2023.3327398","url2":"https://ieeexplore.ieee.org/abstract/document/10297565","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://www.researchgate.net/profile/Benjamin-Lee-34/publication/372551296_Lee2023DesignPatternsSitVis_Supplementalpdf/data/64be359495bbbe0c6e5ac5db/Lee2023DesignPatternsSitVis-Supplemental.pdf","Acknowledgements":"","Abstract":"Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics. Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore. However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts. In this work, we leverage the richness of AR research and apply it to situated visualization. We derive design patterns which summarize common approaches of visualizing data in situ. The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise. We discuss design dimensions that help to describe both our patterns and previous work in the literature. This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world. We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows.","bibtex":"@ARTICLE{10297565,\r\n  author={Lee, Benjamin and Sedlmair, Michael and Schmalstieg, Dieter},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Design Patterns for Situated Visualization in Augmented Reality}, \r\n  year={2023},\r\n  pages={1-12},\r\n  doi={10.1109/TVCG.2023.3327398}}\r\n","notes":"","funding":""},{"Title":"Virtual Reality Training for Nosocomial Infections Prevention","Submission Target":"VIS","Date":"2023-10-23","Type":"Poster","First Author":"Mengjie Fan","Other Authors":"Shaoxing Zhang, Xintian Zhao, Xingyao Yu, Liang Zhou","Key (e.g. for file names)":"fan2023virtual","Publisher URL (official)":"","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Nosocomial infections (or healthcare-associated infections) can greatly affect public health. The prevention and control of nosocomial infections rely on effective training of medical personnel on the correct use of personal prevention equipment (PPE). We introduce a virtual reality (VR) method that simulates the real environment of a hospital and supports repeated immersive practice of PPE donning\r\nand doffing. A VR prototype is created and receives positive feedback from a domain expert. The effectiveness of our method will be evaluated in a comparative user study.","bibtex":"","notes":"","funding":""},{"Title":"LSDvis: Hallucinatory Data Visualisations in Real World Environments","Submission Target":"alt.VIS","Date":"2023-10-23","Type":"Workshop Paper","First Author":"Ari Kouts","Other Authors":"Lonni Besançon, Michael Sedlmair, Benjamin Lee","Key (e.g. for file names)":"kouts2023lsdvis","Publisher URL (official)":"https://arxiv.org/abs/2312.11144","url2":"https://altvis.github.io/","PDF URL (public)":"https://arxiv.org/pdf/2312.11144","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We propose the concept of \"LSDvis\": the (highly exaggerated) visual blending of situated visualisations and the real-world environment to produce data representations that resemble hallucinations. Such hallucinatory visualisations incorporate elements of the physical environment, twisting and morphing their appearance such that they become part of the visualisation itself. We demonstrate LSDvis in a \"proof of proof of concept\", where we use Stable Diffusion to modify images of real environments with abstract data visualisations as input. We conclude by discussing considerations of LSDvis. We hope that our work promotes visualisation designs which deprioritise saliency in favour of quirkiness and ambience.","bibtex":"@misc{kouts2023lsdvis,\r\n      title={LSDvis: Hallucinatory Data Visualisations in Real World Environments}, \r\n      author={Ari Kouts and Lonni Besançon and Michael Sedlmair and Benjamin Lee},\r\n      year={2023},\r\n      eprint={2312.11144},\r\n      archivePrefix={arXiv},\r\n      primaryClass={cs.HC},\r\n      doi={https://doi.org/10.48550/arXiv.2312.11144}\r\n}","notes":"","funding":""},{"Title":"Work vs. Leisure – Differences in Avatar Characteristics Depending on Social Situations","Submission Target":"VINCI","Date":"2023-10-20","Type":"Full Paper","First Author":"Natalie Hube","Other Authors":"Melissa Reinelt, Kresimir Vidackovic, Michael Sedlmair","Key (e.g. for file names)":"hube2023work","Publisher URL (official)":"https://doi.org/10.1145/3615522.3615537","url2":"https://dl.acm.org/doi/abs/10.1145/3615522.3615537","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"User avatars are a critical component in collaborative Virtual Environments. A multitude of tools exist, employing various avatar types for self-representation and the representation of others. However, the optimal appearance for these avatars remains unclear. Consumer applications predominantly utilize stylized avatars, which are less prevalent in enterprise sectors. To investigate users’ avatar preferences, we conducted three user studies in both non-immersive and immersive environments, exploring whether social contexts influence virtual self-avatar selection. We generated individual avatars based on photographs of 91 participants, comprising 71 employees in the automotive industry and 20 individuals external to the company. Our findings indicate that work situations, irrespective of the occupational domain, significantly impact self-avatar choices. Conversely, we observed no correlation between the display medium or personality dimensions and avatar selection. Drawing upon our results, we provide recommendations for future avatar representation in work environments. ","bibtex":"@inproceedings{10.1145/3615522.3615537,\r\nauthor = {Hube, Natalie and Reinelt, Melissa and Vidackovic, Kresimir and Sedlmair, Michael},\r\ntitle = {Work vs. Leisure – Differences in Avatar Characteristics Depending on Social Situations},\r\nyear = {2023},\r\nisbn = {9798400707513},\r\npublisher = {ACM},\r\nurl = {https://doi.org/10.1145/3615522.3615537},\r\ndoi = {10.1145/3615522.3615537},\r\nabstract = {User avatars are a critical component in collaborative Virtual Environments. A multitude of tools exist, employing various avatar types for self-representation and the representation of others. However, the optimal appearance for these avatars remains unclear. Consumer applications predominantly utilize stylized avatars, which are less prevalent in enterprise sectors. To investigate users’ avatar preferences, we conducted three user studies in both non-immersive and immersive environments, exploring whether social contexts influence virtual self-avatar selection. We generated individual avatars based on photographs of 91 participants, comprising 71 employees in the automotive industry and 20 individuals external to the company. Our findings indicate that work situations, irrespective of the occupational domain, significantly impact self-avatar choices. Conversely, we observed no correlation between the display medium or personality dimensions and avatar selection. Drawing upon our results, we provide recommendations for future avatar representation in work environments.},\r\nbooktitle = {Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},\r\narticleno = {15},\r\nnumpages = {9},\r\nkeywords = {user study, virtual reality, avatars},\r\nseries = {VINCI '23}\r\n}","notes":"","funding":""},{"Title":"guitARhero: Interactive Augmented Reality Guitar Tutorials","Submission Target":"ISMAR","Date":"2023-10-16","Type":"Full Paper","First Author":"Lucchas R Skreinig","Other Authors":"Denis Kalkofen, Ana Stanescu, Peter Mohr, Frank Heyen, Shohei Mori, Michael Sedlmair, Dieter Schmalstieg, Alexander Plopski","Key (e.g. for file names)":"skreinig2023guitarhero","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2023.3320266","url2":"https://ieeexplore.ieee.org/abstract/document/10268399","PDF URL (public)":"https://ieeexplore.ieee.org/iel7/2945/4359476/10268399.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"This paper presents guitARhero, an Augmented Reality application for interactively teaching guitar playing to beginners\r\nthrough responsive visualizations overlaid on the guitar neck. We support two types of visual guidance, a highlighting of the frets\r\nthat need to be pressed and a 3D hand overlay, as well as two display scenarios, one using a desktop magic mirror and one using a\r\nvideo see-through head-mounted display. We conducted a user study with 20 participants to evaluate how well users could follow\r\ninstructions presented with different guidance and display combinations and compare these to a baseline where users had to follow\r\nvideo instructions. Our study highlights the trade-off between the provided information and visual clarity affecting the user’s ability to\r\ninterpret and follow instructions for fine-grained tasks. We show that the perceived usefulness of instruction integration into an HMD\r\nview highly depends on the hardware capabilities and instruction details.","bibtex":"@ARTICLE{10268399,\r\n  author={Skreinig, Lucchas Ribeiro and Kalkofen, Denis and Stanescu, Ana and Mohr, Peter and Heyen, Frank and Mori, Shohei and Sedlmair, Michael and Schmalstieg, Dieter and Plopski, Alexander},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={guitARhero: Interactive Augmented Reality Guitar Tutorials}, \r\n  year={2023},\r\n  pages={1-10},\r\n  doi={10.1109/TVCG.2023.3320266}}\r\n","notes":"","funding":""},{"Title":"Designing Situated Dashboards: Challenges and Opportunities","Submission Target":"ISMAR","Date":"2023-10-16","Type":"Workshop Paper","First Author":"Anika Sayara","Other Authors":"Benjamin Lee, Carlos Quijano-Chavez, Michael Sedlmair","Key (e.g. for file names)":"sayara2023designing","Publisher URL (official)":"","url2":"https://arxiv.org/abs/2309.02945","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Situated Visualization is an emerging field that unites several areas - visualization, augmented reality, human-computer interaction, and internet-of-things, to support human data activities within the ubiquitous world. Likewise, dashboards are broadly used to simplify complex data through multiple views. However, dashboards are only adapted for desktop settings, and requires visual strategies to support situatedness. We propose the concept of AR-based situated dashboards and present design considerations and challenges developed over interviews with experts. These challenges aim to propose directions and opportunities for facilitating the effective designing and authoring of situated dashboards.","bibtex":"@misc{sayara2023designing,\r\n      title={Designing Situated Dashboards: Challenges and Opportunities}, \r\n      author={Anika Sayara and Benjamin Lee and Carlos Quijano-Chavez and Michael Sedlmair},\r\n      year={2023},\r\n      eprint={2309.02945},\r\n      archivePrefix={arXiv},\r\n      primaryClass={cs.HC}\r\n}","notes":"","funding":""},{"Title":"Usability Evaluation of an Augmented Reality System for Collaborative Fabrication between Multiple Humans and Industrial Robots","Submission Target":"SUI","Date":"2023-10-13","Type":"Full Paper","First Author":"Xiliu Yang","Other Authors":"Aimee Sousa Calepso, Felix Amtsberg, Achim Menges, Michael Sedlmair","Key (e.g. for file names)":"yang2023usability","Publisher URL (official)":"https://doi.org/10.1145/3607822.3614528","url2":"https://dl.acm.org/doi/abs/10.1145/3607822.3614528","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Semi-automated timber fabrication tasks demand the expertise and dexterity of human workers in addition to the use of automated robotic systems. In this paper, we introduce a human-robot collaborative system based on Augmented Reality (AR). To assess our approach, we conducted an exploratory user study on a head-mounted display (HMD) interface for task sharing between humans and an industrial robotic platform (N=16). Instead of screen-based interfaces, HMDs allowed users to receive information in-situ, regardless of their location in the workspace and the need to use their hands to handle tools or carry out tasks. We analyzed the resulting open-ended, qualitative user feedback as well as the quantitative user experience data. From the results, we derived challenges to tackle in future implementations and questions that need to be investigated to improve AR-based HRI in fabrication scenarios. The results also suggest that some aspects of human-robot interaction, like communication and trust, are more prominent when implementing a non-dyadic scenario and dealing with larger robots. The study is intended as a prequel to future work into AR-based collaboration between multiple humans and industrial robots. ","bibtex":"@inproceedings{10.1145/3607822.3614528,\r\nauthor = {Yang, Xiliu and Sousa Calepso, Aim\\'{e}e and Amtsberg, Felix and Menges, Achim and Sedlmair, Michael},\r\ntitle = {Usability Evaluation of an Augmented Reality System for Collaborative Fabrication between Multiple Humans and Industrial Robots},\r\nyear = {2023},\r\nisbn = {9798400702815},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3607822.3614528},\r\ndoi = {10.1145/3607822.3614528},\r\nabstract = {Semi-automated timber fabrication tasks demand the expertise and dexterity of human workers in addition to the use of automated robotic systems. In this paper, we introduce a human-robot collaborative system based on Augmented Reality (AR). To assess our approach, we conducted an exploratory user study on a head-mounted display (HMD) interface for task sharing between humans and an industrial robotic platform (N=16). Instead of screen-based interfaces, HMDs allowed users to receive information in-situ, regardless of their location in the workspace and the need to use their hands to handle tools or carry out tasks. We analyzed the resulting open-ended, qualitative user feedback as well as the quantitative user experience data. From the results, we derived challenges to tackle in future implementations and questions that need to be investigated to improve AR-based HRI in fabrication scenarios. The results also suggest that some aspects of human-robot interaction, like communication and trust, are more prominent when implementing a non-dyadic scenario and dealing with larger robots. The study is intended as a prequel to future work into AR-based collaboration between multiple humans and industrial robots.},\r\nbooktitle = {Proceedings of the 2023 ACM Symposium on Spatial User Interaction},\r\narticleno = {18},\r\nnumpages = {10},\r\nkeywords = {user experience, robotic fabrication, human-robot interaction, augmented reality},\r\nlocation = {Sydney, NSW, Australia},\r\nseries = {SUI '23}\r\n}","notes":"","funding":""},{"Title":"Moving Haptics Research into Practice: Four Case Studies from Automotive Engineering","Submission Target":"University of Stuttgart","Date":"2023-10-11","Type":"PhD Thesis","First Author":"Alexander Achberger","Other Authors":"","Key (e.g. for file names)":"achberger2023moving","Publisher URL (official)":"http://dx.doi.org/10.18419/opus-13903","url2":"https://elib.uni-stuttgart.de/handle/11682/13922","PDF URL (public)":"https://elib.uni-stuttgart.de/bitstream/11682/13922/3/Diss_Achberger_Final.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Virtual Reality (VR) has gained popularity and found applications in various fields, including the automotive industry. Over the years, VR has been used in assembly, ergonomic studies, and other automotive use cases to aid development. Engineers already benefit from using VR at different stages of development, but the current setups provide only visual, auditory, and limited vibrotactile feedback on controllers. These feedback types cannot accurately simulate forces for realistic collisions or weight simulation of virtual car components. Despite ongoing research in haptic technology, there are still limitations in creating a perfect haptic feedback system that can accurately simulate all tactile and kinesthetic stimuli without hindering movement or comfort. The automotive industry needs suitable haptic feedback devices, but most research focuses on developing new devices rather than integrating them practically. We conducted four case studies to address this challenge to bridge the gap between haptic research and practical applications in automotive VR tasks. During these studies, we collaborated closely with automotive VR engineers to understand their needs and obtain feedback on using haptic devices. Our approach involved developing new haptic feedback devices based on technique- and problem-driven approaches. We created the PropellerHand, an ungrounded hand-mounted haptic device that allows forces on the hand without hindering hand use. STRIVE and STROE were developed based on problem-driven approaches, providing string-based haptic feedback devices to simulate collisions and weight. Finally, we created a multimodal haptic feedback system by combining STRIVE, STROE, and the haptic feedback glove SenseGlove, enabling users to simultaneously experience grabbing, weight, and collision feedback. Over three years, we extensively researched and implemented haptic feedback devices in practical settings. We interviewed more than 25 VR experts from the automotive industry, observed over 45 VR use cases, and collected feedback from over 200 individuals who tested our feedback devices. Based on this information, we formulated recommendations for moving haptic research into practice.\r\n","bibtex":"@article{achberger2023moving,\r\n  title={Moving haptics research into practice: four case studies from automotive engineering},\r\n  author={Achberger, Alexander},\r\n  year={2023},\r\n  doi={http://dx.doi.org/10.18419/opus-13903}\r\n}","notes":"","funding":""},{"Title":"Exploring Augmented Reality for Situated Analytics with Many\r\nMovable Physical Referents","Submission Target":"VRST","Date":"2023-10-09","Type":"Full Paper","First Author":"Aimee Sousa Calepso","Other Authors":"Philipp Fleck, Dieter Schmalstieg, Michael Sedlmair","Key (e.g. for file names)":"calepso2023exploring","Publisher URL (official)":"https://doi.org/10.1145/3611659.3615700","url2":"https://dl.acm.org/doi/10.1145/3611659.3615700","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3611659.3615700","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Situated analytics (SitA) uses visualization in the context of physical referents, typically by using augmented reality (AR). We want to pave the way toward studying SitA in more suitable and realistic settings. Toward this goal, we contribute a testbed to evaluate SitA based on a scenario in which participants play the role of a museum curator and need to organize an exhibition of music artifacts. We conducted two experiments: First, we evaluated an AR headset interface and the testbed itself in an exploratory manner. Second, we compared the AR headset to a tablet interface. We summarize the lessons learned as guidance for designing and evaluating SitA.","bibtex":"@inproceedings{10.1145/3611659.3615700,\r\nauthor = {Calepso, Aimee Sousa and Fleck, Philipp and Schmalstieg, Dieter and Sedlmair, Michael},\r\ntitle = {Exploring Augmented Reality for Situated Analytics with Many Movable Physical Referents},\r\nyear = {2023},\r\nisbn = {9798400703287},\r\npublisher = {Association for Computing Machinery},\r\nurl = {https://doi.org/10.1145/3611659.3615700},\r\ndoi = {10.1145/3611659.3615700},\r\nabstract = {Situated analytics (SitA) uses visualization in the context of physical referents, typically by using augmented reality (AR). We want to pave the way toward studying SitA in more suitable and realistic settings. Toward this goal, we contribute a testbed to evaluate SitA based on a scenario in which participants play the role of a museum curator and need to organize an exhibition of music artifacts. We conducted two experiments: First, we evaluated an AR headset interface and the testbed itself in an exploratory manner. Second, we compared the AR headset to a tablet interface. We summarize the lessons learned as guidance for designing and evaluating SitA.},\r\nbooktitle = {Proceedings of the 29th ACM Symposium on Virtual Reality Software and Technology},\r\narticleno = {6},\r\nnumpages = {12},\r\nkeywords = {Immersive analytics, Augmented Reality, Situated analytics},\r\nlocation = {Christchurch, New Zealand},\r\nseries = {VRST '23}\r\n}","notes":"","funding":""},{"Title":"What’s (Not) Tracking? Factors of Influence in Industrial Augmented Reality Tracking: A Use Case Study in an Automotive Environment","Submission Target":"AutomotiveUI","Date":"2023-09-18","Type":"Full Paper","First Author":"Jonas Haischt","Other Authors":"Michael Sedlmair","Key (e.g. for file names)":"haischt2023whats","Publisher URL (official)":"https://doi.org/10.1145/3580585.3607156","url2":"https://dl.acm.org/doi/abs/10.1145/3580585.3607156","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Augmented Reality (AR) is a key technology for digitization in enterprises. However, often there is a lack of stable tracking solutions when used inside manufacturing environments. Many different tracking technologies are available, yet, it can be difficult to choose the most appropriate tracking solution for different use cases with their varying conditions. In order to shed light on common tracking requirements and conditions for automotive AR use cases we conducted a use case study spanning 61 use cases within the complete product life-cycle of a large automotive manufacturer. By analyzing the gathered data we were able to note the frequency of different tracking requirements and conditions within automotive AR use cases. Based on these use cases we could also derive common factors of influence for AR tracking in the automotive industry, which show the various challenges automotive AR tracking is currently facing.","bibtex":"@inproceedings{10.1145/3580585.3607156,\r\nauthor = {Haischt, Jonas and Sedlmair, Michael},\r\ntitle = {What’s (Not) Tracking? Factors of Influence in Industrial Augmented Reality Tracking: A Use Case Study in an Automotive Environment},\r\nyear = {2023},\r\nisbn = {9798400701054},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3580585.3607156},\r\ndoi = {10.1145/3580585.3607156},\r\nabstract = {Augmented Reality (AR) is a key technology for digitization in enterprises. However, often there is a lack of stable tracking solutions when used inside manufacturing environments. Many different tracking technologies are available, yet, it can be difficult to choose the most appropriate tracking solution for different use cases with their varying conditions. In order to shed light on common tracking requirements and conditions for automotive AR use cases we conducted a use case study spanning 61 use cases within the complete product life-cycle of a large automotive manufacturer. By analyzing the gathered data we were able to note the frequency of different tracking requirements and conditions within automotive AR use cases. Based on these use cases we could also derive common factors of influence for AR tracking in the automotive industry, which show the various challenges automotive AR tracking is currently facing.},\r\nbooktitle = {Proceedings of the 15th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},\r\npages = {42–51},\r\nnumpages = {10},\r\nkeywords = {Automotive Industry, Augmented Reality Tracking, Use Case Study},\r\nlocation = {Ingolstadt, Germany},\r\nseries = {AutomotiveUI '23}\r\n}","notes":"","funding":""},{"Title":"Multi-Akteur-Fabrikation im Bauwesen","Submission Target":"Bautechnik","Date":"2023-09-18","Type":"Full Paper","First Author":"Felix Amtsberg","Other Authors":"Xiliu Yang, Lior Skoury, Gili Ron, Benjamin Kaiser, Aimee Sousa Calepso, Michael Sedlmair, Alexander Verl, Thomas Wortmann, Achim Menges","Key (e.g. for file names)":"amtsberg2023multi-akteur-fabrikation","Publisher URL (official)":"https://doi.org/10.1002/bate.202300070","url2":"https://onlinelibrary.wiley.com/doi/full/10.1002/bate.202300070","PDF URL (public)":"https://papers.cumincad.org/data/works/att/ecaade2023_422.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Die aktuelle Generation von Planern ist mit der Aufgabe konfrontiert, neue, nachhaltigere Bausysteme zu entwickeln, um der Problematik der Ressourcenknappheit, der Verstädterung, dem Klima-, aber auch dem demografischen Wandel entgegenzuwirken. Hierbei ist die Frage nach deren effizienter Herstellung unter Nutzung nachwachsender Rohstoffe in den Fokus der Forschungsbemühungen gerückt. In den letzten Jahren hat der Automatisierungsgrad in der Vorfertigung unter der Agenda der produktbasierten Bausysteme zugenommen, die ein hohes Potenzial aufweisen, aber in Umgebungen, wo eine höhere Flexibilität erforderlich ist, nur bedingt zielführend sind. Die Realisierung ressourcensparender Entwürfe führt unweigerlich zu kleinen Losgrößen und einmalig zu produzierenden Bauelementen, die für eine effiziente Fabrikation anpassungsfähige Vorfertigungsanlagen für eine Vielzahl von Lösungen erfordern. Am Exzellenzcluster Integratives computerbasiertes Planen und Bauen für die Architektur (IntCDC) werden Methoden erforscht, welche mittels Mensch-Maschine-Kollaboration eine Steigerung von Flexibilität und Adaptivität der Vorfabrikation und Konstruktion von Bauwerken anstreben. Dazu werden neben der Durchführung von Untersuchungen zur Augmented-Reality-Technologie-unterstützten Integration und Kommunikation von Mensch und Maschine auch auf maschinellem Lernen basierende Trainingsmethoden und Vorhersagemodelle entwickelt.\r\n\r\nThe current generation of planners is confronted with the task of developing new, more sustainable building systems to counteract the problems of resource scarcity, urbanization, climate change and demographic change. In this context, the question of their efficient production using renewable raw materials has become the focus of research efforts. In recent years, the degree of automation in prefabrication has increased under the agenda of product-based building systems, which have a high potential but are of limited use, where greater flexibility is required. The realization of resource-efficient designs inevitably leads to small lot sizes and one-off building elements that require adaptable prefabrication facilities for a variety of solutions for efficient fabrication. At the Cluster of Excellence Integrative Computational Design and Construction for Architecture (IntCDC) methods are being researched which aim to increase the flexibility and adaptivity of prefabrication and construction of buildings by means of human-machine collaboration. To this end, research is also being conducted on the integration and communication of humans and machines supported by augmented reality technology, as well as on training methods and prediction models based on machine learning.","bibtex":"@article{https://doi.org/10.1002/bate.202300070,\r\nauthor = {Amtsberg, Felix and Yang, Xiliu and Skoury, Lior and Ron, Gili and Kaiser, Benjamin and Sousa Calepso, Aimée and Sedlmair, Michael and Verl, Alexander and Wortmann, Thomas and Menges, Achim},\r\ntitle = {Multi-Akteur-Fabrikation im Bauwesen},\r\njournal = {Bautechnik},\r\nkeywords = {Mensch-Roboter-Kollaboration, Mensch-Roboter-Kooperation, Augmented Reality, Vorfabrikation, maschinelles Lernen, Co-Design, human-robot collaboration, human-robot collaboration, augmented reality, prefabrication, machine learning, co-design},\r\ndoi = {https://doi.org/10.1002/bate.202300070},\r\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bate.202300070},\r\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/bate.202300070},\r\nabstract = {Abstract Die aktuelle Generation von Planern ist mit der Aufgabe konfrontiert, neue, nachhaltigere Bausysteme zu entwickeln, um der Problematik der Ressourcenknappheit, der Verstädterung, dem Klima-, aber auch dem demografischen Wandel entgegenzuwirken. Hierbei ist die Frage nach deren effizienter Herstellung unter Nutzung nachwachsender Rohstoffe in den Fokus der Forschungsbemühungen gerückt. In den letzten Jahren hat der Automatisierungsgrad in der Vorfertigung unter der Agenda der produktbasierten Bausysteme zugenommen, die ein hohes Potenzial aufweisen, aber in Umgebungen, wo eine höhere Flexibilität erforderlich ist, nur bedingt zielführend sind. Die Realisierung ressourcensparender Entwürfe führt unweigerlich zu kleinen Losgrößen und einmalig zu produzierenden Bauelementen, die für eine effiziente Fabrikation anpassungsfähige Vorfertigungsanlagen für eine Vielzahl von Lösungen erfordern. Am Exzellenzcluster Integratives computerbasiertes Planen und Bauen für die Architektur (IntCDC) werden Methoden erforscht, welche mittels Mensch-Maschine-Kollaboration eine Steigerung von Flexibilität und Adaptivität der Vorfabrikation und Konstruktion von Bauwerken anstreben. Dazu werden neben der Durchführung von Untersuchungen zur Augmented-Reality-Technologie-unterstützten Integration und Kommunikation von Mensch und Maschine auch auf maschinellem Lernen basierende Trainingsmethoden und Vorhersagemodelle entwickelt.}\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"A Virtual Reality Simulator for Timber Fabrication Tasks Using Industrial Robotic Arms","Submission Target":"MuC","Date":"2023-09-03","Type":"Demo","First Author":"Eric Bossecker","Other Authors":"Aimee Sousa Calepso, Benjamin Kaiser, Alexander Verl, Michael Sedlmair","Key (e.g. for file names)":"bossecker2023a","Publisher URL (official)":"https://doi.org/10.1145/3603555.3609316","url2":"https://dl.acm.org/doi/abs/10.1145/3603555.3609316","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3603555.3609316","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Virtual Reality (VR) simulators are well known applications for immersive spaces. When it comes to Human-Robot Collaboration(HRC), training and safety are important aspects that can be supported by simulators. In this demo, we propose a workflow between ROS and a VR application that allows the use of real robot programming plans to control the robot’s digital twin. We also provide the results of a short evaluation that was done with 6 experts, where they provided insights for future improvement and use cases. Our demo is the first step towards a full integrated system where path planning and fabrication steps can be tested with users without any of the risk and cost that are involved when using real industrial robots.","bibtex":"@inproceedings{10.1145/3603555.3609316,\r\nauthor = {Bossecker, Eric and Sousa Calepso, Aim\\'{e}e and Kaiser, Benjamin and Verl, Alexander and Sedlmair, Michael},\r\ntitle = {A Virtual Reality Simulator for Timber Fabrication Tasks Using Industrial Robotic Arms},\r\nyear = {2023},\r\nisbn = {9798400707711},\r\npublisher = {ACM},\r\nurl = {https://doi.org/10.1145/3603555.3609316},\r\ndoi = {10.1145/3603555.3609316},\r\nabstract = {Virtual Reality (VR) simulators are well known applications for immersive spaces. When it comes to Human-Robot Collaboration(HRC), training and safety are important aspects that can be supported by simulators. In this demo, we propose a workflow between ROS and a VR application that allows the use of real robot programming plans to control the robot’s digital twin. We also provide the results of a short evaluation that was done with 6 experts, where they provided insights for future improvement and use cases. Our demo is the first step towards a full integrated system where path planning and fabrication steps can be tested with users without any of the risk and cost that are involved when using real industrial robots.},\r\nbooktitle = {Proceedings of Mensch Und Computer (MuC)},\r\npages = {568–570},\r\nnumpages = {3},\r\nseries = {MuC '23}\r\n}","notes":"","funding":""},{"Title":"Skill-based Robot Programming in Mixed Reality with Ad-hoc Validation Using a Force-enabled Digital Twin","Submission Target":"ICRA","Date":"2023-07-04","Type":"Full Paper","First Author":"Jan Krieglstein","Other Authors":"Gesche Held, Balázs A. Bálint, Frank Nägele, Werner Kraus","Key (e.g. for file names)":"krieglstein2023skill-based","Publisher URL (official)":"https://doi.org/10.1109/ICRA48891.2023.10161095","url2":"https://ieeexplore.ieee.org/abstract/document/10161095","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Skill-based programming has proven to be advantageous for assembly tasks, but still requires expert knowledge, especially for force-controlled applications. However, it is error-prone due to the multitude of parameters, e.g. different coordinate frames and either position-, velocity- or force-controlled motions on the axes of a frame. We propose a mixed reality based solution, which systematically visualizes the geometric constraints of advanced high-level skills directly in the real-world robotic environment and provides a user interface to create applications efficiently and safely in mixed reality. Therefore, state-machine information is also visualized, and a holographic digital twin allows the user to ad-hoc validate the program via force-enabled simulation. The approach is evaluated on a top hat rail mounting task, proving the capability of the system to handle advanced assembly programming tasks efficiently and tangibly.","bibtex":"@INPROCEEDINGS{10161095,\r\n  author={Krieglstein, Jan and Held, Gesche and Bálint, Balázs A. and Nägele, Frank and Kraus, Werner},\r\n  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)}, \r\n  title={Skill-based Robot Programming in Mixed Reality with Ad-hoc Validation Using a Force-enabled Digital Twin}, \r\n  year={2023},\r\n  pages={11612-11618},\r\n  doi={10.1109/ICRA48891.2023.10161095}}\r\n","notes":"","funding":""},{"Title":"3D Hapkit: A Low-Cost, Open-Source, 3-DOF Haptic Device Based on the Delta Parallel Mechanism","Submission Target":"IEEE World Haptics Conference","Date":"2023-07-01","Type":"","First Author":"Han Zhang","Other Authors":"Jan Ulrich Bartels, Jeremy Brown","Key (e.g. for file names)":"zhang20233d","Publisher URL (official)":"","url2":"","PDF URL (public)":"https://2023.worldhaptics.org/wp-content/uploads/2023/06/1070-doc.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present our work on an open-source, low-cost, 3-Degree Of Freedom (3-DOF) haptic device for use in haptic education. The device is based on three open-source 1-DOF haptic devices in a delta configuration and can be produced using commonly available rapid prototyping methodologies such as 3D printing and laser cutting. We also demonstrate a simple interaction with a virtual environment.","bibtex":"","notes":"","funding":""},{"Title":"Visual Planning and Analysis of Latin Formation Dance Patterns","Submission Target":"EuroVis","Date":"2023-06-12","Type":"Poster","First Author":"Samuel Beck","Other Authors":"Nina Doerr, Fabian Schmierer, Michael Sedlmair, Steffen Koch","Key (e.g. for file names)":"beck2023visual","Publisher URL (official)":"https://doi.org/10.2312/evp.20231076","url2":"https://diglib.eg.org/xmlui/handle/10.2312/evp20231076","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://doi.org/10.2312/evp.20231076","Acknowledgements":"","Abstract":"Latin formation dancing is a team sport in which up to eight couples perform a coordinated choreography. A central part are the patterns formed by the dancers on the dance floor and the transitions between them. Planning and practicing patterns are some of the most challenging aspects of Latin formation dancing. Interactive visualization approaches can support instructors as well as dancers in tackling these challenges. We present a web-based visualization prototype that assists with the planning, training, and analysis of patterns. Its design was iteratively developed with the involvement of experienced formation instructors. The interface offers views of the dancers' positions and orientations, pattern transitions, poses, and analytical information like dance floor utilization and movement distances. In a first expert study with formation instructors, the prototype was well received.","bibtex":"@inproceedings {10.2312:evp.20231076,\r\nbooktitle = {EuroVis 2023 - Posters},\r\neditor = {Gillmann, Christina and Krone, Michael and Lenti, Simone},\r\ntitle = {{Visual Planning and Analysis of Latin Formation Dance Patterns}},\r\nauthor = {Beck, Samuel and Doerr, Nina and Schmierer, Fabian and Sedlmair, Michael and Koch, Steffen},\r\nyear = {2023},\r\npublisher = {The Eurographics Association},\r\nISBN = {978-3-03868-220-2},\r\nDOI = {10.2312/evp.20231076}\r\n}","notes":"","funding":""},{"Title":"Visual Gaze Labeling for Augmented Reality Studies","Submission Target":"EuroVis","Date":"2023-06-12","Type":"Full Paper","First Author":"Seyda Öney","Other Authors":"Nelusa Pathmanathan, Michael Becher, Michael Sedlmair, Daniel Weiskopf, Kuno Kurzhals","Key (e.g. for file names)":"oeney2023visual","Publisher URL (official)":"https://doi.org/10.1111/cgf.14837","url2":"https://diglib.eg.org/xmlui/handle/10.1111/cgf14837","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://doi.org/10.1111/cgf.14837","Acknowledgements":"","Abstract":"Augmented Reality (AR) provides new ways for situated visualization and human-computer interaction in physical environments. Current evaluation procedures for AR applications rely primarily on questionnaires and interviews, providing qualitative means to assess usability and task solution strategies. Eye tracking extends these existing evaluation methodologies by providing indicators for visual attention to virtual and real elements in the environment. However, the analysis of viewing behavior, especially the comparison of multiple participants, is difficult to achieve in AR. Specifically, the definition of areas of interest (AOIs), which is often a prerequisite for such analysis, is cumbersome and tedious with existing approaches. To address this issue, we present a new visualization approach to define AOIs, label fixations, and investigate the resulting annotated scanpaths. Our approach utilizes automatic annotation of gaze on virtual objects and an image-based approach that also considers spatial context for the manual annotation of objects in the real world. Our results show, that with our approach, eye tracking data from AR scenes can be annotated and analyzed flexibly with respect to data aspects and annotation strategies.","bibtex":"@article {10.1111:cgf.14837,\r\njournal = {Computer Graphics Forum},\r\ntitle = {{Visual Gaze Labeling for Augmented Reality Studies}},\r\nauthor = {Öney, Seyda and Pathmanathan, Nelusa and Becher, Michael and Sedlmair, Michael and Weiskopf, Daniel and Kurzhals, Kuno},\r\nyear = {2023},\r\npublisher = {The Eurographics Association and John Wiley & Sons Ltd.},\r\nISSN = {1467-8659},\r\nDOI = {10.1111/cgf.14837}\r\n}","notes":"","funding":""},{"Title":"Been There, Seen That: Visualization of Movement and 3D Eye Tracking Data from Real-World Environments","Submission Target":"EuroVis","Date":"2023-06-12","Type":"Full Paper","First Author":"Nelusa Pathmanathan","Other Authors":"Seyda Öney, Michael Becher, Michael Sedlmair, Daniel Weiskopf, Kuno Kurzhals","Key (e.g. for file names)":"pathmanathan2023been","Publisher URL (official)":"https://doi.org/10.1111/cgf.14838","url2":"https://diglib.eg.org/handle/10.1111/cgf14838","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://doi.org/10.1111/cgf.14838","Acknowledgements":"","Abstract":"The distribution of visual attention can be evaluated using eye tracking, providing valuable insights into usability issues and interaction patterns. However, when used in real, augmented, and collaborative environments, new challenges arise that go beyond desktop scenarios and purely virtual environments. Toward addressing these challenges, we present a visualization technique that provides complementary views on the movement and eye tracking data recorded from multiple people in realworld environments. Our method is based on a space-time cube visualization and a linked 3D replay of recorded data. We showcase our approach with an experiment that examines how people investigate an artwork collection. The visualization provides insights into how people moved and inspected individual pictures in their spatial context over time. In contrast to existing methods, this analysis is possible for multiple participants without extensive annotation of areas of interest. Our technique was evaluated with a think-aloud experiment to investigate analysis strategies and an interview with domain experts to examine the applicability in other research fields.","bibtex":"@article {10.1111:cgf.14838,\r\njournal = {Computer Graphics Forum},\r\ntitle = {{Been There, Seen That: Visualization of Movement and 3D Eye Tracking Data from Real-World Environments}},\r\nauthor = {Pathmanathan, Nelusa and Öney, Seyda and Becher, Michael and Sedlmair, Michael and Weiskopf, Daniel and Kurzhals, Kuno},\r\nyear = {2023},\r\npublisher = {The Eurographics Association and John Wiley & Sons Ltd.},\r\nISSN = {1467-8659},\r\nDOI = {10.1111/cgf.14838}\r\n}","notes":"","funding":""},{"Title":"Reading Strategies for Graph Visualizations that Wrap Around in Torus Topology","Submission Target":"ETRA","Date":"2023-05-30","Type":"Full Paper","First Author":"Kun-Ting Chen","Other Authors":"Quynh Quang Ngo, Kuno Kurzhals, Kim Marriott, Tim Dwyer, Michael Sedlmair, Daniel Weiskopf","Key (e.g. for file names)":"chen2023reading","Publisher URL (official)":"https://doi.org/10.1145/3588015.3589841","url2":"https://dl.acm.org/doi/10.1145/3588015.3589841","PDF URL (public)":"https://arxiv.org/pdf/2303.17066.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We investigate reading strategies for node-link diagrams that wrap around the boundaries in a flattened torus topology by examining eye tracking data recorded in a previous controlled study. Prior work showed that torus drawing affords greater flexibility in clutter reduction than traditional node-link representations, but impedes link-and-path exploration tasks, while repeating tiles around boundaries aids comprehension. However, it remains unclear what strategies users apply in different wrapping settings. This is important for design implications for future work on more effective wrapped visualizations for network applications, and cyclic data that could benefit from wrapping. We perform visual-exploratory data analysis of gaze data, and conduct statistical tests derived from the patterns identified. Results show distinguishable gaze behaviors, with more visual glances and transitions between areas of interest in the non-replicated layout. Full-context has more successful visual searches than partial-context, but the gaze allocation indicates that the layout could be more space-efficient.","bibtex":"@inproceedings{10.1145/3588015.3589841,\r\nauthor = {Chen, Kun-Ting and Ngo, Quynh Quang and Kurzhals, Kuno and Marriott, Kim and Dwyer, Tim and Sedlmair, Michael and Weiskopf, Daniel},\r\ntitle = {Reading Strategies for Graph Visualizations That Wrap Around in Torus Topology},\r\nyear = {2023},\r\nisbn = {9798400701504},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3588015.3589841},\r\ndoi = {10.1145/3588015.3589841},\r\nabstract = {We investigate reading strategies for node-link diagrams that wrap around the boundaries in a flattened torus topology by examining eye tracking data recorded in a previous controlled study. Prior work showed that torus drawing affords greater flexibility in clutter reduction than traditional node-link representations, but impedes link-and-path exploration tasks, while repeating tiles around boundaries aids comprehension. However, it remains unclear what strategies users apply in different wrapping settings. This is important for design implications for future work on more effective wrapped visualizations for network applications, and cyclic data that could benefit from wrapping. We perform visual-exploratory data analysis of gaze data, and conduct statistical tests derived from the patterns identified. Results show distinguishable gaze behaviors, with more visual glances and transitions between areas of interest in the non-replicated layout. Full-context has more successful visual searches than partial-context, but the gaze allocation indicates that the layout could be more space-efficient.},\r\nbooktitle = {Proceedings of the 2023 Symposium on Eye Tracking Research and Applications},\r\narticleno = {67},\r\nnumpages = {7},\r\nkeywords = {area of interest, graph visualization, Eye tracking, scanpath analysis},\r\nlocation = {Tubingen, Germany},\r\nseries = {ETRA '23}\r\n}","notes":"","funding":""},{"Title":"Auxiliary Means to Improve Motion Guidance Memorability in Extended Reality","Submission Target":"VRW","Date":"2023-05-01","Type":"","First Author":"Patrick Gebhardt","Other Authors":"Maximilian Weiß, Pascal Huszár, Xingyao Yu, Alexander Achberger, Xiaobing Zhang, Michael Sedlmair","Key (e.g. for file names)":"gebhardt2023auxiliary","Publisher URL (official)":"https://doi.org/10.1109/VRW58643.2023.00187","url2":"https://ieeexplore.ieee.org/abstract/document/10108756","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"VR-based motion guidance systems can provide 3D movement instructions and real-time feedback for practicing movement without a live instructor. However, the precise visualization of movement paths or postures may be insufficient to learn a new motor skill, as they might make users too dependent and lead to poor performance when there is no guidance. In this paper, we propose to use enhanced error visualization, asymptotic path, increasing transparency, and haptic constraint to improve the memorability of motion guidance. Our study results indicated that adding an enhanced error feedback visualization helped the users with short-term retention.","bibtex":"@INPROCEEDINGS{10108756,\r\n  author={Gebhardt, Patrick and Weiß, Maximilian and Huszár, Pascal and Yu, Xingyao and Achberger, Alexander and Zhang, Xiaobing and Sedlmair, Michael},\r\n  booktitle={2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, \r\n  title={Auxiliary Means to Improve Motion Guidance Memorability in Extended Reality}, \r\n  year={2023},\r\n  pages={689-690},\r\n  doi={10.1109/VRW58643.2023.00187}}\r\n","notes":"","funding":""},{"Title":"VR, Gaze, and Visual Impairment: An Exploratory Study of the Perception of Eye Contact across different Sensory Modalities for People with Visual Impairments in Virtual Reality","Submission Target":"CHI","Date":"2023-04-19","Type":"Extended Abstract","First Author":"Markus Wieland","Other Authors":"Michael Sedlmair, Tonja-Katrin Machulla\r\n\r\n","Key (e.g. for file names)":"wieland2023vr","Publisher URL (official)":"https://doi.org/10.1145/3544549.3585726","url2":"https://dl.acm.org/doi/abs/10.1145/3544549.3585726","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=D9Z9058RyxQ","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"As social virtual reality (VR) becomes more popular, avatars are being designed with realistic behaviors incorporating non-verbal cues like eye contact. However, perceiving eye contact during a onversation can be challenging for people with visual impairments. VR presents an opportunity to display eye contact cues in alternative ways, making them perceivable for people with visual impairments. We performed an exploratory study to gain initial insights on designing eye contact cues for people with visual impairments, including a focus group for a deeper understanding of the topic. We implemented eye contact cues via visual, auditory, and tactile sensory modalities in VR and tested these approaches with eleven participants with visual impairments and collected qualitative feedback. The results show that visual cues indicating the gaze direction were preferred, but auditory and tactile cues were also prevalent as they do not superimpose additional visual information. ","bibtex":"@inproceedings{10.1145/3544549.3585726,\r\nauthor = {Wieland, Markus and Sedlmair, Michael and Machulla, Tonja-Katrin},\r\ntitle = {VR, Gaze, and Visual Impairment: An Exploratory Study of the Perception of Eye Contact across Different Sensory Modalities for People with Visual Impairments in Virtual Reality},\r\nyear = {2023},\r\nisbn = {9781450394222},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3544549.3585726},\r\ndoi = {10.1145/3544549.3585726},\r\nabstract = {As social virtual reality (VR) becomes more popular, avatars are being designed with realistic behaviors incorporating non-verbal cues like eye contact. However, perceiving eye contact during a conversation can be challenging for people with visual impairments. VR presents an opportunity to display eye contact cues in alternative ways, making them perceivable for people with visual impairments. We performed an exploratory study to gain initial insights on designing eye contact cues for people with visual impairments, including a focus group for a deeper understanding of the topic. We implemented eye contact cues via visual, auditory, and tactile sensory modalities in VR and tested these approaches with eleven participants with visual impairments and collected qualitative feedback. The results show that visual cues indicating the gaze direction were preferred, but auditory and tactile cues were also prevalent as they do not superimpose additional visual information.},\r\nbooktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},\r\narticleno = {313},\r\nnumpages = {6},\r\nkeywords = {eye contact, assistive technology, visual impairment, social virtual reality},\r\nlocation = {Hamburg, Germany},\r\nseries = {CHI EA '23}\r\n}","notes":"","funding":""},{"Title":"\"In Your Face!\": Visualizing Fitness Tracker Data in Augmented Reality","Submission Target":"CHI","Date":"2023-04-19","Type":"Extended Abstract","First Author":"Sebastian Rigling","Other Authors":"Xingyao Yu, Michael Sedlmair","Key (e.g. for file names)":"rigling2023inyourface","Publisher URL (official)":"https://doi.org/10.1145/3544549.3585912","url2":"https://dl.acm.org/doi/abs/10.1145/3544549.3585912","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3544549.3585912","Video":"https://www.youtube.com/watch?v=2l5ImWh8uJY","Video2":"","Supplemental":"https://dl.acm.org/doi/10.1145/3544549.3585912","Acknowledgements":"","Abstract":"The benefits of augmented reality (AR) have been demonstrated in both medicine and fitness, while its application in areas where these two fields overlap has been barely explored. We argue that AR opens up new opportunities to interact with, understand and share personal health data. To this end, we developed an app prototype that uses a Snapchat-like face filter to visualize personal health data from a fitness tracker in AR. We tested this prototype in two pilot studies and found that AR does have potential in this type of application. We suggest that AR cannot replace the current interfaces of smartwatches and mobile apps, but it can pick up where current technology falls short in creating intrinsic motivation and personal health awareness. We also provide ideas for future work in this direction.","bibtex":"@inproceedings{10.1145/3544549.3585912,\r\nauthor = {Rigling, Sebastian and Yu, Xingyao and Sedlmair, Michael},\r\ntitle = {“In Your Face!”: Visualizing Fitness Tracker Data in Augmented Reality},\r\nyear = {2023},\r\nisbn = {9781450394222},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3544549.3585912},\r\ndoi = {10.1145/3544549.3585912},\r\nabstract = {The benefits of augmented reality (AR) have been demonstrated in both medicine and fitness, while its application in areas where these two fields overlap has been barely explored. We argue that AR opens up new opportunities to interact with, understand and share personal health data. To this end, we developed an app prototype that uses a Snapchat-like face filter to visualize personal health data from a fitness tracker in AR. We tested this prototype in two pilot studies and found that AR does have potential in this type of application. We suggest that AR cannot replace the current interfaces of smartwatches and mobile apps, but it can pick up where current technology falls short in creating intrinsic motivation and personal health awareness. We also provide ideas for future work in this direction.},\r\nbooktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},\r\narticleno = {176},\r\nnumpages = {7},\r\nkeywords = {fitness tracker, health, visualization, augmented reality},\r\nlocation = {Hamburg, Germany},\r\nseries = {CHI EA '23}\r\n}","notes":"","funding":""},{"Title":"Bees, Birds and Butterflies: Investigating the Influence of Distractors on Visual Attention Guidance Techniques","Submission Target":"CHI","Date":"2023-04-19","Type":"Extended Abstract","First Author":"Nina Doerr","Other Authors":"Katrin Angerbauer, Melissa Reinelt, Michael Sedlmair","Key (e.g. for file names)":"doerr2023bees","Publisher URL (official)":"https://doi.org/10.1145/3544549.3585816","url2":"https://dl.acm.org/doi/abs/10.1145/3544549.3585816","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=17ALkNxm1mI","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Visual attention guidance methods direct the viewer’s gaze in immersive environments by visually highlighting elements of interest. The highlighting can be done, for instance, by adding a colored circle around elements, adding animated swarms (HiveFive), or removing objects from one eye in a stereoscopic display (Deadeye). We contribute a controlled user experiment (N=30) comparing these three techniques under the influence of visual distractors, such as bees flying by. Our results show that Circle and HiveFive performed best in terms of task performance and qualitative feedback, and were largely robust against different levels of distractions. Furthermore, we discovered a high mental demand for Deadeye.","bibtex":"@inproceedings{10.1145/3544549.3585816,\r\nauthor = {Doerr, Nina and Angerbauer, Katrin and Reinelt, Melissa and Sedlmair, Michael},\r\ntitle = {Bees, Birds and Butterflies: Investigating the Influence of Distractors on Visual Attention Guidance Techniques},\r\nyear = {2023},\r\nisbn = {9781450394222},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3544549.3585816},\r\ndoi = {10.1145/3544549.3585816},\r\nabstract = {Visual attention guidance methods direct the viewer’s gaze in immersive environments by visually highlighting elements of interest. The highlighting can be done, for instance, by adding a colored circle around elements, adding animated swarms (HiveFive), or removing objects from one eye in a stereoscopic display (Deadeye). We contribute a controlled user experiment (N=30) comparing these three techniques under the influence of visual distractors, such as bees flying by. Our results show that Circle and HiveFive performed best in terms of task performance and qualitative feedback, and were largely robust against different levels of distractions. Furthermore, we discovered a high mental demand for Deadeye.},\r\nbooktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},\r\narticleno = {51},\r\nnumpages = {7},\r\nkeywords = {perception, visual attention, attention guidance, virtual reality},\r\nlocation = {Hamburg, Germany},\r\nseries = {CHI EA '23}\r\n}","notes":"","funding":""},{"Title":"Deimos: A Grammar of Dynamic Embodied Immersive Visualisation Morphs and Transitions","Submission Target":"CHI","Date":"2023-04-19","Type":"Full Paper","First Author":"Benjamin Lee","Other Authors":"Arvind Satyanarayan, Maxime Cordeil, Arnaud Prouzeau, Bernhard Jenny, Tim Dwyer","Key (e.g. for file names)":"lee2023deimos","Publisher URL (official)":"https://doi.org/10.1145/3544548.3580754","url2":"https://dl.acm.org/doi/abs/10.1145/3544548.3580754","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=L9Ngzh1w7nM","Video2":"","Supplemental":"https://dl.acm.org/doi/abs/10.1145/3544548.3580754","Acknowledgements":"","Abstract":"We present Deimos, a grammar for specifying dynamic embodied immersive visualisation morphs and transitions. A morph is a collection of animated transitions that are dynamically applied to immersive visualisations at runtime and is conceptually modelled as a state machine. It is comprised of state, transition, and signal specifications. States in a morph are used to generate animation keyframes, with transitions connecting two states together. A transition is controlled by signals, which are composable data streams that can be used to enable embodied interaction techniques. Morphs allow immersive representations of data to transform and change shape through user interaction, facilitating the embodied cognition process. We demonstrate the expressivity of Deimos in an example gallery and evaluate its usability in an expert user study of six immersive analytics researchers. Participants found the grammar to be powerful and expressive, and showed interest in drawing upon Deimos’ concepts and ideas in their own research.","bibtex":"@inproceedings{10.1145/3544548.3580754,\r\nauthor = {Lee, Benjamin and Satyanarayan, Arvind and Cordeil, Maxime and Prouzeau, Arnaud and Jenny, Bernhard and Dwyer, Tim},\r\ntitle = {Deimos: A Grammar of Dynamic Embodied Immersive Visualisation Morphs and Transitions},\r\nyear = {2023},\r\nisbn = {9781450394215},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3544548.3580754},\r\ndoi = {10.1145/3544548.3580754},\r\nabstract = {We present Deimos, a grammar for specifying dynamic embodied immersive visualisation morphs and transitions. A morph is a collection of animated transitions that are dynamically applied to immersive visualisations at runtime and is conceptually modelled as a state machine. It is comprised of state, transition, and signal specifications. States in a morph are used to generate animation keyframes, with transitions connecting two states together. A transition is controlled by signals, which are composable data streams that can be used to enable embodied interaction techniques. Morphs allow immersive representations of data to transform and change shape through user interaction, facilitating the embodied cognition process. We demonstrate the expressivity of Deimos in an example gallery and evaluate its usability in an expert user study of six immersive analytics researchers. Participants found the grammar to be powerful and expressive, and showed interest in drawing upon Deimos’ concepts and ideas in their own research.},\r\nbooktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},\r\narticleno = {810},\r\nnumpages = {18},\r\nkeywords = {grammar, user study, data visualisation, Immersive Analytics, embodied interaction, animated transitions},\r\nlocation = {Hamburg, Germany},\r\nseries = {CHI '23}\r\n}","notes":"","funding":""},{"Title":"Configuring Augmented Reality Users: Analysing YouTube Commercials to Understand Industry Expectations","Submission Target":"Behaviour & Information Technology","Date":"2023-01-09","Type":"","First Author":"Ann-Kathrin Wortmeier","Other Authors":"Aimee Sousa Calepso, Cordula Kropp, Michael Sedlmair, Daniel Weiskopf","Key (e.g. for file names)":"wortmeier2023configuring","Publisher URL (official)":"https://doi.org/10.1080/0144929X.2022.2163693","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://www.tandfonline.com/doi/figure/10.1080/0144929X.2022.2163693?scroll=top&needAccess=true&role=tab","Acknowledgements":"","Abstract":"Commercial videos are often used to familiarise potential buyers and users with new technologies and their possibilities. In addition, presenting visions of future applications is a way to configure users and define social worlds of technology use. We analyse 30 YouTube videos featuring augmented reality (AR) devices in industrial manufacturing and construction, to explore how these commercial videos situate AR technology and future users by showcasing techno-euphoric promises and imagined use cases. With a video analysis based on Grounded Theory and Situational Analysis, we untangle the promises of AR for manufacturing and construction work; second, we present two prevailing configurations of AR users: ‘experts in situ’ and ‘smart dummies’; and third, we discuss how YouTube videos put forward developmental expectations. In addition, we identify discrepancies between expectations and foreseeable requirements in construction work. Finally, our research could contribute to a more holistic understanding of workplaces and socially robust AR applications.","bibtex":"@article{doi:10.1080/0144929X.2022.2163693,\r\n  author    = {Ann-Kathrin Wortmeier and Aimée Sousa Calepso and Cordula Kropp and Michael Sedlmair and Daniel Weiskopf},\r\n  title     = {Configuring augmented reality users: analysing YouTube commercials to understand industry expectations},\r\n  journal   = {Behaviour \\& Information Technology},\r\n  pages     = {1-16},\r\n  year      = {2023},\r\n  publisher = {Taylor & Francis},\r\n  doi       = {10.1080/0144929X.2022.2163693},\r\n  url       = {https://doi.org/10.1080/0144929X.2022.2163693},\r\n  eprint    = {https://doi.org/10.1080/0144929X.2022.2163693},\r\n  abstract  = {Commercial videos are often used to familiarise potential buyers and users with new technologies and their possibilities. In addition, presenting visions of future applications is a way to configure users and define social worlds of technology use. We analyse 30 YouTube videos featuring augmented reality (AR) devices in industrial manufacturing and construction, to explore how these commercial videos situate AR technology and future users by showcasing techno-euphoric promises and imagined use cases. With a video analysis based on Grounded Theory and Situational Analysis, we untangle the promises of AR for manufacturing and construction work; second, we present two prevailing configurations of AR users: ‘experts in situ’ and ‘smart dummies’; and third, we discuss how YouTube videos put forward developmental expectations. In addition, we identify discrepancies between expectations and foreseeable requirements in construction work. Finally, our research could contribute to a more holistic understanding of workplaces and socially robust AR applications.}\r\n}\r\n","notes":"","funding":""},{"Title":"Study on the Influence of Upper Limb Representations and Haptic Feedback in Virtual Reality","Submission Target":"ISMAR-adjunct","Date":"2022-12-15","Type":"","First Author":"Natalie Hube","Other Authors":"Alexander Achberger, Philipp Liepert, Jonas Vogelsang, Kresimir Vidackovic, Michael Sedlmair","Key (e.g. for file names)":"hube2022study","Publisher URL (official)":"https://doi.org/10.1109/ISMAR-Adjunct57072.2022.00172","url2":"","PDF URL (public)":"https://ieeexplore.ieee.org/iel7/9973799/9974160/09974563.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Visual representations are used in various digitization processes to display objects in immersive environments. In industrial environments, the self-representation can be very helpful when reviewing 3D models to conform realistic proportions, particularly in combi-nation with haptic feedback that goes beyond vibrating controllers. For instance, haptic feedback in combination with a virtual representation supports working on use cases where collision is an important part to maintain data quality. To understand the dependency of both, we conducted a pilot study with 15 users from the automotive sector to examine the influence of upper limb representations on haptic feedback in Virtual Reality. Each participant was assigned with one of three upper limb representations which was used in different scenarios using haptic feedback devices. Overall, we found that the realistic arm representation was rated highest in terms of perceived realism and achieved the best task performance.","bibtex":"@INPROCEEDINGS{9974563,\r\n  author={Hube, Natalie and Achberger, Alexander and Liepert, Philipp and Vogelsang, Jonas and Vidačković, Krešimir and Sedlmair, Michael},\r\n  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, \r\n  title={Study on the Influence of Upper Limb Representations and Haptic Feedback in Virtual Reality}, \r\n  year={2022},\r\n  pages={802-807},\r\n  doi={10.1109/ISMAR-Adjunct57072.2022.00172}}\r\n","notes":"","funding":""},{"Title":"Towards reducing visual workload in surgical navigation: proof-of-concept of an augmented reality haptic guidance system","Submission Target":"Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization","Date":"2022-12-05","Type":"Full Paper","First Author":"Gesiren Zhang","Other Authors":"Jan Ulrich Bartels, Alejandro Martin-Gomez, Mehran Armand","Key (e.g. for file names)":"zhang2022towards","Publisher URL (official)":"https://doi.org/10.1080/21681163.2022.2152372","url2":"https://www.tandfonline.com/doi/full/10.1080/21681163.2022.2152372","PDF URL (public)":"https://www.tandfonline.com/doi/pdf/10.1080/21681163.2022.2152372","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The integration of planning and navigation capabilities into the operating room has enabled surgeons take on more precise procedures. Traditionally, planning and navigation information is presented using monitors in the surgical theatre. But the monitors force the surgeon to frequently look away from the surgical area. Augmented reality technologies have enabled surgeons to visualise navigation information in-situ. However, burdening the visual field with additional information can be distracting. We propose integrating haptic feedback into a surgical tool handle to enable surgical guidance capabilities. This property reduces the amount of visual information, freeing surgeons to maintain visual attention over the patient and the surgical site. To investigate the feasibility of this guidance paradigm we conducted a pilot study with six subjects. Participants traced paths, pinpointed locations and matched alignments with a mock surgical tool featuring a novel haptic handle. We collected quantitative data, tracking user’s accuracy and time to completion as well as subjective cognitive load. Our results show that haptic feedback can guide participants using a tool to sub-millimetre and sub-degree accuracy with only little training. Participants were able to match a location with an average error of 0.82mm, desired pivot alignments with an average error of 0.83° and desired rotations to 0.46°.","bibtex":"@article{doi:10.1080/21681163.2022.2152372,\r\nauthor = {Gesiren Zhang, Jan Bartels, Alejandro Martin-Gomez and Mehran Armand},\r\ntitle = {Towards reducing visual workload in surgical navigation: proof-of-concept of an augmented reality haptic guidance system},\r\njournal = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging \\& Visualization},\r\nvolume = {11},\r\nnumber = {4},\r\npages = {1073--1080},\r\nyear = {2023},\r\npublisher = {Taylor \\& Francis},\r\ndoi = {10.1080/21681163.2022.2152372},\r\nURL = {https://doi.org/10.1080/21681163.2022.2152372},\r\n}","notes":"","funding":""},{"Title":"Visualization for AI-Assisted Composing","Submission Target":"ISMIR","Date":"2022-12-04","Type":"Full Paper","First Author":"Simeon Rau, Frank Heyen","Other Authors":"Stefan Wagner, Michael Sedlmair","Key (e.g. for file names)":"rau2022visualization","Publisher URL (official)":"https://doi.org/10.5281/zenodo.7316618","url2":"https://ismir2022program.ismir.net/poster_217.html","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://github.com/visvar/vis-ai-comp","Acknowledgements":"This work was funded by the Cyber Valley Research Fund\r\nand the Artificial Intelligence Software Academy (AISA).","Abstract":"We propose a visual approach for interactive, AI-assisted composition that serves as a compromise between fully automatic and fully manual composition. Instead of generating a whole piece, the AI takes on the role of an assistant that generates short melodies for the composer to choose from and adapt. In an iterative process, the composer queries the AI for continuations or alternative fill-ins, chooses a suggestion, and adds it to the piece. As listening to many suggestions would take time, we explore different ways to visualize them, to allow the composer to focus on the most interesting-looking melodies. We also present the results of a qualitative evaluation with five composers. ","bibtex":"@inproceedings{simeon_rau_2022_7316618,\r\n  author       = {Simeon Rau and\r\n                  Frank Heyen and\r\n                  Stefan Wagner and\r\n                  Michael Sedlmair},\r\n  title        = {Visualization for AI-Assisted Composing},\r\n  booktitle    = {{Proceedings of the 23rd International Society for \r\n                   Music Information Retrieval Conference (ISMIR)}},\r\n  year         = {2022},\r\n  pages        = {151-159},\r\n  publisher    = {ISMIR},\r\n  address      = {Bengaluru, India},\r\n  month        = dec,\r\n  venue        = {Bengaluru, India},\r\n  doi          = {10.5281/zenodo.7316618},\r\n  url          = {https://doi.org/10.5281/zenodo.7316618}\r\n}","notes":"Best Paper Award Nomination","funding":""},{"Title":"A Web-Based MIDI Controller for Music Live Coding","Submission Target":"ISMIR","Date":"2022-12-04","Type":"LBD Poster","First Author":"Frank Heyen","Other Authors":"Dilara Aygün, Michael Sedlmair","Key (e.g. for file names)":"heyen2022postermidicontroller","Publisher URL (official)":"https://ismir2022program.ismir.net/lbd_379.html","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://github.com/visvar/sonic-pi-controller","Acknowledgements":"This work was funded by the Cyber Valley Research Fund.","Abstract":"We contribute an interactive visual frontend to live coding environments, which allows live coders and performers to influence the behavior of their code more quickly and efficiently. Users can trigger actions and change parameters via instruments, buttons, and sliders, instead of only inside the code. For instance, toggling a loop or controlling a fading effect through mouse or touch interaction on a screen is faster than editing code. While this kind of control has already been possible with hardware MIDI devices, we provide a more accessible, easy-to-use, and customizable alternative that only requires a web browser. With examples, we show how users perform live-coded music faster and more easily with our design compared to using pure code.","bibtex":"@misc{heyen2022midicontroller,\r\n      title={A Web-Based MIDI Controller for Music Live Coding}, \r\n      author={Frank Heyen and Dilara Aygün and Michael Sedlmair},\r\n      howpublished={International Conference on Music Information Retrieval ({ISMIR}) Late-breaking Demo},\r\n      year={2022},\r\n}","notes":"","funding":""},{"Title":"Augmented Reality Visualization for Musical Instrument Learning","Submission Target":"ISMIR","Date":"2022-12-04","Type":"LBD Poster","First Author":"Frank Heyen","Other Authors":"Michael Sedlmair","Key (e.g. for file names)":"heyen2022augmented","Publisher URL (official)":"https://ismir2022program.ismir.net/lbd_376.html","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"This work was funded by the Cyber Valley Research Fund.","Abstract":"We contribute two design studies for augmented reality visualizations that support learning musical instruments. First, we designed simple, glanceable encodings for drum kits, which we display through a projector. As second instrument, we chose guitar and designed visualizations to be displayed either on a screen as an augmented mirror or an an optical see-through AR headset. These modalities allow us to also show information around the instrument and in 3D. We evaluated our prototypes through case studies and our results demonstrate the general effectivity and revealed design-related and technical limitations.","bibtex":"@misc{heyen2022augmented,\r\n      title={Augmented Reality Visualization for Musical Instrument Learning}, \r\n      author={Frank Heyen and Michael Sedlmair},\r\n      howpublished={International Conference on Music Information Retrieval ({ISMIR}) Late-breaking Demo},\r\n      year={2022},\r\n}","notes":"","funding":""},{"Title":"Towards Immersive Collaborative Sensemaking","Submission Target":"ISS","Date":"2022-11-14","Type":"","First Author":"Ying Yang","Other Authors":"Tim Dwyer, Michael Wybrow, Benjamin Lee, Maxime Cordeil, Mark Billinghurst, Bruce H Thomas","Key (e.g. for file names)":"yang2022collaborative","Publisher URL (official)":"https://doi.org/10.1145/3567741","url2":"https://dl.acm.org/doi/abs/10.1145/3567741","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=8AxNxvPAdYk","Video2":"","Supplemental":"https://dl.acm.org/doi/abs/10.1145/3567741#sec-supp","Acknowledgements":"","Abstract":"When collaborating face-to-face, people commonly use the surfaces and spaces around them to perform sensemaking tasks, such as spatially organising documents, notes or images. However, when people collaborate remotely using desktop interfaces they no longer feel like they are sharing the same space. This limitation may be overcome through collaboration in immersive environments, which simulate the physical in-person experience. In this paper, we report on a between-groups study comparing collaborations on image organisation tasks, in an immersive Virtual Reality (VR) environment to more conventional desktop conferencing. Collecting data from 40 subjects in groups of four, we measured task performance, user behaviours, collaboration engagement and awareness. Overall, the VR and desktop interface resulted in similar speed, accuracy and social presence rating, but we observed more conversations and interaction with objects, and more equal contributions to the interaction from participants within groups in VR. We also identified differences in coordination and collaborative awareness behaviours between VR and desktop platforms. We report on a set of systematic measures for assessing VR collaborative experience and a new analysis tool that we have developed to capture user behaviours in collaborative setting. Finally, we provide design considerations and directions for future work.","bibtex":"@article{10.1145/3567741,\r\nauthor = {Yang, Ying and Dwyer, Tim and Wybrow, Michael and Lee, Benjamin and Cordeil, Maxime and Billinghurst, Mark and Thomas, Bruce H.},\r\ntitle = {Towards Immersive Collaborative Sensemaking},\r\nyear = {2022},\r\nissue_date = {December 2022},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nvolume = {6},\r\nnumber = {ISS},\r\nurl = {https://doi.org/10.1145/3567741},\r\ndoi = {10.1145/3567741},\r\nabstract = {When collaborating face-to-face, people commonly use the surfaces and spaces around them to perform sensemaking tasks, such as spatially organising documents, notes or images. However, when people collaborate remotely using desktop interfaces they no longer feel like they are sharing the same space. This limitation may be overcome through collaboration in immersive environments, which simulate the physical in-person experience. In this paper, we report on a between-groups study comparing collaborations on image organisation tasks, in an immersive Virtual Reality (VR) environment to more conventional desktop conferencing. Collecting data from 40 subjects in groups of four, we measured task performance, user behaviours, collaboration engagement and awareness. Overall, the VR and desktop interface resulted in similar speed, accuracy and social presence rating, but we observed more conversations and interaction with objects, and more equal contributions to the interaction from participants within groups in VR. We also identified differences in coordination and collaborative awareness behaviours between VR and desktop platforms. We report on a set of systematic measures for assessing VR collaborative experience and a new analysis tool that we have developed to capture user behaviours in collaborative setting. Finally, we provide design considerations and directions for future work.},\r\njournal = {Proc. ACM Hum.-Comput. Interact.},\r\nmonth = {nov},\r\narticleno = {588},\r\nnumpages = {25},\r\nkeywords = {Virtual Reality, Collaborative Sensemaking}\r\n}","notes":"","funding":""},{"Title":"MolecuSense: Using Force-Feedback Gloves for Creating and Interacting with Ball-and-Stick Molecules in VR","Submission Target":"VINCI","Date":"2022-10-31","Type":"Short Paper","First Author":"Patrick Gebhardt","Other Authors":"Xingyao Yu, Andreas Köhn, Michael Sedlmair","Key (e.g. for file names)":"gebhardt2022molecusense","Publisher URL (official)":"https://doi.org/10.1145/3554944.3554956","url2":"","PDF URL (public)":"https://arxiv.org/pdf/2203.09577.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We contribute MolecuSense, a virtual version of a physical molecule construction kit, based on visualization in Virtual Reality (VR) and interaction with force-feedback gloves. Targeting at chemistry education, our goal is to make virtual molecule structures more tangible. Results of an initial user study indicate that the VR molecular construction kit was positively received. Compared to a physical construction kit, the VR molecular construction kit is on the same level in terms of natural interaction. Besides, it fosters the typical digital advantages though, such as saving, exporting, and sharing of molecules. Feedback from the study participants has also revealed potential future avenues for tangible molecule visualizations. ","bibtex":"@inproceedings{10.1145/3554944.3554956,\r\n  author = {Gebhardt, Patrick and Yu, Xingyao and K\\\"{o}hn, Andreas and Sedlmair, Michael},\r\n  title = {{MolecuSense}: Using Force-Feedback Gloves for Creating and Interacting with Ball-and-Stick Molecules in {VR}},\r\n  year = {2022},\r\n  isbn = {9781450398060},\r\n  publisher = {ACM},\r\n  url = {https://doi.org/10.1145/3554944.3554956},\r\n  doi = {10.1145/3554944.3554956},\r\n  abstract = {We contribute MolecuSense, a virtual version of a physical molecule construction kit, based on visualization in Virtual Reality (VR) and interaction with force-feedback gloves. Targeting at chemistry education, our goal is to make virtual molecule structures more tangible. Results of an initial user study indicate that the VR molecular construction kit was positively received. Compared to a physical construction kit, the VR molecular construction kit is on the same level in terms of natural interaction. Besides, it fosters the typical digital advantages though, such as saving, exporting, and sharing of molecules. Feedback from the study participants has also revealed potential future avenues for tangible molecule visualizations.},\r\n  booktitle = {Proceedings of the 15th International Symposium on Visual Information Communication and Interaction (VINCI)},\r\n  articleno = {15},\r\n  numpages = {5},\r\n  keywords = {human-computer interaction, Virtual reality, digital gloves},\r\n  series = {VINCI '22}\r\n}","notes":"","funding":""},{"Title":"Toward Inclusion and Accessibility in Visualization Research: Speculations on Challenges, Solution Strategies, and Calls for Action (Position Paper)","Submission Target":"BELIV","Date":"2022-10-17","Type":"Workshop Paper","First Author":"Katrin Angerbauer","Other Authors":"Michael Sedlmair","Key (e.g. for file names)":"angerbauer2022toward","Publisher URL (official)":"https://doi.org/10.1109/BELIV57783.2022.00007","url2":"https://arxiv.org/abs/2209.05224","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Inclusion and accessibility in visualization research have gained increasing attention in recent years. However, many challenges still remain to be solved on the road toward a more inclusive, shared-experience-driven visualization design and evaluation process. In this position paper, we discuss challenges and speculate about potential solutions, based on related work, our own research, as well as personal experiences. The goal of this paper is to start discussions on the role of accessibility and inclusion in visualization design and evaluation.","bibtex":"@INPROCEEDINGS{9978448,\r\n  author={Angerbauer, Katrin and Sedlmair, Michael},\r\n  booktitle={2022 IEEE Evaluation and Beyond - Methodological Approaches for Visualization (BELIV)}, \r\n  title={Toward Inclusion and Accessibility in Visualization Research: Speculations on Challenges, Solution Strategies, and Calls for Action (Position Paper)}, \r\n  year={2022},\r\n  pages={20-27},\r\n  doi={10.1109/BELIV57783.2022.00007}}\r\n","notes":"","funding":""},{"Title":"Not As Easy As You Think - Experiences and Lessons Learnt from Creating a Visualization Image Typology ","Submission Target":"arXiv","Date":"2022-10-12","Type":"Preprint","First Author":"Jian Chen","Other Authors":"Petra Isenberg, Robert S Laramee, Tobias Isenberg, Michael Sedlmair, Torsten Mōller, Han-Wei Shen","Key (e.g. for file names)":"chen2022not","Publisher URL (official)":"https://doi.org/10.48550/arXiv.2209.07533","url2":"https://hal.inria.fr/hal-03812031/","PDF URL (public)":"https://arxiv.org/pdf/2209.07533","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present and discuss the results of a two-year qualitative analysis of images published in IEEE Visualization (VIS) papers. Specifically, we derive a typology of 13 visualization image types, coded to distinguish visualizations and several image characteristics. The categorization process required much more time and was more difficult than we initially thought. The resulting typology and image analysis may serve a number of purposes: to study the evolution of the community and its research output over time, to facilitate the categorization of visualization images for the purpose of teaching, to identify visual designs for evaluation purposes, or to enable progress towards standardization in visualization. In addition to the typology and image characterization, we provide a dataset of 6,833 tagged images and an online tool that can be used to explore and analyze the large set of tagged images. We thus facilitate a discussion of the diverse visualizations used and how they are published and communicated in our community.","bibtex":"@misc{https://doi.org/10.48550/arxiv.2209.07533,\r\n  doi = {10.48550/ARXIV.2209.07533},\r\n    url = {https://arxiv.org/abs/2209.07533},\r\n    author = {Chen, Jian and Isenberg, Petra and Laramee, Robert S. and Isenberg, Tobias and Sedlmair, Michael and Moeller, Torsten and Shen, Han-Wei},\r\n    keywords = {Graphics (cs.GR), Multimedia (cs.MM), FOS: Computer and information sciences, FOS: Computer and information sciences},\r\n    title = {Not As Easy As You Think -- Experiences and Lessons Learnt from Trying to Create a Bottom-Up Visualization Image Typology},\r\n    publisher = {arXiv},\r\n    year = {2022},\r\n    copyright = {Creative Commons Attribution 4.0 International}\r\n}\r\n","notes":"","funding":""},{"Title":"Scalability in Visualization","Submission Target":"TVCG","Date":"2022-10-12","Type":"Early Access","First Author":"Gaëlle Richer","Other Authors":"Alexis Pister, Moataz Abdelaal, Jean-Daniel Fekete, Michael Sedlmair, Daniel Weiskopf","Key (e.g. for file names)":"richer2022scalability","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2022.3231230","url2":"https://ieeexplore.ieee.org/document/10003102","PDF URL (public)":"https://arxiv.org/pdf/2210.06562","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We introduce a conceptual model for scalability designed for visualization research. With this model, we systematically analyze over 120 visualization publications from 1990-2020 to characterize the different notions of scalability in these works. While many papers have addressed scalability issues, our survey identifies a lack of consistency in the use of the term in the visualization research community. We address this issue by introducing a consistent terminology meant to help visualization researchers better characterize the scalability aspects in their research. It also helps in providing multiple methods for supporting the claim that a work is \"scalable\". Our model is centered around an effort function with inputs and outputs. The inputs are the problem size and resources, whereas the outputs are the actual efforts, for instance, in terms of computational run time or visual clutter. We select representative examples to illustrate different approaches and facets of what scalability can mean in visualization literature. Finally, targeting the diverse crowd of visualization researchers without a scalability tradition, we provide a set of recommendations for how scalability can be presented in a clear and consistent way to improve fair comparison between visualization techniques and systems and foster reproducibility. ","bibtex":"@misc{https://doi.org/10.48550/arxiv.2210.06562,\r\n  doi = {10.48550/ARXIV.2210.06562},\r\n    url = {https://arxiv.org/abs/2210.06562},\r\n    author = {Richer, Gaëlle and Pister, Alexis and Abdelaal, Moataz and Fekete, Jean-Daniel and Sedlmair, Michael and Weiskopf, Daniel},\r\n    keywords = {Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences, H.5.2},\r\n    title = {Scalability in Visualization},\r\n    publisher = {arXiv},\r\n    year = {2022},\r\n    copyright = {Creative Commons Attribution 4.0 International}\r\n}\r\n","notes":"","funding":""},{"Title":"Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations","Submission Target":"TVCG","Date":"2022-10-03","Type":"Full Paper","First Author":"Moataz Abdelaal","Other Authors":"Nathan D. Schiele, Katrin Angerbauer, Kuno Kurzhals, Michael Sedlmair, Daniel Weiskopf","Key (e.g. for file names)":"abdelaal2022comparative","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2022.3209427","url2":"https://dl.acm.org/doi/abs/10.1145/3554944.3554970","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"This work investigates and compares the performance of node-link diagrams, adjacency matrices, and bipartite layouts for visualizing networks. In a crowd-sourced user study ( n=150 ), we measure the task accuracy and completion time of the three representations for different network classes and properties. In contrast to the literature, which covers mostly topology-based tasks (e.g., path finding) in small datasets, we mainly focus on overview tasks for large and directed networks. We consider three overview tasks on networks with 500 nodes: (T1) network class identification, (T2) cluster detection, and (T3) network density estimation, and two detailed tasks: (T4) node in-degree vs. out-degree and (T5) representation mapping, on networks with 50 and 20 nodes, respectively. Our results show that bipartite layouts are beneficial for revealing the overall network structure, while adjacency matrices are most reliable across the different tasks.","bibtex":"@ARTICLE{9908291,\r\n  author={Abdelaal, Moataz and Schiele, Nathan D. and Angerbauer, Katrin and Kurzhals, Kuno and Sedlmair, Michael and Weiskopf, Daniel},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Comparative Evaluation of Bipartite, Node-Link, and Matrix-Based Network Representations}, \r\n  year={2023},\r\n  volume={29},\r\n  number={1},\r\n  pages={896-906},\r\n  doi={10.1109/TVCG.2022.3209427}}\r\n","notes":"","funding":""},{"Title":"Predicting User Preferences of Dimensionality Reduction Embedding Quality","Submission Target":"TVCG","Date":"2022-09-27","Type":"Full Paper","First Author":"Cristina Morariu","Other Authors":"Adrien Bibal, Rene Cutura, Benoit Frenay, Michael Sedlmair","Key (e.g. for file names)":"morariu2022predicting","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2022.3209449","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://doi.org/10.1109/TVCG.2022.3209449/mm1","Acknowledgements":"Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (Project A08).","Abstract":"A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional hyper-parametrization (e.g., t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D embeddings is usually qualitatively decided, by setting embeddings side-by-side and letting human judgment decide which embedding is the best. In this work, we propose a quantitative way of evaluating embeddings, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select “good” and “misleading” views between scatterplots of low-dimensional embeddings of image datasets, simulating the way people usually select embeddings. We use the study data as labels for a set of quality metrics for a supervised machine learning model whose purpose is to discover and quantify what exactly people are looking for when deciding between embeddings. With the model as a proxy for human judgments, we use it to rank embeddings on new datasets, explain why they are relevant, and quantify the degree of subjectivity when people select preferred embeddings.","bibtex":"@article{morariu2022predicting,\r\n  title={{Predicting User Preferences of Dimensionality Reduction Embedding Quality}},\r\n  author={Morariu, Cristina and Bibal, Adrien and Cutura, Rene and Frenay, Benoit and Sedlmair, Michael},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics (TVCG)},\r\n  year={2022},\r\n  publisher={IEEE},\r\n  pages={1-11},\r\n  doi={10.1109/TVCG.2022.3209449}}\r\n","notes":"","funding":""},{"Title":"Immersive Analytics: An Overview","Submission Target":"it - Information Technology","Date":"2022-09-06","Type":"","First Author":"Karsten Klein","Other Authors":"Michael Sedlmair, Falk Schreiber","Key (e.g. for file names)":"klein2022immersive","Publisher URL (official)":"https://doi.org/10.1515/itit-2022-0037","url2":"https://www.degruyter.com/document/doi/10.1515/itit-2022-0037/html","PDF URL (public)":"https://www.degruyter.com/document/doi/10.1515/itit-2022-0037/pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Immersive Analytics is concerned with the systematic examination of the benefits and challenges of using immersive environments for data analysis, and the development of corresponding designs that improve the quality and efficiency of the analysis process. While immersive technologies are now broadly available, practical solutions haven’t received broad acceptance in real-world applications outside of several core areas, and proper guidelines on the design of such solutions are still under development. Both fundamental research and applications bring together topics and questions from several fields, and open a wide range of directions regarding underlying theory, evidence from user studies, and practical solutions tailored towards the requirements of application areas. We give an overview on the concepts, topics, research questions, and challenges.","bibtex":"@article{KleinSedlmairSchreiber+2022+155+168,\r\n  url = {https://doi.org/10.1515/itit-2022-0037},\r\n  title = {Immersive analytics: An overview},\r\n  author = {Karsten Klein and Michael Sedlmair and Falk Schreiber},\r\n  pages = {155--168},\r\n  volume = {64},\r\n  number = {4-5},\r\n  journal = {it - Information Technology},\r\n  doi = {doi:10.1515/itit-2022-0037},\r\n  year = {2022}\r\n}","notes":"","funding":""},{"Title":"Towards Inclusive Conversations in Virtual Reality for People with Visual Impairments","Submission Target":"MuC","Date":"2022-09-04","Type":"Workshop Paper","First Author":"Markus Wieland","Other Authors":"Tonja Machulla","Key (e.g. for file names)":"wieland2022towards","Publisher URL (official)":"https://doi.org/10.18420/muc2022-mci-ws11-467","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Current mainstream social Virtual Reality (VR) spaces pose barriers to the equal participation of people with visual impairments (PVI) in social interactions. At present, VR is first and primarily a visual medium with a strong emphasis on the visual design of the VR scene and the available avatars. If social communication cues, such as non-verbal communication, are available at all, they are often not provided in a form accessible to PVI. Such cues are essential in social interactions to successfully participate in social interactions and experience a conversation in VR as realistic. Here, we summarize previous research regarding specific requirements for social VR spaces to be accessible to PVIs. We describe how people with disabilities recognize and identify potential conversational partners and how non-verbal communication works between PVI and sighted people. Our goal was to provide an overview of valuable features that can be implemented for inclusive conversations in a social VR space.","bibtex":"@inproceedings{mci/Wieland2022,\r\nauthor = {Wieland, Markus AND Machulla, Tonja},\r\ntitle = {Towards Inclusive Conversations in Virtual Reality for People with Visual Impairments},\r\nbooktitle = {Mensch und Computer 2022 - Workshopband},\r\nyear = {2022},\r\neditor = {Marky, Karola AND Grünefeld, Uwe AND Kosch, Thomas} ,\r\ndoi = { 10.18420/muc2022-mci-ws11-467 },\r\npublisher = {Gesellschaft für Informatik e.V.},\r\naddress = {Bonn}\r\n} ","notes":"","funding":""},{"Title":"Machine learning meets visualization – Experiences and lessons learned","Submission Target":"it - Information Technology","Date":"2022-09-02","Type":"","First Author":"Quynh Quang Ngo","Other Authors":"Frederik L. Dennig, Daniel A. Keim, Michael Sedlmair","Key (e.g. for file names)":"ngo2022machine","Publisher URL (official)":"https://doi.org/10.1515/itit-2022-0034","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) within the projects A03 and A08 of TRR 161 (Project-ID 251654672)","Abstract":"In this article, we discuss how Visualization (VIS) with Machine Learning (ML) could mutually benefit from each other. We do so through the lens of our own experience working at this intersection for the last decade. Particularly we focus on describing how VIS supports explaining ML models and aids ML-based Dimensionality Reduction techniques in solving tasks such as parameter space analysis. In the other direction, we discuss approaches showing how ML helps improve VIS, such as applying ML-based automation to improve visualization design. Based on the examples and our own perspective, we describe a number of open research challenges that we frequently encountered in our endeavors to combine ML and VIS.","bibtex":"@article{ngo2022machine,\r\n  url = {https://doi.org/10.1515/itit-2022-0034},\r\n  title = {Machine learning meets visualization – Experiences and lessons learned},\r\n  author = {Quynh Quang Ngo and Frederik L. Dennig and Daniel A. Keim and Michael Sedlmair},\r\n  journal = {it - Information Technology},\r\n  doi = {doi:10.1515/itit-2022-0034},\r\n  year = {2022},\r\n}","notes":"","funding":""},{"Title":"Toward In-Situ Authoring of Situated Visualization with Chorded Keyboards","Submission Target":"VINCI","Date":"2022-08-16","Type":"","First Author":"Sarah Dosdall","Other Authors":"Katrin Angerbauer, Leonel Merino, Michael Sedlmair, Daniel Weiskopf","Key (e.g. for file names)":"dosdall2022toward","Publisher URL (official)":"https://doi.org/10.1145/3554944.3554970","url2":"","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3554944.3554970","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Authoring situated visualizations in-situ is challenging due to the need of writing code in a mobile and highly dynamic fashion. To provide better support for that, we define requirements for text input methods that target situated visualization authoring. We identify wearable chorded keyboards as a potentially suitable method that fulfills some of these requirements. To further investigate this approach, we tailored a chorded keyboard device to visualization authoring, developed a learning application, and conducted a pilot user study. Our results confirm that learning a high number of chords is the main barrier for adoption, as in other application areas. Based on that, we discuss ideas on how chorded keyboards with a strongly reduced alphabet, hand gestures, and voice recognition might be used as a viable, multi-modal support for authoring situated visualizations in-situ.","bibtex":"@inproceedings{10.1145/3554944.3554970,\r\nauthor = {Dosdall, Sarah and Angerbauer, Katrin and Merino, Leonel and Sedlmair, Michael and Weiskopf, Daniel},\r\ntitle = {Toward In-Situ Authoring of Situated Visualization with Chorded Keyboards},\r\nyear = {2022},\r\nisbn = {9781450398060},\r\npublisher = {ACM},\r\nurl = {https://doi.org/10.1145/3554944.3554970},\r\ndoi = {10.1145/3554944.3554970},\r\nabstract = {Authoring situated visualizations in-situ is challenging due to the need of writing code in a mobile and highly dynamic fashion. To provide better support for that, we define requirements for text input methods that target situated visualization authoring. We identify wearable chorded keyboards as a potentially suitable method that fulfills some of these requirements. To further investigate this approach, we tailored a chorded keyboard device to visualization authoring, developed a learning application, and conducted a pilot user study. Our results confirm that learning a high number of chords is the main barrier for adoption, as in other application areas. Based on that, we discuss ideas on how chorded keyboards with a strongly reduced alphabet, hand gestures, and voice recognition might be used as a viable, multi-modal support for authoring situated visualizations in-situ.},\r\nbooktitle = {Proceedings of the 15th International Symposium on Visual Information Communication and Interaction},\r\narticleno = {14},\r\nnumpages = {5},\r\nkeywords = {mixed reality, chorded keyboard, pilot study, Situated visualization},\r\nseries = {VINCI '22}\r\n}","notes":"","funding":""},{"Title":"Hagrid: using Hilbert and Gosper curves to gridify scatterplots","Submission Target":"JoV","Date":"2022-07-11","Type":"Full Paper","First Author":"Rene Cutura","Other Authors":"Rene Cutura, Cristina Morariu, Zhanglin Cheng, Yunhai Wang, Daniel Weiskopf, Michael Sedlmair ","Key (e.g. for file names)":"cutura2022hagrid","Publisher URL (official)":"https://doi.org/10.1007/s12650-022-00854-7","url2":"https://link.springer.com/article/10.1007/s12650-022-00854-7","PDF URL (public)":"https://link.springer.com/content/pdf/10.1007/s12650-022-00854-7.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"This work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 251654672 - TRR 161.","Abstract":"A common enhancement of scatterplots represents points as small multiples, glyphs, or thumbnail images. As this encoding often results in overlaps, a general strategy is to alter the position of the data points, for instance, to a grid-like structure. Previous approaches rely on solving expensive optimization problems or on dividing the space that alter the global structure of the scatterplot. To find a good balance between efficiency and neighborhood and layout preservation, we propose HAGRID, a technique that uses space-filling curves (SFCs) to “gridify” a scatterplot without employing expensive collision detection and handling mechanisms. Using SFCs ensures that the points are plotted close to their original position, retaining approximately the same global structure. The resulting scatterplot is mapped onto a rectangular or hexagonal grid, using Hilbert and Gosper curves. We discuss and evaluate the theoretic runtime of our approach and quantitatively compare our approach to three state-of-the-art gridifying approaches, DGRID, Small multiples with gaps SMWG, and CorrelatedMultiples CMDS, in an evaluation comprising 339 scatterplots. Here, we compute several quality measures for neighborhood preservation together with an analysis of the actual runtimes. The main results show that, compared to the best other technique, HAGRID is faster by a factor of four, while achieving similar or even better quality of the gridified layout. Due to its computational efficiency, our approach also allows novel applications of gridifying approaches in interactive settings, such as removing local overlap upon hovering over a scatterplot.","bibtex":"@article{cutura2022hagrid,\r\n  title={{Hagrid}: using {Hilbert} and {Gosper} curves to gridify scatterplots},\r\n  author={Cutura, Rene and Morariu, Cristina and Cheng, Zhanglin and Wang, Yunhai and Weiskopf, Daniel and Sedlmair, Michael},\r\n  journal={Journal of Visualization},\r\n  pages={1--17},\r\n  year={2022},\r\n  publisher={Springer}\r\n}","notes":"","funding":"Open Access funding enabled and organized by Projekt DEAL"},{"Title":"Non-verbal Communication and Joint Attention Between People with and Without Visual Impairments: Deriving Guidelines for Inclusive Conversations in Virtual Realities","Submission Target":"Lecture Notes in Computer Science","Date":"2022-07-01","Type":"Full Paper","First Author":"Markus Wieland","Other Authors":"Lauren Thevin, Albrecht Schmidt, Tonja Machulla","Key (e.g. for file names)":"wieland2022nonverbal","Publisher URL (official)":"https://doi.org/10.1007/978-3-031-08648-9_34","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"With the emergence of mainstream virtual reality (VR) platforms for social interactions, non-verbal communicative cues are increasingly being transmitted into the virtual environment. Since VR is primarily a visual medium, accessible VR solutions are required for people with visual impairments (PVI). However, existing propositions do not take into account social interactions, and therefore PVI are excluded from this type of experience. To address this issue, we conducted semi-structured interviews with eleven participants, seven of whom were PVI and four of whom were partners or close friends without visual impairments, to explore how non-verbal cues and joint attention are used and perceived in everyday social situations and conversations. Our goal was to provide guidelines for inclusive conversations in virtual environments for PVI. Our findings suggest that gaze, head direction, head movements, and facial expressions are important for both groups in conversations but often difficult to identify visually for PVI. From our findings, we provide concrete suggestions for the design of social VR spaces, inclusive to PVI.","bibtex":"@InProceedings{10.1007/978-3-031-08648-9_34,\r\nauthor=\"Wieland, Markus\r\nand Thevin, Lauren\r\nand Schmidt, Albrecht\r\nand Machulla, Tonja\",\r\ntitle=\"Non-verbal Communication and Joint Attention Between People with and Without Visual Impairments: Deriving Guidelines for Inclusive Conversations in Virtual Realities\",\r\nbooktitle=\"Computers Helping People with Special Needs\",\r\nyear=\"2022\",\r\npublisher=\"Springer International Publishing\",\r\npages=\"295--304\",\r\nabstract=\"With the emergence of mainstream virtual reality (VR) platforms for social interactions, non-verbal communicative cues are increasingly being transmitted into the virtual environment. Since VR is primarily a visual medium, accessible VR solutions are required for people with visual impairments (PVI). However, existing propositions do not take into account social interactions, and therefore PVI are excluded from this type of experience. To address this issue, we conducted semi-structured interviews with eleven participants, seven of whom were PVI and four of whom were partners or close friends without visual impairments, to explore how non-verbal cues and joint attention are used and perceived in everyday social situations and conversations. Our goal was to provide guidelines for inclusive conversations in virtual environments for PVI. Our findings suggest that gaze, head direction, head movements, and facial expressions are important for both groups in conversations but often difficult to identify visually for PVI. From our findings, we provide concrete suggestions for the design of social VR spaces, inclusive to PVI.\",\r\nisbn=\"978-3-031-08648-9\"\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Touching Data with PropellerHand","Submission Target":"Journal of Visualization","Date":"2022-06-06","Type":"Full Paper","First Author":"Alexander Achberger","Other Authors":"Frank Heyen, Kresimir Vidackovic, Michael Sedlmair ","Key (e.g. for file names)":"achberger2022touching","Publisher URL (official)":"https://doi.org/10.1007/s12650-022-00859-2","url2":"","PDF URL (public)":"https://link.springer.com/content/pdf/10.1007/s12650-022-00859-2.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"Partially supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy – EXC 2120/1 – 390831618","Abstract":"Immersive analytics often takes place in virtual environments which promise the users immersion. To fulfill this promise, sensory feedback, such as haptics, is an important component, which is however not well supported yet. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our prototype through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality. Additionally, we conducted a second user study in the field of immersive visualization to investigate the potential benefits of PropellerHand there.","bibtex":"@article{achberger2022touching,\r\n    title={Touching data with PropellerHand},\r\n    author={Achberger, Alexander and Heyen, Frank and Vidackovic, Kresimir and Sedlmair, Michael},\r\n    journal={Journal of Visualization},\r\n    year={2022},\r\n    doi={10.1007/s12650-022-00859-2},\r\n    url={https://doi.org/10.1007/s12650-022-00859-2}\r\n}","notes":"","funding":"Open Access funding enabled and organized by Projekt DEAL"},{"Title":"Parameter Adaptation In Situ: Design Impacts and Trade-Offs","Submission Target":"In Situ Visualization for Computational Science","Date":"2022-05-05","Type":"Book Chapter","First Author":"Steffen Frey","Other Authors":"Valentin Bruder, Florian Frieß, Patrick Gralka, Tobias Rau, Thomas Ertl, Guido Reina","Key (e.g. for file names)":"frey2022parameter","Publisher URL (official)":"https://doi.org/10.1007/978-3-030-81627-8_8","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"This chapter presents a study of parameter adaptation in situ, exploring the resulting trade-offs in rendering quality and workload distribution. Four different use cases are analyzed with respect to configuration changes. First, the performance impact of load balancing and resource allocation variants on both simulation and visualization is investigated using the MegaMol framework. Its loose coupling scheme and architecture enable minimally invasive in situ operation without impacting the stability of the simulation with (potentially) experimental visualization code. Second, Volumetric Depth Images (VDIs) are considered: a compact, view-dependent intermediate representation that can efficiently be generated and used for post hoc exploration. A study of their inherent trade-offs regarding size, quality, and generation time provides the basis for parameter optimization. Third, streaming for remote visualization allows a user to monitor the progress of a simulation and to steer visualization parameters. Compression settings are adapted dynamically based on predictions via convolutional neural networks across different parts of images to achieve high frame rates for high-resolution displays like powerwalls. Fourth, different performance prediction models for volume rendering address offline scenarios (like hardware acquisition planning) as well as dynamic adaptation of parameters and load balancing. Finally, the chapter concludes by summarizing overarching approaches and challenges, discussing the potential role that adaptive approaches can play in increasing the efficiency of in situ visualization.","bibtex":"@inproceedings{frey2022parameter,\r\n  author = {Frey, Steffen and Bruder, Valentin and Frie{\\ss}, Florian and Gralka, Patrick and Rau, Tobias and Ertl, Thomas and Reina, Guido},\r\n  booktitle = {In Situ Visualization for Computational Science},\r\n  doi = {10.1007/978-3-030-81627-8_8},\r\n  isbn = {978-3-030-81627-8},\r\n  pages = {159--182},\r\n  publisher = {Springer International Publishing},\r\n  title = {Parameter Adaptation In Situ: Design Impacts and Trade-Offs},\r\n  year = 2022\r\n}","notes":"","funding":""},{"Title":"cARdLearner: Using Expressive Virtual Agents when Learning Vocabulary in Augmented Reality","Submission Target":"CHI","Date":"2022-04-29","Type":"Late-Breaking Work","First Author":"Aimee Sousa Calepso","Other Authors":"Natalie Hube, Noah Berenguel Senn, Vincent Brandt, Michael Sedlmair","Key (e.g. for file names)":"calepso2022cardlearner","Publisher URL (official)":"https://doi.org/10.1145/3491101.3519631","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Augmented reality (AR) has a diverse range of applications, including language teaching. When studying a foreign language, one of the biggest challenges learners face is memorizing new vocabulary. While augmented holograms are a promising means of supporting this memorization process, few studies have explored their potential in the language learning context. We demonstrate the possibility of using flashcard along with an expressive holographic agent on vocabulary learning. Users scan a flashcard and play an animation that is connected with an emotion related to the word they are seeing. Our goal is to propose an alternative to the traditional use of flashcards, and also introduce another way of using AR in the association process.","bibtex":"@inproceedings{10.1145/3491101.3519631,\r\nauthor = {Sousa Calepso, Aimee and Hube, Natalie and Berenguel Senn, Noah and Brandt, Vincent and Sedlmair, Michael},\r\ntitle = {CARdLearner: Using Expressive Virtual Agents When Learning Vocabulary in Augmented Reality},\r\nyear = {2022},\r\nisbn = {9781450391566},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3491101.3519631},\r\ndoi = {10.1145/3491101.3519631},\r\nabstract = { Augmented reality (AR) has a diverse range of applications, including language teaching. When studying a foreign language, one of the biggest challenges learners face is memorizing new vocabulary. While augmented holograms are a promising means of supporting this memorization process, few studies have explored their potential in the language learning context. We demonstrate the possibility of using flashcard along with an expressive holographic agent on vocabulary learning. Users scan a flashcard and play an animation that is connected with an emotion related to the word they are seeing. Our goal is to propose an alternative to the traditional use of flashcards, and also introduce another way of using AR in the association process.},\r\nbooktitle = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},\r\narticleno = {245},\r\nnumpages = {6},\r\nkeywords = {contextual learning, augmented reality, language learning},\r\nlocation = {New Orleans, LA, USA},\r\nseries = {CHI EA '22}\r\n}","notes":"","funding":""},{"Title":"Immersive Visual Analysis of Cello Bow Movements","Submission Target":"CHI IMI Workshop","Date":"2022-04-29","Type":"Workshop Paper","First Author":"Frank Heyen","Other Authors":"Yannik Kohler, Sebastian Triebener, Sebastian Rigling, Michael Sedlmair","Key (e.g. for file names)":"heyen2022cellovis","Publisher URL (official)":"https://doi.org/10.48550/arxiv.2203.13316","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy - EXC 2075 - 390740016, and by Cyber Valley (InstruData project).","Abstract":"We propose a 3D immersive visualization environment for analyzing the right hand movements of a cello player. To achieve this, we track the position and orientation of the cello bow and record audio. As movements mostly occur in a shallow volume and the motion is therefore mostly two-dimensional, we use the third dimension to encode time. Our concept further explores various mappings from motion and audio data to spatial and other visual attributes. We work in close cooperation with a cellist and plan to evaluate our prototype through a user study with a group of cellists in the near future.","bibtex":"@misc{https://doi.org/10.48550/arxiv.2203.13316,\r\n  doi = {10.48550/ARXIV.2203.13316},\r\n  url = {https://arxiv.org/abs/2203.13316},\r\n  author = {Heyen, Frank and Kohler, Yannik and Triebener, Sebastian and Rigling, Sebastian and Sedlmair, Michael},\r\n  keywords = {Human-Computer Interaction (cs.HC), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},\r\n  title = {Immersive Visual Analysis of Cello Bow Movements},\r\n  publisher = {arXiv},\r\n  year = {2022},\r\n  copyright = {Creative Commons Attribution Share Alike 4.0 International}\r\n}","notes":"","funding":""},{"Title":"Data-Driven Visual Reflection on Music Instrument Practice","Submission Target":"CHI IMI Workshop","Date":"2022-04-29","Type":"Workshop Paper","First Author":"Frank Heyen","Other Authors":"Quynh Quang Ngo, Kuno Kurzhals, Michael Sedlmair","Key (e.g. for file names)":"heyen2022datadriven","Publisher URL (official)":"https://doi.org/10.48550/arxiv.2203.13320","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy - EXC 2075 - 390740016, and by Cyber Valley (InstruData project).","Abstract":"We propose a data-driven approach to music instrument practice that allows studying patterns and long-term trends through visualization. Inspired by life logging and fitness tracking, we imagine musicians to record their practice sessions over the span of months or years. The resulting data in the form of MIDI or audio recordings can then be analyzed sporadically to track progress and guide decisions. Toward this vision, we started exploring various visualization designs together with a group of nine guitarists, who provided us with data and feedback over the course of three months.","bibtex":"@misc{https://doi.org/10.48550/arxiv.2203.13320,\r\n  doi = {10.48550/ARXIV.2203.13320},\r\n  url = {https://arxiv.org/abs/2203.13320},\r\n  author = {Heyen, Frank and Ngo, Quynh Quang and Kurzhals, Kuno and Sedlmair, Michael},\r\n  keywords = {Human-Computer Interaction (cs.HC), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},\r\n  title = {Data-Driven Visual Reflection on Music Instrument Practice},\r\n  publisher = {arXiv},\r\n  year = {2022},\r\n  copyright = {Creative Commons Attribution Share Alike 4.0 International}\r\n}\r\n","notes":"","funding":""},{"Title":"Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study","Submission Target":"CHI","Date":"2022-04-29","Type":"Late-Breaking Work","First Author":"Natalie Hube","Other Authors":" Kresimir Vidackovic, Michael Sedlmair","Key (e.g. for file names)":"hube2022using","Publisher URL (official)":"https://doi.org/10.1145/3491101.3519822","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Virtual avatars are widely used for collaborating in virtual environments. Yet, often these avatars lack expressiveness to determine a state of mind. Prior work has demonstrated efective usage of determining emotions and animated lip movement through analyzing mere audio tracks of spoken words. To provide this information on a virtual avatar, we created a natural audio data set consisting of 17 audio fles from which we then extracted the underlying emotion and lip movement. To conduct a pilot study, we developed a prototypical system that displays the extracted visual parameters and then maps them on a virtual avatar while playing the corresponding audio fle. We tested the system with 5 participants in two conditions: (i) while seeing the virtual avatar only an audio fle was played. (ii) In addition to the audio fle, the extracted facial visual parameters were displayed on the virtual avatar. Our results suggest the validity of using additional visual parameters in the avatars face as it helps to determine emotions. We conclude with a brief discussion on the outcomes and their implications on future work.","bibtex":"@inproceedings{10.1145/3491101.3519822,\r\nauthor = {Hube, Natalie and Vidackovic, Kresimir and Sedlmair, Michael},\r\ntitle = {Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study},\r\nyear = {2022},\r\nisbn = {9781450391566},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3491101.3519822},\r\ndoi = {10.1145/3491101.3519822},\r\nabstract = { Virtual avatars are widely used for collaborating in virtual environments. Yet, often these avatars lack expressiveness to determine a state of mind. Prior work has demonstrated effective usage of determining emotions and animated lip movement through analyzing mere audio tracks of spoken words. To provide this information on a virtual avatar, we created a natural audio data set consisting of 17 audio files from which we then extracted the underlying emotion and lip movement. To conduct a pilot study, we developed a prototypical system that displays the extracted visual parameters and then maps them on a virtual avatar while playing the corresponding audio file. We tested the system with 5 participants in two conditions: (i) while seeing the virtual avatar only an audio file was played. (ii) In addition to the audio file, the extracted facial visual parameters were displayed on the virtual avatar. Our results suggest the validity of using additional visual parameters in the avatars’ face as it helps to determine emotions. We conclude with a brief discussion on the outcomes and their implications on future work.},\r\nbooktitle = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},\r\narticleno = {260},\r\nnumpages = {7},\r\nkeywords = {emotion, avatars, lip synchronization, virtual reality},\r\nlocation = {New Orleans, LA, USA},\r\nseries = {CHI EA '22}\r\n}","notes":"","funding":""},{"Title":"Tangible Globes for Data Visualisation in Augmented Reality","Submission Target":"CHI","Date":"2022-04-29","Type":"Full Paper","First Author":"Kadek Satriadi","Other Authors":"Jim Smiley, Barrett Ens, Maxime Cordeil, Tobias Czauderna, Benjamin Lee, Ying Yang, Tim Dwyer, Bernhard Jenny","Key (e.g. for file names)":"satriadi2022tangible","Publisher URL (official)":"https://doi.org/10.1145/3491102.3517715","url2":"https://dl.acm.org/doi/abs/10.1145/3491102.3517715","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=QwlpML4D9lo","Video2":"","Supplemental":"https://dl.acm.org/doi/abs/10.1145/3491102.3517715#sec-supp","Acknowledgements":"","Abstract":"Head-mounted augmented reality (AR) displays allow for the seamless integration of virtual visualisation with contextual tangible references, such as physical (tangible) globes. We explore the design of immersive geospatial data visualisation with AR and tangible globes. We investigate the “tangible-virtual interplay” of tangible globes with virtual data visualisation, and propose a conceptual approach for designing immersive geospatial globes. We demonstrate a set of use cases, such as augmenting a tangible globe with virtual overlays, using a physical globe as a tangible input device for interacting with virtual globes and maps, and linking an augmented globe to an abstract data visualisation. We gathered qualitative feedback from experts about our use case visualisations, and compiled a summary of key takeaways as well as ideas for envisioned future improvements. The proposed design space, example visualisations and lessons learned aim to guide the design of tangible globes for data visualisation in AR. ","bibtex":"@inproceedings{10.1145/3491102.3517715,\r\nauthor = {Satriadi, Kadek Ananta and Smiley, Jim and Ens, Barrett and Cordeil, Maxime and Czauderna, Tobias and Lee, Benjamin and Yang, Ying and Dwyer, Tim and Jenny, Bernhard},\r\ntitle = {Tangible Globes for Data Visualisation in Augmented Reality},\r\nyear = {2022},\r\nisbn = {9781450391573},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3491102.3517715},\r\ndoi = {10.1145/3491102.3517715},\r\nabstract = {Head-mounted augmented reality (AR) displays allow for the seamless integration of virtual visualisation with contextual tangible references, such as physical (tangible) globes. We explore the design of immersive geospatial data visualisation with AR and tangible globes. We investigate the “tangible-virtual interplay” of tangible globes with virtual data visualisation, and propose a conceptual approach for designing immersive geospatial globes. We demonstrate a set of use cases, such as augmenting a tangible globe with virtual overlays, using a physical globe as a tangible input device for interacting with virtual globes and maps, and linking an augmented globe to an abstract data visualisation. We gathered qualitative feedback from experts about our use case visualisations, and compiled a summary of key takeaways as well as ideas for envisioned future improvements. The proposed design space, example visualisations and lessons learned aim to guide the design of tangible globes for data visualisation in AR.},\r\nbooktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},\r\narticleno = {505},\r\nnumpages = {16},\r\nkeywords = {immersive analytics, augmented reality, geographic visualisation, tangible user interface},\r\nlocation = {New Orleans, LA, USA},\r\nseries = {CHI '22}\r\n}","notes":"","funding":""},{"Title":"Accessibility for Color Vision Deficiencies: Challenges and Findings of a Large Scale Study on Paper Figures","Submission Target":"CHI","Date":"2022-04-28","Type":"Full Paper","First Author":"Katrin Angerbauer","Other Authors":"Nils Rodrigues, Rene Cutura, Seyda Öney, Nelusa Pathmanathan, Cristina Morariu, Daniel Weiskopf, Michael Sedlmair","Key (e.g. for file names)":"angerbauer2022accessibility","Publisher URL (official)":"https://doi.org/10.1145/3491102.3502133","url2":"","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=fhhg0k2LLkk","Video2":"","Supplemental":"","Acknowledgements":"Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (projects A08 and B01). We thank all our study participants and in particular Sajid Baloch for his valuable input.","Abstract":"We present an exploratory study on the accessibility of images in publications when viewed with color vision deficiencies (CVDs). The study is based on 1,710 images sampled from a visualization dataset (VIS30K) over five years. We simulated four CVDs on each image. First, four researchers (one with a CVD) identified existing issues and helpful aspects in a subset of the images. Based on the resulting labels, 200 crowdworkers provided  30,000 ratings on present CVD issues in the simulated images. We analyzed this data for correlations, clusters, trends, and free text comments to gain a first overview of paper figure accessibility. Overall, about 60 % of the images were rated accessible. Furthermore, our study indicates that accessibility issues are subjective and hard to detect. On a meta-level, we reflect on our study experience to point out challenges and opportunities of large-scale accessibility studies for future research directions.","bibtex":"@inproceedings{10.1145/3491102.3502133,\r\nauthor = {Angerbauer, Katrin and Rodrigues, Nils and Cutura, Rene and \\\"{O}ney, Seyda and Pathmanathan, Nelusa and Morariu, Cristina and Weiskopf, Daniel and Sedlmair, Michael},\r\ntitle = {Accessibility for Color Vision Deficiencies: Challenges and Findings of a Large Scale Study on Paper Figures},\r\nyear = {2022},\r\nisbn = {9781450391573},\r\npublisher = {ACM},\r\nurl = {https://doi.org/10.1145/3491102.3502133},\r\ndoi = {10.1145/3491102.3502133},\r\nabstract = {We present an exploratory study on the accessibility of images in publications when viewed with color vision deficiencies (CVDs). The study is based on 1,710 images sampled from a visualization dataset (VIS30K) over five years. We simulated four CVDs on each image. First, four researchers (one with a CVD) identified existing issues and helpful aspects in a subset of the images. Based on the resulting labels, 200 crowdworkers provided  30,000 ratings on present CVD issues in the simulated images. We analyzed this data for correlations, clusters, trends, and free text comments to gain a first overview of paper figure accessibility. Overall, about 60 % of the images were rated accessible. Furthermore, our study indicates that accessibility issues are subjective and hard to detect. On a meta-level, we reflect on our study experience to point out challenges and opportunities of large-scale accessibility studies for future research directions. },\r\nbooktitle = {CHI Conference on Human Factors in Computing Systems},\r\narticleno = {134},\r\nnumpages = {23},\r\nkeywords = {color vision deficiency, crowdsourcing, visualization, accessibility},\r\nseries = {CHI '22}\r\n}","notes":"","funding":""},{"Title":"A Design Space For Data Visualisation Transformations Between 2D And 3D In Mixed-Reality Environments","Submission Target":"CHI","Date":"2022-04-28","Type":"","First Author":"Benjamin Lee","Other Authors":"Maxime Cordeil, Arnaud Prouzeau, Bernhard Jenny, Tim Dwyer","Key (e.g. for file names)":"lee2022design","Publisher URL (official)":"https://doi.org/10.1145/3491102.3501859","url2":"https://dl.acm.org/doi/abs/10.1145/3491102.3501859","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=jjB99Ruc5gY","Video2":"","Supplemental":"https://dl.acm.org/doi/abs/10.1145/3491102.3501859#sec-supp","Acknowledgements":"","Abstract":"As mixed-reality (MR) technologies become more mainstream, the delineation between data visualisations displayed on screens or other surfaces and those floating in space becomes increasingly blurred. Rather than the choice of using either a 2D surface or the 3D space for visualising data being a dichotomy, we argue that users should have the freedom to transform visualisations seamlessly between the two as needed. However, the design space for such transformations is large, and practically uncharted. To explore this, we first establish an overview of the different states that a data visualisation can take in MR, followed by how transformations between these states can facilitate common visualisation tasks. We then describe a design space of how these transformations function, in terms of the different stages throughout the transformation, and the user interactions and input parameters that affect it. This design space is then demonstrated with multiple exemplary techniques based in MR.","bibtex":"@inproceedings{10.1145/3491102.3501859,\r\nauthor = {Lee, Benjamin and Cordeil, Maxime and Prouzeau, Arnaud and Jenny, Bernhard and Dwyer, Tim},\r\ntitle = {A Design Space For Data Visualisation Transformations Between 2D And 3D In Mixed-Reality Environments},\r\nyear = {2022},\r\nisbn = {9781450391573},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3491102.3501859},\r\ndoi = {10.1145/3491102.3501859},\r\nabstract = {As mixed-reality (MR) technologies become more mainstream, the delineation between data visualisations displayed on screens or other surfaces and those floating in space becomes increasingly blurred. Rather than the choice of using either a 2D surface or the 3D space for visualising data being a dichotomy, we argue that users should have the freedom to transform visualisations seamlessly between the two as needed. However, the design space for such transformations is large, and practically uncharted. To explore this, we first establish an overview of the different states that a data visualisation can take in MR, followed by how transformations between these states can facilitate common visualisation tasks. We then describe a design space of how these transformations function, in terms of the different stages throughout the transformation, and the user interactions and input parameters that affect it. This design space is then demonstrated with multiple exemplary techniques based in MR.},\r\nbooktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},\r\narticleno = {25},\r\nnumpages = {14},\r\nkeywords = {animated transitions, mixed reality, Immersive Analytics, direct manipulation, visualisation},\r\nlocation = {New Orleans, LA, USA},\r\nseries = {CHI '22}\r\n}","notes":"Honourable Mention Award","funding":""},{"Title":"STROE: An Ungrounded String-Based Weight Simulation Device","Submission Target":"VR","Date":"2022-04-20","Type":"Full Paper","First Author":"Alexander Achberger","Other Authors":"Pirathipan Arulrajah, Michael Sedlmair, Kresimir Vidackovic","Key (e.g. for file names)":"achberger2022stroe","Publisher URL (official)":"https://doi.org/10.1109/VR51125.2022.00029","url2":"","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=9edaBf7VqNY","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user’s hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users’ perceived realism and immersion of VR scenes.","bibtex":"@INPROCEEDINGS{9756818,\r\n  author={Achberger, Alexander and Arulrajah, Pirathipan and Sedlmair, Michael and Vidackovic, Kresimir},\r\n  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, \r\n  title={{STROE}: An Ungrounded String-Based Weight Simulation Device}, \r\n  year={2022},\r\n  pages={112-120},\r\n  abstract={We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user’s hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users’ perceived realism and immersion of VR scenes.},\r\n  doi={10.1109/VR51125.2022.00029},\r\n  ISSN={2642-5254}\r\n}\r\n","notes":"","funding":""},{"Title":"Metaphorical Visualization: Mapping Data to Familiar Concepts","Submission Target":"alt.CHI","Date":"2022-04-01","Type":"Extended Abstract","First Author":"Gleb Tkachev","Other Authors":"Rene Cutura, Michael Sedlmair, Steffen Frey, Thomas Ertl","Key (e.g. for file names)":"tkachev2022metaphorical","Publisher URL (official)":"https://doi.org/10.1145/3491101.3516393","url2":"","PDF URL (public)":"https://gleb-t.com/publication/metaphor-vis/metaphor-vis.pdf","Video":"https://gleb-t.com/media/videos/MetaphorVisFastForward_LowBitrate.mp4","Video2":"","Supplemental":"https://gleb-t.com/publication/metaphor-vis/metaphorvis_2022_supplemental.pdf","Acknowledgements":"Funded by the Deutsche Forschungsgemeinschaft (DFG, German\r\nResearch Foundation) under Germany’s Excellence Strategy EXC\r\n2075 – 390740016, and TRR 161 – 251654672.","Abstract":"We present a new approach to visualizing data that is well-suited for personal and casual applications. The idea is to map the data to another dataset that is already familiar to the user, and then rely on their existing knowledge to illustrate relationships in the data. We construct the map by preserving pairwise distances or by maintaining relative values of specific data attributes. This metaphorical mapping is very flexible and allows us to adapt the visualization to its application and target audience. We present several examples where we map data to different domains and representations. This includes mapping data to cat images, encoding research interests with neural style transfer and representing movies as stars in the night sky. Overall, we find that although metaphors are not as accurate as the traditional techniques, they can help design engaging and personalized visualizations","bibtex":"@inproceedings{10.1145/3491101.3516393,\r\ntitle = {Metaphorical Visualization: Mapping Data to Familiar Concepts},\r\n  author = {Tkachev, Gleb and Cutura, Rene and Sedlmair, Michael and Frey, Steffen and Ertl, Thomas},\r\n  year = {2022},\r\n  isbn = {9781450391566},\r\n  publisher = {ACM},\r\n  url = {https://doi.org/10.1145/3491101.3516393},\r\n  doi = {10.1145/3491101.3516393},\r\n  booktitle = {Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems},\r\n  articleno = {10},\r\n  numpages = {10},\r\n  keywords = {Metaphor, Image embedding, Word embedding},\r\n  series = {CHI EA '22},\r\n  eventtitle = {Alt.{{CHI}} 2022}\r\n}","notes":"","funding":""},{"Title":"AR Hero: Generating Interactive Augmented Reality Guitar Tutorials","Submission Target":"VRW","Date":"2022-03-12","Type":"","First Author":"Lucchas Ribeiro Skreinig","Other Authors":"Ana Stanescu, Shohei Mori, Frank Heyen, Peter Mohr, Michael Sedlmair, Dieter Schmalstieg, Denis Kalkofen","Key (e.g. for file names)":"skreinig2022ar","Publisher URL (official)":"https://doi.org/10.1109/VRW55335.2022.00086","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We introduce a system capable of generating interactive Augmented Reality guitar tutorials by parsing common digital guitar tablature and by capturing the performance of an expert using a multi-camera array. Instructions are presented to the user in an Augmented Reality application using either an abstract visualization, a 3D virtual hand, or a 3D video. To support individual users at different skill levels the system provides full control of the playback of a tutorial, including its speed and looping behavior, while delivering live feedback on the user’s performance.","bibtex":"@INPROCEEDINGS{9757565,\r\n  author={Skreinig, Lucchas Ribeiro and Stanescu, Ana and Mori, Shohei and Heyen, Frank and Mohr, Peter and Sedlmair, Michael and Schmalstieg, Dieter and Kalkofen, Denis},\r\n  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, \r\n  title={AR Hero: Generating Interactive Augmented Reality Guitar Tutorials}, \r\n  year={2022},\r\n  pages={395-401},\r\n  doi={10.1109/VRW55335.2022.00086}}\r\n","notes":"","funding":""},{"Title":"RagRug: A Toolkit for Situated Analytics","Submission Target":"TVCG","Date":"2022-03-07","Type":"Full Paper","First Author":"Philipp Fleck","Other Authors":"Aimee Sousa Calepso, Sebastian Hubenschmid, Michael Sedlmair, Dieter Schmalstieg","Key (e.g. for file names)":"fleck2022ragrug","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2022.3157058","url2":"","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=mFxSdvQhSVU","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present RagRug, an open-source toolkit for situated analytics. The abilities of RagRug go beyond previous immersive analytics toolkits by focusing on specific requirements emerging when using augmented reality (AR) rather than virtual reality. RagRug combines state of the art visual encoding capabilities with a comprehensive physical-virtual model, which lets application developers systematically describe the physical objects in the real world and their role in AR. We connect AR visualization with data streams from the Internet of Things using distributed dataflow. To this aim, we use reactive programming patterns so that visualizations become context-aware, i.e., they adapt to events coming in from the environment. The resulting authoring system is low-code; it emphasises describing the physical and the virtual world and the dataflow between the elements contained therein. We describe the technical design and implementation of RagRug, and report on five example applications illustrating the toolkit's abilities.","bibtex":"@article{fleck2022ragrug,\r\n  title={{RagRug}: A Toolkit for Situated Analytics},\r\n  author={Fleck, Philipp and Calepso, Aimee Sousa and Hubenschmid, Sebastian and Sedlmair, Michael and Schmalstieg, Dieter},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics},\r\n  year={2022},\r\n  publisher={IEEE}\r\n}","notes":"","funding":""},{"Title":"RfX: A Design Study for the Interactive Exploration of a Random Forest to Enhance Testing Procedures for Electrical Engines","Submission Target":"CGF","Date":"2022-03-01","Type":"Full Paper","First Author":"Joscha Eirich","Other Authors":"M. Münch, Dominik Jäckle, Michael Sedlmair, Jakob Bonart, Tobias Schreck","Key (e.g. for file names)":"eirich2022rfx","Publisher URL (official)":"https://doi.org/10.1111/cgf.14452","url2":"https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14452","PDF URL (public)":"https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14452","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Random Forests (RFs) are a machine learning (ML) technique widely used across industries. The interpretation of a given RF usually relies on the analysis of statistical values and is often only possible for data analytics experts. To make RFs accessible to experts with no data analytics background, we present RfX, a Visual Analytics (VA) system for the analysis of a RF's decision-making process. RfX allows to interactively analyse the properties of a forest and to explore and compare multiple trees in a RF. Thus, its users can identify relationships within a RF's feature subspace and detect hidden patterns in the model's underlying data. We contribute a design study in collaboration with an automotive company. A formative evaluation of RFX was carried out with two domain experts and a summative evaluation in the form of a field study with five domain experts. In this context, new hidden patterns such as increased eccentricities in an engine's rotor by observing secondary excitations of its bearings were detected using analyses made with RfX. Rules derived from analyses with the system led to a change in the company's testing procedures for electrical engines, which resulted in 80% reduced testing time for over 30% of all components.","bibtex":"@article{https://doi.org/10.1111/cgf.14452,\r\nauthor = {Eirich, J. and Münch, M. and Jäckle, D. and Sedlmair, M. and Bonart, J. and Schreck, T.},\r\ntitle = {{RfX}: A Design Study for the Interactive Exploration of a Random Forest to Enhance Testing Procedures for Electrical Engines},\r\njournal = {Computer Graphics Forum (CGF)},\r\nkeywords = {human–computer interfaces, interaction, visual analytics, visualization},\r\ndoi = {https://doi.org/10.1111/cgf.14452},\r\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14452},\r\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14452},\r\nabstract = {Abstract Random Forests (RFs) are a machine learning (ML) technique widely used across industries. The interpretation of a given RF usually relies on the analysis of statistical values and is often only possible for data analytics experts. To make RFs accessible to experts with no data analytics background, we present RfX, a Visual Analytics (VA) system for the analysis of a RF's decision-making process. RfX allows to interactively analyse the properties of a forest and to explore and compare multiple trees in a RF. Thus, its users can identify relationships within a RF's feature subspace and detect hidden patterns in the model's underlying data. We contribute a design study in collaboration with an automotive company. A formative evaluation of RFX was carried out with two domain experts and a summative evaluation in the form of a field study with five domain experts. In this context, new hidden patterns such as increased eccentricities in an engine's rotor by observing secondary excitations of its bearings were detected using analyses made with RfX. Rules derived from analyses with the system led to a change in the company's testing procedures for electrical engines, which resulted in 80\\% reduced testing time for over 30\\% of all components.}\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World","Submission Target":"CG&A","Date":"2022-02-09","Type":"","First Author":"Moataz Abdelaal","Other Authors":"Felix Amtsberg, Michael Becher, Rebeca Duque Estrada, Fabian Kannenberg, Aimee Sousa Calepso, Hans Jakob Wagner, Guido Reina, Michael Sedlmair, Achim Menges, Daniel Weiskopf","Key (e.g. for file names)":"abdelaal2022visualization","Publisher URL (official)":"https://doi.org/10.1109/MCG.2022.3149837","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Our built world is one of the most important factors for a livable future, accounting for massive impact on resource and energy use, as well as climate change, but also the social and economic aspects that come with population growth. The architecture, engineering, and construction industry is facing the challenge that it needs to substantially increase its productivity, let alone the quality of buildings of the future. In this article, we discuss these challenges in more detail, focusing on how digitization can facilitate this transformation of the industry, and link them to opportunities for visualization and augmented reality research. We illustrate solution strategies for advanced building systems based on wood and fiber.","bibtex":"@ARTICLE{abdelaal2022visualization,\r\n  author={Abdelaal, Moataz and Amtsberg, Felix and Becher, Michael and Estrada, Rebeca Duque and Kannenberg, Fabian and Calepso, Aim&#x00E9;e Sousa and Wagner, Hans Jakob and Reina, Guido and Sedlmair, Michael and Menges, Achim and Weiskopf, Daniel},\r\n  journal={IEEE Computer Graphics and Applications}, \r\n  title={Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World}, \r\n  year={2022},\r\n  volume={42},\r\n  number={2},\r\n  pages={10-20},\r\n  abstract={Our built world is one of the most important factors for a livable future, accounting for massive impact on resource and energy use, as well as climate change, but also the social and economic aspects that come with population growth. The architecture, engineering, and construction industry is facing the challenge that it needs to substantially increase its productivity, let alone the quality of buildings of the future. In this article, we discuss these challenges in more detail, focusing on how digitization can facilitate this transformation of the industry, and link them to opportunities for visualization and augmented reality research. We illustrate solution strategies for advanced building systems based on wood and fiber.},\r\n  doi={10.1109/MCG.2022.3149837},\r\n  ISSN={1558-1756},\r\n  month={March}}\r\n","notes":"","funding":"10.13039/501100001659-Deutsche Forschungsgemeinschaft (Grant Number: 279064222—SFB 1244)"},{"Title":"Visual Support for Human-AI Co-Composition","Submission Target":"ISMIR","Date":"2021-11-07","Type":"Late-Breaking Demo Poster","First Author":"Simeon Rau","Other Authors":"Frank Heyen, Michael Sedlmair","Key (e.g. for file names)":"rau2021visual","Publisher URL (official)":"https://archives.ismir.net/ismir2021/latebreaking/000014.pdf","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"This work was funded by the Cyber Valley Research Fund – Project InstruData.","Abstract":"We propose a visual approach for AI-assisted music composition, where the user interactively generates, selects, and adapts short melodies. Based on an entered start melody, we automatically generate multiple continuation samples. Repeating this step and in turn generating continuations for these samples results in a tree or graph of melodies. We visualize this structure with two visualizations, where nodes display the piano roll of the corresponding sample. By interacting with these visualizations, the user can quickly listen to, choose, and adapt melodies, to iteratively create a composition. A third visualization provides an overview over larger numbers of samples, allowing for insights into the AI's predictions and the sample space.","bibtex":"@inproceedings{rau2021visual,\r\n  title={Visual Support for Human-{AI} Co-Composition},\r\n  author={Rau, Simeon and Heyen, Frank and Sedlmair, Michael},\r\n  year={2021},\r\n  booktitle={Extended Abstracts for the Late-Breaking Demo Session of the 22nd Int. Society for Music Information Retrieval Conf. (ISMIR)},\r\n  url={https://archives.ismir.net/ismir2021/latebreaking/000014.pdf}\r\n}","notes":"","funding":""},{"Title":"The MADE-Axis: A Modular Actuated Device to Embody the Axis of a Data Dimension","Submission Target":"ISS","Date":"2021-11-05","Type":"","First Author":"Jim Smiley","Other Authors":"Benjamin Lee, Siddhant Tandon, Maxime Cordeil, Lonni Besançon, Jarrod Knibbe, Bernhard Jenny, Tim Dwyer","Key (e.g. for file names)":"smiley2021madeaxis","Publisher URL (official)":"https://doi.org/10.1145/3488546","url2":"https://dl.acm.org/doi/abs/10.1145/3488546","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=ILZlecsvUbw","Video2":"","Supplemental":"https://dl.acm.org/doi/abs/10.1145/3488546#sec-supp","Acknowledgements":"","Abstract":"Tangible controls-especially sliders and rotary knobs-have been explored in a wide range of interactive applications for desktop and immersive environments. Studies have shown that they support greater precision and provide proprioceptive benefits, such as support for eyes-free interaction. However, such controls tend to be expressly designed for specific applications. We draw inspiration from a bespoke controller for immersive data visualisation, but decompose this design into a simple, wireless, composable unit featuring two actuated sliders and a rotary encoder. Through these controller units, we explore the interaction opportunities around actuated sliders; supporting precise selection, infinite scrolling, adaptive data representations, and rich haptic feedback; all within a mode-less interaction space. We demonstrate the controllers' use for simple, ad hoc desktop interaction,before moving on to more complex, multi-dimensional interactions in VR and AR. We show that the flexibility and composability of these actuated controllers provides an emergent design space which covers the range of interactive dynamics for visual analysis. In a user study involving pairs performing collaborative visual analysis tasks in mixed-reality, our participants were able to easily compose rich visualisations, make insights and discuss their findings.","bibtex":"@article{10.1145/3488546,\r\nauthor = {Smiley, Jim and Lee, Benjamin and Tandon, Siddhant and Cordeil, Maxime and Besan\\c{c}on, Lonni and Knibbe, Jarrod and Jenny, Bernhard and Dwyer, Tim},\r\ntitle = {The MADE-Axis: A Modular Actuated Device to Embody the Axis of a Data Dimension},\r\nyear = {2021},\r\nissue_date = {November 2021},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nvolume = {5},\r\nnumber = {ISS},\r\nurl = {https://doi.org/10.1145/3488546},\r\ndoi = {10.1145/3488546},\r\nabstract = {Tangible controls-especially sliders and rotary knobs-have been explored in a wide range of interactive applications for desktop and immersive environments. Studies have shown that they support greater precision and provide proprioceptive benefits, such as support for eyes-free interaction. However, such controls tend to be expressly designed for specific applications. We draw inspiration from a bespoke controller for immersive data visualisation, but decompose this design into a simple, wireless, composable unit featuring two actuated sliders and a rotary encoder. Through these controller units, we explore the interaction opportunities around actuated sliders; supporting precise selection, infinite scrolling, adaptive data representations, and rich haptic feedback; all within a mode-less interaction space. We demonstrate the controllers' use for simple, ad hoc desktop interaction,before moving on to more complex, multi-dimensional interactions in VR and AR. We show that the flexibility and composability of these actuated controllers provides an emergent design space which covers the range of interactive dynamics for visual analysis. In a user study involving pairs performing collaborative visual analysis tasks in mixed-reality, our participants were able to easily compose rich visualisations, make insights and discuss their findings.},\r\njournal = {Proc. ACM Hum.-Comput. Interact.},\r\nmonth = {nov},\r\narticleno = {501},\r\nnumpages = {23},\r\nkeywords = {data visualization, embodied interfaces}\r\n}","notes":"Honourable Mention Award","funding":""},{"Title":"VR Collaboration in Large Companies: An Interview Study on the Role of Avatars","Submission Target":"ISMAR","Date":"2021-11-04","Type":"Short Paper","First Author":"Natalie Hube","Other Authors":"Katrin Angerbauer, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair","Key (e.g. for file names)":"hube2021ismar","Publisher URL (official)":"https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Collaboration is essential in companies and often physical presence is required, thus, more and more Virtual Reality (VR) systems are used to work together remotely. To support social interaction, human representations in form of avatars are used in collaborative virtual environment (CVE) tools. However, up to now, the avatar representations often are limited in their design and functionality, which may hinder effective collaboration. In our interview study, we explored the status quo of VR collaboration in a large automotive company setting with a special focus on the role of avatars. We collected inter-view data from 21 participants, from which we identified challenges of current avatar representations used in our setting. Based on these findings, we discuss design suggestions for avatars in a company setting, which aim to improve social interaction. As opposed to state-of-the-art research, we found that users within the context of a large automotive company have an altered need with respect to avatar representations.","bibtex":"@inproceedings{hube2021ismar,\r\n\tAuthor = {Natalie Hube and Katrin Angerbauer and Daniel Pohlandt and Kresimir Vidackovic and Michael Sedlmair},\r\n\tTitle = {VR Collaboration in Large Companies: An Interview Study on the Role of Avatars},\r\n\tBooktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct, Short paper)},\r\n\tpages = {139--144},\r\n\turl = {https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037},\r\n  \tdoi = {10.1109/ISMAR-Adjunct54149.2021.00037},\r\n\tYear = {2021}\r\n}","notes":"","funding":""},{"Title":"DaRt: Generative Art using Dimensionality Reduction Algorithms","Submission Target":"VIS","Date":"2021-10-26","Type":"Pictorial","First Author":"Rene Cutura","Other Authors":"Katrin Angerbauer, Frank Heyen, Natalie Hube, Michael Sedlmair","Key (e.g. for file names)":"cutura2021visap","Publisher URL (official)":"https://doi.org/10.1109/VISAP52981.2021.00013","url2":"","PDF URL (public)":"https://renecutura.eu/pdfs/DaRt.pdf","Video":"https://youtu.be/pOcksJOiAPw","Video2":"","Supplemental":"","Acknowledgements":"Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 251654672 - TRR 161","Abstract":"Dimensionality Reduction (DR) is a popular technique that is often used in Machine Learning and Visualization communities to analyze high-dimensional data. The approach is empirically proven to be powerful for uncovering previously unseen structures in the data. While observing the results of the intermediate optimization steps of DR algorithms, we coincidently discovered the artistic beauty of the DR process. With enthusiasm for the beauty, we decided to look at DR from a generative art lens rather than their technical application aspects and use DR techniques to create artwork. Particularly, we use the optimization process to generate images, by drawing each intermediate step of the optimization process with some opacity over the previous intermediate result. As another alternative input, we used a neural-network model for face-landmark detection, to apply DR to portraits, while maintaining some facial properties, resulting in abstracted facial avatars. In this work, we provide such a collection of such artwork.","bibtex":"@inproceedings{cutura2021dart,\r\n  title={{DaRt}: Generative Art using Dimensionality Reduction Algorithms},\r\n  author={Cutura, Rene and Angerbauer, Katrin and Heyen, Frank and Hube, Natalie and Sedlmair, Michael},\r\n  booktitle={2021 IEEE VIS Arts Program (VISAP)},\r\n  pages={59--72},\r\n  year={2021},\r\n  organization={IEEE},\r\n}","notes":"","funding":""},{"Title":"Does the Layout Really Matter? A Study on Visual Model Accuracy Estimation","Submission Target":"VIS","Date":"2021-10-24","Type":"Short Paper","First Author":"Nicolas Grossmann","Other Authors":"Jürgen Bernard, Michael Sedlmair, Manuela Waldner","Key (e.g. for file names)":"grossmann2021vis","Publisher URL (official)":"https://doi.org/10.1109/VIS49827.2021.9623326","url2":"","PDF URL (public)":"https://arxiv.org/abs/2110.07188","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In visual interactive labeling, users iteratively assign labels to data items until the machine model reaches an acceptable accuracy. A crucial step of this process is to inspect the model’s accuracy and decide whether it is necessary to label additional elements. In scenarios with no or very little labeled data, visual inspection of the predictions is required. Similarity-preserving scatterplots created through a dimensionality reduction algorithm are a common visualization that is used in these cases. Previous studies investigated the effects of layout and image complexity on tasks like labeling. However, model evaluation has not been studied systematically. We present the results of an experiment studying the influence of image complexity and visual grouping of images on model accuracy estimation. We found that users outperform traditional automated approaches when estimating a model’s accuracy. Furthermore, while the complexity of images impacts the overall performance, the layout of the items in the plot has little to no effect on estimations.","bibtex":"@inproceedings{grossmann2021vis,\r\n\tAuthor = {Nicolas Grossmann and J{\\\"u}rgen Bernard and Michael Sedlmair and Manuela Waldner},\r\n\tTitle = {Does the Layout Really Matter? A Study on Visual Model Accuracy Estimation},\r\n\tBooktitle = {IEEE Visualization Conference  (VIS, Short Paper)},\r\n\tpages = {61--65},\r\n\turl = {https://arxiv.org/abs/2110.07188},\r\n  \tdoi = {10.1109/VIS49827.2021.9623326},\r\n\tYear = {2021}\r\n}","notes":"","funding":""},{"Title":"Strive: String-Based Force Feedback for Automotive Engineering","Submission Target":"UIST","Date":"2021-10-10","Type":"Full Paper","First Author":"Alexander Achberger","Other Authors":"Fabian Aust, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair","Key (e.g. for file names)":"achberger2021uist","Publisher URL (official)":"https://doi.org/10.1145/3472749.3474790","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The large potential of force feedback devices for interacting in Virtual Reality (VR) has been illustrated in a plethora of research prototypes. Yet, these devices are still rarely used in practice and it remains an open challenge how to move this research into practice. To that end, we contribute a participatory design study on the use of haptic feedback devices in the automotive industry. Based on a 10-month observing process with 13 engineers, we developed STRIVE, a string-based haptic feedback device. In addition to the design of STRIVE, this process led to a set of requirements for introducing haptic devices into industrial settings, which center around a need for flexibility regarding forces, comfort, and mobility. We evaluated STRIVE with 16 engineers in five different day-to-day automotive VR use cases. The main results show an increased level of trust and perceived safety as well as further challenges towards moving haptics research into practice. ","bibtex":"@inproceedings{achberger2021uist,\r\n\tAuthor = {Alexander Achberger and Fabian Aust and Daniel Pohlandt and Kresimir Vidackovic and Michael Sedlmair},\r\n\tTitle = {{STRIVE}: String-Based Force Feedback for Automotive Engineering},\r\n\tBooktitle = {ACM Symposium on User Interface Software and Technology (UIST)},\r\n\tpages = {841--853},\r\n\turl = {https://doi.org/10.1145/3472749.3474790},\r\n  \tdoi = {10.1145/3472749.3474790},\r\n\tYear = {2021}\r\n} ","notes":"","funding":""},{"Title":"IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines","Submission Target":"TVCG","Date":"2021-09-29","Type":"Full Paper","First Author":"Joscha Eirich","Other Authors":"Jakob Bonart, Dominik Jäckle, Michael Sedlmair, Ute Schmid, Kai Fischbach, Tobias Schreck, Jürgen Bernard","Key (e.g. for file names)":"eirich2021vis","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2021.3114797","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this design study, we present IRVINE, a Visual Analytics (VA) system, which facilitates the analysis of acoustic data to detect and understand previously unknown errors in the manufacturing of electrical engines. In serial manufacturing processes, signatures from acoustic data provide valuable information on how the relationship between multiple produced engines serves to detect and understand previously unknown errors. To analyze such signatures, IRVINE leverages interactive clustering and data labeling techniques, allowing users to analyze clusters of engines with similar signatures, drill down to groups of engines, and select an engine of interest. Furthermore, IRVINE allows to assign labels to engines and clusters and annotate the cause of an error in the acoustic raw measurement of an engine. Since labels and annotations represent valuable knowledge, they are conserved in a knowledge database to be available for other stakeholders. We contribute a design study, where we developed IRVINE in four main iterations with engineers from a company in the automotive sector. To validate IRVINE, we conducted a field study with six domain experts. Our results suggest a high usability and usefulness of IRVINE as part of the improvement of a real-world manufacturing process. Specifically, with IRVINE domain experts were able to label and annotate produced electrical engines more than 30% faster.","bibtex":"@article{eirich2021vis,\r\n\tauthor = {Joscha Eirich and Jakob Bonart and Dominik J{\\\"a}ckle and Michael Sedlmair and Ute Schmid and Kai Fischbach and Tobias Schreck and J{\\\"u}rgen Bernard},\r\n\ttitle = {{IRVINE}: A Design Study on Analyzing Correlation Patterns of Electrical Engines},\r\n\tjournal = {IEEE Trans. Visualization and Computer Graphics (TVCG, Proc. VIS 2021)},\r\n\tnote = {To appear. Best paper award},\r\n\turl = {https://doi.org/10.1109/TVCG.2021.3114797},\r\n  \tdoi = {10.1109/TVCG.2021.3114797},\r\n\tyear = {2021}\r\n}","notes":"Best Paper Award","funding":""},{"Title":"A Design Space for User Interface Elements using Finger Orientation Input","Submission Target":"MuC","Date":"2021-09-13","Type":"Full Paper","First Author":"Jonas Vogelsang","Other Authors":"Francisco Kiss, Sven Mayer","Key (e.g. for file names)":"vogelsang2021a","Publisher URL (official)":"https://doi.org/10.1145/3473856.3473862","url2":"https://dl.acm.org/doi/abs/10.1145/3473856.3473862","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3473856.3473862","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Despite touchscreens being used by billions of people every day, today’s touch-based interactions are limited in their expressiveness as they mostly reduce the rich information of the finger down to a single 2D point. Researchers have proposed using finger orientation as input to overcome these limitations, adding two extra dimensions – the finger’s pitch and yaw angles. While finger orientation has been studied in-depth over the last decade, we describe an updated design space. Therefore, we present expert interviews combined with a literature review to describe the wide range of finger orientation input opportunities. First, we present a comprehensive set of finger orientation input enhanced user interface elements supported by expert interviews. Second, we extract design implications as a result of the additional input parameters. Finally, we introduce a design space for finger orientation input.","bibtex":"@inproceedings{10.1145/3473856.3473862,\r\nauthor = {Vogelsang, Jonas and Kiss, Francisco and Mayer, Sven},\r\ntitle = {A Design Space for User Interface Elements Using Finger Orientation Input},\r\nyear = {2021},\r\nisbn = {9781450386456},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3473856.3473862},\r\ndoi = {10.1145/3473856.3473862},\r\nabstract = {Despite touchscreens being used by billions of people every day, today’s touch-based interactions are limited in their expressiveness as they mostly reduce the rich information of the finger down to a single 2D point. Researchers have proposed using finger orientation as input to overcome these limitations, adding two extra dimensions – the finger’s pitch and yaw angles. While finger orientation has been studied in-depth over the last decade, we describe an updated design space. Therefore, we present expert interviews combined with a literature review to describe the wide range of finger orientation input opportunities. First, we present a comprehensive set of finger orientation input enhanced user interface elements supported by expert interviews. Second, we extract design implications as a result of the additional input parameters. Finally, we introduce a design space for finger orientation input.},\r\nbooktitle = {Proceedings of Mensch Und Computer 2021},\r\npages = {1–10},\r\nnumpages = {10},\r\nkeywords = {finger orientation, touch devices, interaction, design space, touchscreen, interfaces, expert interviews},\r\nlocation = {Ingolstadt, Germany},\r\nseries = {MuC '21}\r\n}","notes":"","funding":""},{"Title":"PropellerHand: Hand-Mounted, Propeller-Based Force Feedback Device","Submission Target":"VINCI","Date":"2021-09-06","Type":"Full Paper","First Author":"Alexander Achberger","Other Authors":"Frank Heyen, Kresimir Vidackovic, Michael Sedlmair","Key (e.g. for file names)":"achberger2021vinci","Publisher URL (official)":"https://doi.org/10.1145/3481549.3481563","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Immersive analytics is a fast growing field that is often applied in virtual reality (VR). VR environments often lack immersion due to missing sensory feedback when interacting with data. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our final version through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality.","bibtex":"@inproceedings{achberger2021vinci,\r\n  author = {Alexander Achberger and Frank Heyen and Kresimir Vidackovic and Michael Sedlmair},\r\n  title = {PropellerHand: {A} Hand-Mounted, Propeller-Based Force Feedback Device},\r\n  booktitle = {International Symposium on Visual Information Communication and Interaction (VINCI)},\r\n  pages     = {4:1--4:8},\r\n  publisher = {ACM},\r\n  year      = {2021},\r\n  url       = {https://doi.org/10.1145/3481549.3481563},\r\n  doi       = {10.1145/3481549.3481563}\r\n}","notes":"","funding":""},{"Title":"Hagrid — Gridify Scatterplots with Hilbert and Gosper Curves","Submission Target":"VINCI","Date":"2021-09-06","Type":"Full Paper","First Author":"Rene Cutura","Other Authors":"\r\nCristina Morariu, Zhanglin Cheng, Yunhai Wang, Daniel Weiskopf, Michael Sedlmair","Key (e.g. for file names)":"cutura2021vinci","Publisher URL (official)":"https://doi.org/10.1145/3481549.3481569","url2":"","PDF URL (public)":"https://renecutura.eu/pdfs/hagrid.pdf","Video":"https://youtu.be/E_XP31_JzGY","Video2":"","Supplemental":"https://renecutura.eu/pdfs/hagrid_supplemental.pdf","Acknowledgements":"This work was supported by the BMK FFG ICT of the Future program via the ViSciPub project (no. 867378), and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161.","Abstract":"A common enhancement of scatterplots represents points as small multiples, glyphs, or thumbnail images. As this encoding often results in overlaps, a general strategy is to alter the position of the data points, for instance, to a grid-like structure. Previous approaches rely on solving expensive optimization problems or on dividing the space that alter the global structure of the scatterplot. To find a good balance between efficiency and neighborhood and layout preservation, we propose Hagrid, a technique that uses space-filling curves (SFCs) to “gridify” a scatterplot without employing expensive collision detection and handling mechanisms. Using SFCs ensures that the points are plotted close to their original position, retaining approximately the same global structure. The resulting scatterplot is mapped onto a rectangular or hexagonal grid, using Hilbert and Gosper curves. We discuss and evaluate the theoretic runtime of our approach and quantitatively compare our approach to three state-of-the-art gridifying approaches, DGrid, Small multiples with gaps SMWG, and CorrelatedMultiples CMDS, in an evaluation comprising 339 scatterplots. Here, we compute several quality measures for neighborhood preservation together with an analysis of the actual runtimes. The main results show that, compared to the best other technique, Hagrid is faster by a factor of four, while achieving similar or even better quality of the gridified layout. Due to its computational efficiency, our approach also allows novel applications of gridifying approaches in interactive settings, such as removing local overlap upon hovering over a scatterplot.","bibtex":"@inproceedings{cutura2021hagrid,\r\n\tauthor = {Cutura, Rene and Morariu, Cristina and Cheng, Zhanglin and Wang, Yunhai and Weiskopf, Daniel and Sedlmair, Michael},\r\n\ttitle = {{Hagrid -- Gridify Scatterplots with Hilbert and Gosper Curves}},\r\n\tyear = {2021},\r\n\tisbn = {9781450386470},\r\n\tpublisher = {Association for Computing Machinery},\r\n\taddress = {New York, NY, USA},\r\n\turl = {https://doi.org/10.1145/3481549.3481569},\r\n\tdoi = {10.1145/3481549.3481569},\r\n\tbooktitle = {The 14th International Symposium on Visual Information Communication and Interaction},\r\n\tarticleno = {1},\r\n\tnumpages = {8},\r\n\tkeywords = {Grid layout, Neighborhood-preserving., Space-filling curve},\r\n\tlocation = {Potsdam, Germany},\r\n\tseries = {VINCI 2021}\r\n}","notes":"","funding":""},{"Title":"Don't Catch It: An Interactive Virtual-Reality Environment to Learn About COVID-19 Measures Using Gamification Elements","Submission Target":"MuC","Date":"2021-09-05","Type":"Demo","First Author":"Christian Krauter","Other Authors":"Jonas Vogelsang, Aimee Sousa Calepso, Katrin Angerbauer, Michael Sedlmair","Key (e.g. for file names)":"krauter2021muc","Publisher URL (official)":"https://doi.org/10.1145/3473856.3474031","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The world is still under the influence of the COVID-19 pandemic. Even though vaccines are deployed as rapidly as possible, it is still necessary to use other measures to reduce the spread of the virus. Measures such as social distancing or wearing a mask receive a lot of criticism. Therefore, we want to demonstrate a serious game to help the players understand these measures better and show them why they are still necessary. The player of the game has to avoid other agents to keep their risk of a COVID-19 infection low. The game uses Virtual Reality through a Head-Mounted-Display to deliver an immersive and enjoyable experience. Gamification elements are used to engage the user with the game while they explore various environments. We also implemented visualizations that help the user with social distancing.","bibtex":"@inproceedings{krauter2021don,\r\n  title        = {Don't Catch It: An Interactive Virtual-Reality Environment to Learn About COVID-19 Measures Using Gamification Elements},\r\n  shorttitle   = {Don't Catch It},\r\n  author       = {Krauter*, Christian and Vogelsang*, Jonas and Sousa Calepso, Aim{\\'e}e and Angerbauer, Katrin and Sedlmair, Michael},\r\n  year         = 2021,\r\n  month        = sep,\r\n  booktitle    = {Proc. Mensch Und Computer},\r\n  series       = {MuC},\r\n  publisher    = {ACM},\r\n  pages        = {593--596},\r\n  doi          = {10.1145/3473856.3474031},\r\n  isbn         = {978-1-4503-8645-6},\r\n  abstract     = {The world is still under the influence of the COVID-19 pandemic. Even though vaccines are deployed as rapidly as possible, it is still necessary to use other measures to reduce the spread of the virus. Measures such as social distancing or wearing a mask receive a lot of criticism. Therefore, we want to demonstrate a serious game to help the players understand these measures better and show them why they are still necessary. The player of the game has to avoid other agents to keep their risk of a COVID-19 infection low. The game uses Virtual Reality through a Head-Mounted-Display to deliver an immersive and enjoyable experience. Gamification elements are used to engage the user with the game while they explore various environments. We also implemented visualizations that help the user with social distancing. * Both authors contributed equally to this research.},\r\n  bibtex_show  = {true},\r\n  preview      = {krauter2021Don_teaser.png},\r\n  pdf          = {krauter2021Don.pdf},\r\n  video        = {https://www.doi.org/10.1145/3473856.3474031}\r\n}","notes":"","funding":""},{"Title":"Illegible Semantics: Exploring the Design Space of Metal Logos","Submission Target":"alt.VIS","Date":"2021-09-03","Type":"Workshop Paper","First Author":"Gerrit J. Rijken","Other Authors":"Rene Cutura, Frank Heyen, Michael Sedlmair, Michael Correll, Jason Dykes, Noeska Smit","Key (e.g. for file names)":"rijken2021illegible","Publisher URL (official)":"https://doi.org/10.48550/arXiv.2109.01688","url2":"","PDF URL (public)":"https://arxiv.org/ftp/arxiv/papers/2109/2109.01688.pdf","Video":"https://www.youtube.com/watch?v=BZOdIhU-mrA","Video2":"","Supplemental":"http://illegiblesemantics.com","Acknowledgements":"","Abstract":"The logos of metal bands can be by turns gaudy, uncouth, or nearly illegible. Yet, these logos work: they communicate sophisticated notions of genre and emotional affect. In this paper we use the design considerations of metal logos to explore the space of “illegible semantics”: the ways that text can communicate information at the cost of readability, which is not always the most important objective. In this work, drawing on formative visualization theory, professional design expertise, and empirical assessments of a corpus ofmetal band logos, we describe a design space of metal logos and present a tool through which logo characteristics can be explored through visualization. We investigate ways in which logo designers imbue their text with meaning and consider opportunities and implications for visualization more widely.","bibtex":"@inproceedings{rijken2021altvis,\r\n\tAuthor = {Gerrit J Rijken and Rene Cutura and Frank Heyen and Michael Sedlmair and Michael Correll and Jason Dykes and Noeska Smit},\r\n\tTitle = {Illegible Semantics: Exploring the Design Space of Metal Logos},\r\n\tBooktitle = {{IEEE VIS} alt.VIS Workshop},\r\n\turl = {https://arxiv.org/abs/2109.01688},\r\n\tYear = {2021}\r\n}","notes":"","funding":""},{"Title":"A Taxonomy of Property Measures to Unify Active Learning and Human-centered Approaches to Data Labeling","Submission Target":"TiiS","Date":"2021-08-31","Type":"Full Paper","First Author":"Jürgen Bernard","Other Authors":" Marco Hutter, Michael Sedlmair, Matthias Zeppelzauer, Tamara Munzner","Key (e.g. for file names)":"bernard2021tiis","Publisher URL (official)":"https://doi.org/10.1145/3439333","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Strategies for selecting the next data instance to label, in service of generating labeled data for machine learning, have been considered separately in the machine learning literature on active learning and in the visual analytics literature on human-centered approaches. We propose a unified design space for instance selection strategies to support detailed and fine-grained analysis covering both of these perspectives. We identify a concise set of 15 properties, namely measureable characteristics of datasets or of machine learning models applied to them, that cover most of the strategies in these literatures. To quantify these properties, we introduce Property Measures (PM) as fine-grained building blocks that can be used to formalize instance selection strategies. In addition, we present a taxonomy of PMs to support the description, evaluation, and generation of PMs across four dimensions: machine learning (ML) Model Output, Instance Relations, Measure Functionality, and Measure Valence. We also create computational infrastructure to support qualitative visual data analysis: a visual analytics explainer for PMs built around an implementation of PMs using cascades of eight atomic functions. It supports eight analysis tasks, covering the analysis of datasets and ML models using visual comparison within and between PMs and groups of PMs, and over time during the interactive labeling process. We iteratively refined the PM taxonomy, the explainer, and the task abstraction in parallel with each other during a two-year formative process, and show evidence of their utility through a summative evaluation with the same infrastructure. This research builds a formal baseline for the better understanding of the commonalities and differences of instance selection strategies, which can serve as the stepping stone for the synthesis of novel strategies in future work.","bibtex":"@article{bernard2021tiis,\r\n\tauthor = {J{\\\"u}rgen Bernard and Marco Hutter and Michael Sedlmair and Matthias Zeppelzauer and Tamara Munzner},\r\n\ttitle = {A Taxonomy of Property Measures to Unify Active Learning and Human-centered Approaches to Data Labeling},\r\n\tjournal = {ACM Transactions on Interactive Intelligent Systems (TiiS)},\r\n\tvolume={11},\r\n    number={3-4},\r\n    pages={1--42},\r\n\turl = {https://doi.org/10.1145/3439333},\r\n  \tdoi = {10.1145/3439333},\r\n\tyear = {2021}\r\n}","notes":"","funding":""},{"Title":"Is IEEE VIS *that* good? On key factors in the initial assessment of manuscript and venue quality","Submission Target":"alt.VIS","Date":"2021-07-30","Type":"","First Author":"Nicholas Spyrison","Other Authors":"Benjamin Lee, Lonni Besançon","Key (e.g. for file names)":"spyrison2021is","Publisher URL (official)":"https://doi.org/10.31219/osf.io/65wm7","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://osf.io/ch6p4/","Acknowledgements":"","Abstract":"Background: Academic performance is at the heart of hiring decisions and funding applications. It is based on a combination of qualitative and quantitative metrics. One of those is the venue in which scholarly publications are published. Depending on the perceived (qualitative) or measured (quantitative) prestige associated with a venue, a specific publication will have more or less weight. \r\n\r\nObjectives: We want to understand how visualization researchers consider the prestige of a venue when looking for papers that they could use in their own manuscripts, and how they determine the prestige of any given venue.\r\n\r\nMethod: We ran an online survey open for 10 days that we sent out to visualization researchers.\r\n\r\nResults: We gathered 46 responses from a sample of convenience. We found that publication venue plays the biggest part in how visualization researchers assess research articles. Interestingly, rating systems and metrics are least important criteria for researchers when assessing the quality of a venue. \r\n\r\nConclusion: We highlight the potential risks around focusing on venue when assessing research articles. We further underline the necessity to discuss with the community on strategies to switch the focus to robustness and reliability to foster better practices and less stressful publishing expectations.\r\n\r\nReproducibility: Data, materials and preregistration available on osf.io/ch6p4/","bibtex":"@misc{spyrison_lee_besancon_2021,\r\n title={\"Is IEEE VIS *that* good?\" On key factors in the initial assessment of manuscript and venue quality},\r\n url={osf.io/65wm7},\r\n DOI={10.31219/osf.io/65wm7},\r\n publisher={OSF Preprints},\r\n author={Spyrison, Nicholas and Lee, Benjamin and Besançon, Lonni},\r\n year={2021},\r\n month={Jul}\r\n}","notes":"","funding":""},{"Title":"The Value of Immersive Visualization","Submission Target":"CG&A","Date":"2021-06-15","Type":"Full Paper","First Author":"Matthias Kraus","Other Authors":"Karsten Klein, Johannes Fuchs, Daniel A Keim, Falk Schreiber, Michael Sedlmair","Key (e.g. for file names)":"kraus2021cga","Publisher URL (official)":"https://doi.org/10.1109/MCG.2021.3075258","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In recent years, research on immersive environments has experienced a new wave of interest, and immersive analytics has been established as a new research field. Every year, a vast amount of different techniques, applications, and user studies are published that focus on employing immersive environments for visualizing and analyzing data. Nevertheless, immersive analytics is still a relatively unexplored field that needs more basic research in many aspects and is still viewed with skepticism. Rightly so, because in our opinion, many researchers do not fully exploit the possibilities offered by immersive environments and, on the contrary, sometimes even overestimate the power of immersive visualizations. Although a growing body of papers has demonstrated individual advantages of immersive analytics for specific tasks and problems, the general benefit of using immersive environments for effective analytic tasks remains controversial. In this article, we reflect on when and how immersion may be appropriate for the analysis and present four guiding scenarios. We report on our experiences, discuss the landscape of assessment strategies, and point out the directions where we believe immersive visualizations have the greatest potential.","bibtex":"@article{kraus2021cga,\r\n\tauthor = {Matthias Kraus and Karsten Klein and Johannes Fuchs and Daniel A Keim and Falk Schreiber and Michael Sedlmair},\r\n\ttitle = {The Value of Immersive Visualization},\r\n    journal={IEEE Computer Graphics and Applications (CG\\&A)},\r\n\tvolume={41},\r\n    number={4},\r\n    pages={125-132},\r\n\turl = {https://doi.org/10.1109/MCG.2021.3075258},\r\n  \tdoi = {10.1109/MCG.2021.3075258},\r\n\tyear = {2021}\r\n}","notes":"","funding":""},{"Title":"Document Domain Randomization for Deep Learning Document Layout Extraction","Submission Target":"ICDAR","Date":"2021-05-20","Type":"Full Paper","First Author":"Meng Ling","Other Authors":"Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert S Laramee, Han-Wei Shen, Jian Wu, C Lee Giles","Key (e.g. for file names)":"ling2021icdar","Publisher URL (official)":"https://doi.org/10.1007/978-3-030-86549-8_32","url2":"","PDF URL (public)":"https://arxiv.org/pdf/2105.14931","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present document domain randomization (DDR), the first successful transfer of convolutional neural networks (CNNs) trained only on graphically rendered pseudo-paper pages to real-world document segmentation. DDR renders pseudo-document pages by modeling randomized textual and non-textual contents of interest, with user-defined layout and font styles to support joint learning of fine-grained classes. We demonstrate competitive results using our DDR approach to extract nine document classes from the benchmark CS-150 and papers published in two domains, namely annual meetings of Association for Computational Linguistics (ACL) and IEEE Visualization (VIS). We compare DDR to conditions of style mismatch, fewer or more noisy samples that are more easily obtained in the real world. We show that high-fidelity semantic information is not necessary to label semantic classes but style mismatch between train and test can lower model accuracy. Using smaller training samples had a slightly detrimental effect. Finally, network models still achieved high test accuracy when correct labels are diluted towards confusing labels; this behavior hold across several classes. ","bibtex":"@inproceedings{ling2021icdar,\r\n\tAuthor = {Meng Ling and Jian Chen and Torsten M{\\\"o}ller and Petra Isenberg and Tobias Isenberg and Michael Sedlmair and Robert S Laramee and Han-Wei Shen and Jian Wu and C Lee Giles},\r\n\tTitle = {Document Domain Randomization for Deep Learning Document Layout Extraction},\r\n\tBooktitle = {Document Analysis and Recognition (ICDAR)},\r\n\tpublisher= {Springer International Publishing},\r\n\tpages = {497--513},\r\n\turl = {https://arxiv.org/abs/2105.14931},\r\n  \tdoi = {10.1007/978-3-030-86549-8_32},\r\n  \tisbn = {978-3-030-86549-8},\r\n\tYear = {2021}\r\n}","notes":"","funding":""},{"Title":"Three Benchmark Datasets for Scholarly Article Layout Analysis","Submission Target":"","Date":"2021-05-20","Type":"Dataset","First Author":"Meng Ling","Other Authors":"Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert Laramee, Han-Wei Shen, Jian Wu, Clyde Lee Giles","Key (e.g. for file names)":"ling2021visimages","Publisher URL (official)":"https://doi.org/10.21227/326q-bf39","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://ieee-dataport.org/open-access/three-benchmark-datasets-scholarly-article-layout-analysis","Acknowledgements":"","Abstract":"This dataset contains three benchmark datasets as part of the scholarly output of an ICDAR 2021 paper: \r\n\r\nMeng Ling, Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert S. Laramee, Han-Wei Shen, Jian Wu, and C. Lee Giles, Document Domain Randomization for Deep Learning Document Layout Extraction, 16th International Conference on Document Analysis and Recognition (ICDAR) 2021. September 5-10, Lausanne, Switzerland. \r\n\r\nThis dataset contains nine class lables: abstract, algorithm, author, body text, caption, equation, figure, table, and title.\r\n\r\n* Dataset 1: CS-150x, an extension of the classical benchmark dataset CS-150 from three classes (figure, table, and caption) to nine classes, 1176 pages, Clark, C., Divvala, S.: Looking beyond text: Extracting figures, tables and captions from com- puter science papers. In: Workshops at the 29th AAAI Conference on Artificial Intelligence (2015), https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10092.\r\n\r\n* Dataset 2: ACL300, 300 randomly sampled articles (or 2508 pages) from the 55,759 papers scraped from the ACL anthology website; https://www.aclweb.org/anthology/.\r\n\r\n* Dataset 3: VIS300, about 10% (or 2619 pages) of the document pages in randomly partitioned articles from 26,350 VIS paper pages published in  Chen, J., Ling, M., Li, R., Isenberg, P., Isenberg, T., Sedlmair, M., Möller, T., Laramee, R.S., Shen, H.W., Wünsche, K., Wang, Q.: VIS30K: A collection of figures and tables from IEEE visualization conference publications. IEEE Trans. Vis. Comput. Graph. 27 (2021), to appear doi: 10.1109/TVCG.2021.3054916.\r\n\r\nThis dataset is also available online at https://web.cse.ohio-state.edu/~chen.8028/ICDAR2021Benchmark/.\r\n","bibtex":"@data{ling2021visimages,\r\n\tauthor = {Meng Ling and Jian Chen and Torsten M{\\\"o}ller and Petra Isenberg and Tobias Isenberg and Michael Sedlmair and Robert Laramee and Han-Wei Shen and Jian Wu and Clyde Lee Giles},\r\n\ttitle = {Three Benchmark Datasets for Scholarly Article Layout Analysis},\r\n\tpublisher = {IEEE Dataport},\r\n\turl = {https://ieee-dataport.org/open-access/three-benchmark-datasets-scholarly-article-layout-analysis},\r\n\tdoi = {10.21227/326q-bf39},\r\n\tYear = {2021}\r\n}","notes":"","funding":""},{"Title":"DumbleDR: Predicting User Preferences of Dimensionality Reduction Projection Quality","Submission Target":"arXiv","Date":"2021-05-19","Type":"","First Author":"Cristina Morariu","Other Authors":"Adrien Bibal, Rene Cutura, Benoit Frenay, Michael Sedlmair","Key (e.g. for file names)":"morariu2021arxiv","Publisher URL (official)":"https://doi.org/10.48550/arXiv.2105.09275","url2":"","PDF URL (public)":"https://arxiv.org/pdf/2105.09275.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional parametrization (e.g. t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D projections is usually qualitatively decided, by setting projections side-by-side and letting human judgment decide which projection is the best. In this work, we propose a quantitative way of evaluating projections, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select 'good' and 'misleading' views between scatterplots of low-level projections of image datasets, simulating the way people usually select projections. We use the study data as labels for a set of quality metrics whose purpose is to discover and quantify what exactly people are looking for when deciding between projections. With this proxy for human judgments, we use it to rank projections on new datasets, explain why they are relevant, and quantify the degree of subjectivity in projections selected.","bibtex":"@techreport{morariu2021arxiv,\r\n\ttitle = {{DumbleDR}: Predicting User Preferences of Dimensionality Reduction Projection Quality},\r\n\tauthor = {Cristina Morariu and Adrien Bibal and Rene Cutura and Benoit Frenay and Michael Sedlmair},\r\n\tInstitution = {{arXiv} preprint},\r\n\tNumber = {arXiv:2105.09275},\r\n\tType = {Technical Report},\r\n\turl = {https://arxiv.org/abs/2105.09275},\r\n\tYear = {2021}\r\n}","notes":"","funding":""},{"Title":"ProSeCo: Visual Analysis of Class Separation Measures and Dataset Characteristics","Submission Target":"C&G","Date":"2021-05-01","Type":"Full Paper","First Author":"Jürgen Bernard","Other Authors":"Marco Hutter, Matthias Zeppelzauer, Michael Sedlmair, Tamara Munzner","Key (e.g. for file names)":"bernard2021proseco","Publisher URL (official)":"https://doi.org/10.1016/j.cag.2021.03.004","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Class separation is an important concept in machine learning and visual analytics. We address the visual analysis of class separation measures for both high-dimensional data and its corresponding projections into 2D through dimensionality reduction (DR) methods. Although a plethora of separation measures have been proposed, it is difficult to compare class separation between multiple datasets with different characteristics, multiple separation measures, and multiple DR methods. We present ProSeCo, an interactive visualization approach to support comparison between up to 20 class separation measures and up to 4 DR methods, with respect to any of 7 dataset characteristics: dataset size, dataset dimensions, class counts, class size variability, class size skewness, outlieriness, and real-world vs. synthetically generated data. ProSeCo supports (1) comparing across measures, (2) comparing high-dimensional to dimensionally-reduced 2D data across measures, (3) comparing between different DR methods across measures, (4) partitioning with respect to a dataset characteristic, (5) comparing partitions for a selected characteristic across measures, and (6) inspecting individual datasets in detail. We demonstrate the utility of ProSeCo in two usage scenarios, using datasets [1] posted at https://osf.io/epcf9/.","bibtex":"@article{bernard2021proseco,\r\n\tauthor = {J{\\\"u}rgen Bernard and Marco Hutter and Matthias Zeppelzauer and Michael Sedlmair and Tamara Munzner},\r\n\ttitle = {{ProSeCo}: Visual analysis of class separation measures and dataset characteristics},\r\n\tjournal = {Computers \\& Graphics},\r\n\tpublisher = {Elsevier},\r\n  \tvolume = {96},\r\n  \tpages = {48--60},\r\n  \tyear = {2021},\r\n\turl = {https://doi.org/10.1016/j.cag.2021.03.004},\r\n  \tdoi = {10.1016/j.cag.2021.03.004}\r\n}","notes":"","funding":""},{"Title":"A Visual Analysis Method of Randomness for Classifying and Ranking Pseudo-Random Number Generators","Submission Target":"IS","Date":"2021-04-01","Type":"Full Paper","First Author":"Jeaneth Machicao","Other Authors":"Quynh Quang Ngo, Vladimir Molchanov, Lars Linsen, Odemir M Bruno","Key (e.g. for file names)":"machicao2021a","Publisher URL (official)":"https://doi.org/10.1016/j.ins.2020.10.041","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The development of new pseudo-random number generators (PRNGs) has steadily increased over the years. Commonly, PRNGs’ randomness is “measured” by using statistical pass/fail suite tests, but the question remains, which PRNG is the best when compared to others. Existing randomness tests lack means for comparisons between PRNGs, since they are not quantitatively analysing. It is, therefore, an important task to analyze the quality of randomness for each PRNG, or, in general, comparing the randomness property among PRNGs. In this paper, we propose a novel visual approach to analyze PRNGs randomness allowing for a ranking comparison concerning the PRNGs’ quality. Our analysis approach is applied to ensembles of time series which are outcomes of different PRNG runs. The ensembles are generated by using a single PRNG method with different parameter settings or by using different PRNG methods. We propose a similarity metric for PRNG time series for randomness and apply it within an interactive visual approach for analyzing similarities of PRNG time series and relating them to an optimal result of perfect randomness. The interactive analysis leads to an unsupervised classification, from which respective conclusions about the impact of the PRNGs’ parameters or rankings of PRNGs on randomness are derived. We report new findings using our approach in a study of randomness for state-of-the-art numerical PRNGs such as LCG, PCG, SplitMix, Mersenne Twister, and RANDU as well as chaos-based PRNG families such as K-Logistic map and K-Tent map with varying parameter K.","bibtex":"@article{DBLP:journals/isci/MachicaoNMLB21,\r\n  author    = {Jeaneth Machicao and\r\n               Quynh Quang Ngo and\r\n               Vladimir Molchanov and\r\n               Lars Linsen and\r\n               Odemir M. Bruno},\r\n  title     = {A visual analysis method of randomness for classifying and ranking\r\n               pseudo-random number generators},\r\n  journal   = {Inf. Sci.},\r\n  volume    = {558},\r\n  pages     = {1--20},\r\n  year      = {2021},\r\n  url       = {https://doi.org/10.1016/j.ins.2020.10.041},\r\n  doi       = {10.1016/j.ins.2020.10.041},\r\n  timestamp = {Thu, 29 Apr 2021 15:12:58 +0200},\r\n  biburl    = {https://dblp.org/rec/journals/isci/MachicaoNMLB21.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}","notes":"","funding":""},{"Title":"VIS30K: A Collection of Figures and Tables from IEEE Visualization Conference Publications","Submission Target":"TVCG","Date":"2021-01-27","Type":"Full Paper","First Author":"Jian Chen","Other Authors":"Meng Ling, Rui Li, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Torsten Möller, Robert S Laramee, Han-Wei Shen, Katharina Wünsche, Qiru Wang","Key (e.g. for file names)":"chen2021tvcg","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2021.3054916","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present the VIS30K dataset, a collection of 29,689 images that represents 30 years of figures and tables from each track of the IEEE Visualization conference series (Vis, SciVis, InfoVis, VAST). VIS30K's comprehensive coverage of the scientific literature in visualization not only reflects the progress of the field but also enables researchers to study the evolution of the state-of-the-art and to find relevant work based on graphical content. We describe the dataset and our semi-automatic collection process, which couples convolutional neural networks (CNN) with curation. Extracting figures and tables semi-automatically allows us to verify that no images are overlooked or extracted erroneously. To improve quality further, we engaged in a peer-search process for high-quality figures from early IEEE Visualization papers. With the resulting data, we also contribute VISImageNavigator (VIN, visimagenavigator.github.io ), a web-based tool that facilitates searching and exploring VIS30K by author names, paper keywords, title and abstract, and years.","bibtex":"@article{chen2021tvcg,\r\n\tauthor = {Jian Chen and Meng Ling and Rui Li and Petra Isenberg and Tobias Isenberg and Michael Sedlmair and Torsten M{\\\"o}ller and Robert S Laramee and Han-Wei Shen and Katharina W{\\\"u}nsche and Qiru Wang},\r\n\ttitle = {{VIS30K}: A collection of figures and tables from {IEEE} visualization conference publications},\r\n\tjournal = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},\r\n  \tyear = {2021},\r\n  \tvolume = {27},\r\n  \tnumber = {9},\r\n  \tpages = {3826--3833},\r\n  \turl = {https://doi.org/10.1109/TVCG.2021.3054916},\r\n  \tdoi = {10.1109/TVCG.2021.3054916}\r\n}","notes":"","funding":""},{"Title":"Linking Unstructured Evidence to Structured Observations","Submission Target":"IV","Date":"2021-01-14","Type":"Full Paper","First Author":"Manuela Waldner","Other Authors":"Thomas Geymayer, Dieter Schmalstieg, Michael Sedlmair","Key (e.g. for file names)":"waldner2021iv","Publisher URL (official)":"https://doi.org/10.1177/1473871620986249","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Many professionals, like journalists, writers, or consultants, need to acquire information from various sources, make sense of this unstructured evidence, structure their observations, and finally create and deliver their product, such as a report or a presentation. In formative interviews, we found that tools allowing structuring of observations are often disconnected from the corresponding evidence. Therefore, we designed a sensemaking environment with a flexible observation graph that visually ties together evidence in unstructured documents with the user’s structured knowledge. This is achieved through bi-directional deep links between highlighted document portions and nodes in the observation graph. In a controlled study, we compared users’ sensemaking strategies using either the observation graph or a simple text editor on a large display. Results show that the observation graph represents a holistic, compact representation of users’ observations, which can be linked to unstructured evidence on demand. In contrast, users taking textual notes required much more display space to spatially organize source documents containing unstructured evidence. This implies that spatial organization is a powerful strategy to structure observations even if the available space is limited.","bibtex":"@article{waldner2021iv,\r\n\tauthor = {Manuela Waldner and Thomas Geymayer and Dieter Schmalstieg and Michael Sedlmair},\r\n\ttitle = {Linking unstructured evidence to structured observations},\r\n\tjournal = {Information Visualization},\r\n\tpublisher = {{SAGE} Publications},\r\n\tvolume = {20},\r\n    number = {1},\r\n    pages = {47--65},\r\n    url = {https://doi.org/10.1177/1473871620986249},\r\n    doi = {10.1177/1473871620986249},\r\n\tyear = {2021}\r\n}","notes":"","funding":""},{"Title":"A Comparative Study of Orientation Support Tools in Virtual Reality Environments with Virtual Teleportation","Submission Target":"ISMAR","Date":"2020-12-14","Type":"","First Author":"Matthias Kraus","Other Authors":"Hanna Schaefer, Philipp Meschenmoser, Daniel Schweitzer, Daniel Keim, Michael Sedlmair, Johannes Fuchs","Key (e.g. for file names)":"kraus2020ismar","Publisher URL (official)":"https://doi.org/10.1109/ISMAR50242.2020.00046","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Movement-compensating interactions like teleportation are commonly deployed techniques in virtual reality environments. Although practical, they tend to cause disorientation while navigating. Previous studies show the effectiveness of orientation-supporting tools, such as trails, in reducing such disorientation and reveal different strengths and weaknesses of individual tools. However, to date, there is a lack of a systematic comparison of those tools when teleportation is used as a movement-compensating technique, in particular under consideration of different tasks. In this paper, we compare the effects of three orientation-supporting tools, namely minimap, trail, and heatmap. We conducted a quantitative user study with 48 participants to investigate the accuracy and efficiency when executing four exploration and search tasks. As dependent variables, task performance, completion time, space coverage, amount of revisiting, retracing time, and memorability were measured. Overall, our results indicate that orientation-supporting tools improve task completion times and revisiting behavior. The trail and heatmap tools were particularly useful for speed-focused tasks, minimal revisiting, and space coverage. The minimap increased memorability and especially supported retracing tasks. These results suggest that virtual reality systems should provide orientation aid tailored to the specific tasks of the users.","bibtex":"@INPROCEEDINGS{9284697,\r\n  author={Kraus, Matthias and Schäfer, Hanna and Meschenmoser, Philipp and Schweitzer, Daniel and Keim, Daniel A. and Sedlmair, Michael and Fuchs, Johannes},\r\n  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, \r\n  title={A Comparative Study of Orientation Support Tools in Virtual Reality Environments with Virtual Teleportation}, \r\n  year={2020},\r\n  volume={},\r\n  number={},\r\n  pages={227-238},\r\n  doi={10.1109/ISMAR50242.2020.00046}}\r\n","notes":"","funding":""},{"Title":"Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009-2019)","Submission Target":"ISMAR","Date":"2020-12-14","Type":"","First Author":"Leonel Merino","Other Authors":"Magdalena Schwarzl, Matthias Kraus, Michael Sedlmair, Dieter Schmalstieg, Daniel Weiskopf","Key (e.g. for file names)":"merino2020ismar","Publisher URL (official)":"https://doi.org/10.1109/ISMAR50242.2020.00069","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present a systematic review of 45S papers that report on evaluations in mixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST over a span of 11 years (2009-2019). Our goal is to provide guidance for future evaluations of MR/AR approaches. To this end, we characterize publications by paper type (e.g., technique, design study), research topic (e.g., tracking, rendering), evaluation scenario (e.g., algorithm performance, user performance), cognitive aspects (e.g., perception, emotion), and the context in which evaluations were conducted (e.g., lab vs. in-thewild). We found a strong coupling of types, topics, and scenarios. We observe two groups: (a) technology-centric performance evaluations of algorithms that focus on improving tracking, displays, reconstruction, rendering, and calibration, and (b) human-centric studies that analyze implications of applications and design, human factors on perception, usability, decision making, emotion, and attention. Amongst the 458 papers, we identified 248 user studies that involved 5,761 participants in total, of whom only 1,619 were identified as female. We identified 43 data collection methods used to analyze 10 cognitive aspects. We found nine objective methods, and eight methods that support qualitative analysis. A majority (216/248) of user studies are conducted in a laboratory setting. Often (138/248), such studies involve participants in a static way. However, we also found a fair number (30/248) of in-the-wild studies that involve participants in a mobile fashion. We consider this paper to be relevant to academia and industry alike in presenting the state-of-the-art and guiding the steps to designing, conducting, and analyzing results of evaluations in MR/AR.","bibtex":"@INPROCEEDINGS{9284762,\r\n  author={Merino, Leonel and Schwarzl, Magdalena and Kraus, Matthias and Sedlmair, Michael and Schmalstieg, Dieter and Weiskopf, Daniel},\r\n  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, \r\n  title={Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009-2019)}, \r\n  year={2020},\r\n  volume={},\r\n  number={},\r\n  pages={438-451},\r\n  doi={10.1109/ISMAR50242.2020.00069},\r\n  ISSN={1554-7868},\r\n  month={Nov},}","notes":"","funding":""},{"Title":"Challenges in Evaluating Interactive Visual Machine Learning Systems","Submission Target":"CG&A","Date":"2020-11-23","Type":"","First Author":"Nadia Boukhelifa","Other Authors":"Anastasia Bezerianos, Remco Chang, Chris Collins, Steven Drucker, Alex Endert, Jessica Hullman, Chris North, Michael Sedlmair","Key (e.g. for file names)":"boukhelifa2020eviva","Publisher URL (official)":"https://doi.org/10.1109/MCG.2020.3017064","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In interactive visual machine learning (IVML), humans and machine learning algorithms collaborate to achieve tasks mediated by interactive visual interfaces. This human-in-the-loop approach to machine learning brings forth not only numerous intelligibility, trust, and usability issues, but also many open questions with respect to the evaluation of the IVML system, both as separate components, and as a holistic entity that includes both human and machine intelligence. This article describes the challenges and research gaps identified in an IEEE VIS workshop on the evaluation of IVML systems.","bibtex":"@ARTICLE{9238590,\r\n  author={Boukhelifa, N. and Bezerianos, A. and Chang, R. and Collins, C. and Drucker, S. and Endert, A. and Hullman, J. and North, C. and Sedlmair, M.},\r\n  journal={IEEE Computer Graphics and Applications}, \r\n  title={Challenges in Evaluating Interactive Visual Machine Learning Systems}, \r\n  year={2020},\r\n  volume={40},\r\n  number={6},\r\n  pages={88-96},\r\n  doi={10.1109/MCG.2020.3017064}}\r\n","notes":"","funding":""},{"Title":"Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion","Submission Target":"ISMAR","Date":"2020-11-09","Type":"Poster / Short Paper","First Author":"Natalie Hube","Other Authors":"Oliver Lenz, Lars Engeln, Rainer Groh, Michael Sedlmair","Key (e.g. for file names)":"hube2020comparing","Publisher URL (official)":"https://doi.org/10.1109/ISMAR-Adjunct51615.2020.00023","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.","bibtex":"@INPROCEEDINGS{9288476,\r\n  author={Hube, Natalie and Lenz, Oliver and Engeln, Lars and Groh, Rainer and Sedlmair, Michael},\r\n  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, \r\n  title={Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion}, \r\n  year={2020},\r\n  pages={30-35},\r\n  doi={10.1109/ISMAR-Adjunct51615.2020.00023}}\r\n","notes":"","funding":""},{"Title":"Perspective Matters: Design Implications for Motion Guidance in Mixed Reality","Submission Target":"ISMAR","Date":"2020-11-09","Type":"Full Paper","First Author":"Xingyao Yu","Other Authors":"Katrin Angerbauer, Peter Mohr, Denis Kalkofen, Michael Sedlmair","Key (e.g. for file names)":"yu2020perspective","Publisher URL (official)":"https://doi.org/10.1109/ISMAR50242.2020.00085","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We investigate how Mixed Reality (MR) can be used to guide human body motions, such as in physiotherapy, dancing, or workout applications. While first MR prototypes have shown promising results, many dimensions of the design space behind such applications remain largely unexplored. To better understand this design space, we approach the topic from different angles by contributing three user studies. In particular, we take a closer look at the influence of the perspective, the characteristics of motions, and visual guidance on different user performance measures. Our results indicate that a first-person perspective performs best for all visible motions, whereas the type of visual instruction plays a minor role. From our results we compile a set of considerations that can guide future work on the design of instructions, evaluations, and the technical setup of MR motion guidance systems.","bibtex":"@INPROCEEDINGS{9284729,\r\n  author={Yu, Xingyao and Angerbauer, Katrin and Mohr, Peter and Kalkofen, Denis and Sedlmair, Michael},\r\n  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, \r\n  title={Perspective Matters: Design Implications for Motion Guidance in Mixed Reality}, \r\n  year={2020},\r\n  pages={577-587},\r\n  doi={10.1109/ISMAR50242.2020.00085}}\r\n","notes":"","funding":""},{"Title":"Pipelines Bent, Pipelines Broken: Interdisciplinary Self-Reflection on the Impact of COVID-19 on Current and Future Research (Position Paper)","Submission Target":"BELIV","Date":"2020-10-25","Type":"Workshop","First Author":"Priscilla Balestrucci","Other Authors":"Katrin Angerbauer, Cristina Morariu, Robin Welsch, Lewis L Chuang, Daniel Weiskopf, Marc O Ernst, Michael Sedlmair","Key (e.g. for file names)":"balestrucci2020beliv","Publisher URL (official)":"https://doi.org/10.1109/BELIV51497.2020.00009","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Among the many changes brought about by the COVID-19 pandemic, one of the most pressing for scientific research concerns user testing. For the researchers who conduct studies with human participants, the requirements for social distancing have created a need for reflecting on methodologies that previously seemed relatively straightforward. It has become clear from the emerging literature on the topic and from first-hand experiences of researchers that the restrictions due to the pandemic affect every aspect of the research pipeline. The current paper offers an initial reflection on user-based research, drawing on the authors' own experiences and on the results of a survey that was conducted among researchers in different disciplines, primarily psychology, human-computer interaction (HCI), and visualization communities. While this sampling of researchers is by no means comprehensive, the multi-disciplinary approach and the consideration of different aspects of the research pipeline allow us to examine current and future challenges for user-based research. Through an exploration of these issues, this paper also invites others in the VIS-as well as in the wider-research community, to reflect on and discuss the ways in which the current crisis might also present new and previously unexplored opportunities.","bibtex":"@INPROCEEDINGS{9307759,\r\n  author={Balestrucci, Priscilla and Angerbauer, Katrin and Morariu, Cristina and Welsch, Robin and Chuang, Lewis L. and Weiskopf, Daniel and Ernst, Marc O. and Sedlmair, Michael},\r\n  booktitle={2020 IEEE Workshop on Evaluation and Beyond - Methodological Approaches to Visualization (BELIV)}, \r\n  title={Pipelines Bent, Pipelines Broken: Interdisciplinary Self-Reflection on the Impact of COVID-19 on Current and Future Research (Position Paper)}, \r\n  year={2020},\r\n  pages={11-18},\r\n  doi={10.1109/BELIV51497.2020.00009}}\r\n","notes":"","funding":""},{"Title":"DRUIDJS — A JavaScript Library for Dimensionality Reduction","Submission Target":"VIS","Date":"2020-10-25","Type":"Short Paper","First Author":"Rene Cutura","Other Authors":"Christoph Kralj, Michael Sedlmair","Key (e.g. for file names)":"cutura2020druidjs","Publisher URL (official)":"https://doi.org/10.1109/VIS47514.2020.00029","url2":"","PDF URL (public)":"https://renecutura.eu/pdfs/Druid.pdf","Video":"https://youtu.be/LyiqHl4rq34","Video2":"","Supplemental":"https://renecutura.eu/pdfs/Druid_Supp.pdf","Acknowledgements":"This work was supported by the BMVIT ICT of the Future program via the ViSciPub project (no. 867378) and handled by the FFG.","Abstract":"Dimensionality reduction (DR) is a widely used technique for visualization. Nowadays, many of these visualizations are developed for the web, most commonly using JavaScript as the underlying programming language. So far, only few DR methods have a JavaScript implementation though, necessitating developers to write wrappers around implementations in other languages. In addition, those DR methods that exist in JavaScript libraries, such as PCA, t-SNE, and UMAP, do not offer consistent programming interfaces, hampering the quick integration of different methods. Toward a coherent and comprehensive DR programming framework, we developed an open source JavaScript library named DruidJS. Our library contains implementations of ten different DR algorithms, as well as the required linear algebra techniques, tools, and utilities.","bibtex":"@inproceedings{cutura2020druid,\r\n  title={{DRUIDJS — A JavaScript Library for Dimensionality Reduction}},\r\n  author={Cutura, Rene and Kralj, Christoph and Sedlmair, Michael},\r\n  booktitle={2020 IEEE Visualization Conference (VIS)},\r\n  pages={111--115},\r\n  year={2020},\r\n  organization={IEEE}\r\n}","notes":"","funding":""},{"Title":"Supporting Music Education through Visualizations of MIDI Recordings","Submission Target":"VIS","Date":"2020-10-25","Type":"Poster","First Author":"Frank Heyen","Other Authors":"Michael Sedlmair","Key (e.g. for file names)":"heyen2020supporting","Publisher URL (official)":"","url2":"","PDF URL (public)":"https://fheyen.github.io/files/ieee_vis_2020_poster/IEEE_VIS_Heyen_2020_Supporting.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Musicians mostly have to rely on their ears when they want to analyze what they play, for example to detect errors. Since hearing is sequential, it is not possible to quickly grasp an overview over one or multiple recordings of a whole piece of music at once. We therefore propose various visualizations that allow analyzing errors and stylistic variance. Our current approach focuses on rhythm and uses MIDI data for simplicity.","bibtex":"@misc{heyen2020supporting,\r\n    title={Supporting Music Education through Visualizations of MIDI Recordings},\r\n    author={Frank Heyen, Michael Sedlmair},\r\n    year={2020},\r\n    howpublished={IEEE Visualization Conference Poster},\r\n    abstract = {Musicians mostly have to rely on their ears when they want to analyze what they play, for example to detect errors. Since hearing is sequential, it is not possible to quickly grasp an overview over one or multiple recordings of a whole piece of music at once. We therefore propose various visualizations that allow analyzing errors and stylistic variance. Our current approach focuses on rhythm and uses MIDI data for simplicity.}\r\n}","notes":"","funding":""},{"Title":"Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality","Submission Target":"VIS","Date":"2020-10-19","Type":"","First Author":"Benjamin Lee","Other Authors":"Dave Brown, Bongshin Lee, Christophe Hurter, Steven Drucker, Tim Dwyer","Key (e.g. for file names)":"lee2020data","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2020.3030435","url2":"https://ieeexplore.ieee.org/abstract/document/9229242","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=XmYNISBjL_Q","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.","bibtex":"@ARTICLE{9229242,\r\n  author={Lee, Benjamin and Brown, Dave and Lee, Bongshin and Hurter, Christophe and Drucker, Steven and Dwyer, Tim},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality}, \r\n  year={2021},\r\n  volume={27},\r\n  number={2},\r\n  pages={1095-1105},\r\n  abstract={A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.},\r\n  doi={10.1109/TVCG.2020.3030435},\r\n  ISSN={1941-0506},\r\n  month={Feb},}\r\n","notes":"Honourable Mention Award","funding":""},{"Title":"Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes","Submission Target":"VIS","Date":"2020-10-15","Type":"Full Paper","First Author":"Maximilian Weiß","Other Authors":"Katrin Angerbauer, Alexandra Voit, Magdalena Schwarzl, Michael Sedlmair, Sven Mayer","Key (e.g. for file names)":"weiss2020revisited","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2020.3030400","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.","bibtex":"@ARTICLE{9225008,\r\n  author={Weiß, Maximilian and Angerbauer, Katrin and Voit, Alexandra and Schwarzl, Magdalena and Sedlmair, Michael and Mayer, Sven},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes}, \r\n  year={2021},\r\n  volume={27},\r\n  number={2},\r\n  pages={1204-1213},\r\n  doi={10.1109/TVCG.2020.3030400}}\r\n","notes":"","funding":""},{"Title":"SineStream: Improving the Readability of Streamgraphs by Minimizing Sine Illusion Effects","Submission Target":"TVCG","Date":"2020-10-13","Type":"","First Author":"Chuan Bu","Other Authors":"Quanjie Zhang, Qianwen Wang, Jian Zhang, Oliver Deussen, Michael Sedlmair, Yunhai Wang","Key (e.g. for file names)":"bu2021sinestream","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2020.3030404","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper, we propose SineStream, a new variant of streamgraphs that improves their readability by minimizing sine illusion effects. Such effects reflect the tendency of humans to take the orthogonal rather than the vertical distance between two curves as their distance. In SineStream, we connect the readability of streamgraphs with minimizing sine illusions and by doing so provide a perceptual foundation for their design. As the geometry of a streamgraph is controlled by its baseline (the bottom-most curve) and the ordering of the layers, we re-interpret baseline computation and layer ordering algorithms in terms of reducing sine illusion effects. For baseline computation, we improve previous methods by introducing a Gaussian weight to penalize layers with large thickness changes. For layer ordering, three design requirements are proposed and implemented through a hierarchical clustering algorithm. Quantitative experiments and user studies demonstrate that SineStream improves the readability and aesthetics of streamgraphs compared to state-of-the-art methods.","bibtex":"@ARTICLE{9222035,\r\n  author={Bu, Chuan and Zhang, Quanjie and Wang, Qianwen and Zhang, Jian and Sedlmair, Michael and Deussen, Oliver and Wang, Yunhai},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={SineStream: Improving the Readability of Streamgraphs by Minimizing Sine Illusion Effects}, \r\n  year={2021},\r\n  volume={27},\r\n  number={2},\r\n  pages={1634-1643},\r\n  doi={10.1109/TVCG.2020.3030404}}\r\n","notes":"","funding":""},{"Title":"Palettailor: Discriminable Colorization for Categorical Data","Submission Target":"TVCG","Date":"2020-10-13","Type":"","First Author":"Kecheng Lu","Other Authors":"Mi Feng, Xin Chen, Michael Sedlmair, Oliver Deussen, Dani Lischinski, Zhanglin Cheng, Yunhai Wang","Key (e.g. for file names)":"kecheng2021palettaoilor","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2020.3030406","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present an integrated approach for creating and assigning color palettes to different visualizations such as multi-class scatterplots, line, and bar charts. While other methods separate the creation of colors from their assignment, our approach takes data characteristics into account to produce color palettes, which are then assigned in a way that fosters better visual discrimination of classes. To do so, we use a customized optimization based on simulated annealing to maximize the combination of three carefully designed color scoring functions: point distinctness, name difference, and color discrimination. We compare our approach to state-of-the-art palettes with a controlled user study for scatterplots and line charts, furthermore we performed a case study. Our results show that Palettailor, as a fully-automated approach, generates color palettes with a higher discrimination quality than existing approaches. The efficiency of our optimization allows us also to incorporate user modifications into the color selection process.","bibtex":"@ARTICLE{9222351,\r\n  author={Lu, Kecheng and Feng, Mi and Chen, Xin and Sedlmair, Michael and Deussen, Oliver and Lischinski, Dani and Cheng, Zhanglin and Wang, Yunhai},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Palettailor: Discriminable Colorization for Categorical Data}, \r\n  year={2021},\r\n  volume={27},\r\n  number={2},\r\n  pages={475-484},\r\n  doi={10.1109/TVCG.2020.3030406}}\r\n","notes":"","funding":""},{"Title":"Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment","Submission Target":"VIS","Date":"2020-10-13","Type":"","First Author":"Benjamin Lee","Other Authors":"Xiaoyun Hu, Maxime Cordeil, Arnaud Prouzeau, Bernhard Jenny, Tim Dwyer","Key (e.g. for file names)":"lee2020shared","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2020.3030450","url2":"https://ieeexplore.ieee.org/abstract/document/9222346","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=0ksaAnu9kog","Video2":"","Supplemental":"https://sites.google.com/monash.edu/shared-surfaces-and-spaces","Acknowledgements":"","Abstract":"Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.","bibtex":"@ARTICLE{9222346,\r\n  author={Lee, Benjamin and Hu, Xiaoyun and Cordeil, Maxime and Prouzeau, Arnaud and Jenny, Bernhard and Dwyer, Tim},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment}, \r\n  year={2021},\r\n  volume={27},\r\n  number={2},\r\n  pages={1171-1181},\r\n  abstract={Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.},\r\n  doi={10.1109/TVCG.2020.3030450},\r\n  ISSN={1941-0506},\r\n  month={Feb},}\r\n","notes":"","funding":""},{"Title":"Interactive Generation of 1D Emeddings from 2D Multi-dimensional Data Projections","Submission Target":"VMV","Date":"2020-10-06","Type":"Full Paper","First Author":"Quynh Quang Ngo","Other Authors":"Lars Linsen","Key (e.g. for file names)":"ngo2020interactive","Publisher URL (official)":"https://doi.org/10.2312/vmv.20201190","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Visual analysis of multi-dimensional data is commonly supported by mapping the data to a 2D embedding. When analyzing a sequence of multi-dimensional data, e.g., in case of temporal data, the usage of 1D embeddings allows for plotting the entire sequence in a 2D layout. Despite the good performance in generating 2D embeddings, 1D embeddings often exhibit a much lower quality for pattern recognition tasks. We propose to overcome the issue by involving the user to generate 1D embeddings of multi-dimensional data in a two-step procedure: We first generate a 2D embedding and then leave the task of reducing the 2D to a 1D embedding to the user. We demonstrate that an interactive generation of 1D embeddings from 2D projected views can be performed efficiently, effectively, and targeted towards an analysis task. We compare the performance of our approach against automatically generated 1D and 2D embeddings involving a user study for our interactive approach. We test the 1D approaches when being applied to time-varying multi-dimensional data.","bibtex":"@inproceedings{DBLP:conf/vmv/NgoL20,\r\n  author    = {Quynh Quang Ngo and\r\n               Lars Linsen},\r\n  editor    = {Jens H. Kr{\\\"{u}}ger and\r\n               Matthias Nie{\\ss}ner and\r\n               J{\\\"{o}}rg St{\\\"{u}}ckler},\r\n  title     = {Interactive Generation of 1D Embeddings from 2D Multi-dimensional\r\n               Data Projections},\r\n  booktitle = {25th International Symposium on Vision, Modeling and Visualization,\r\n               {VMV} 2020, T{\\\"{u}}bingen, Germany, September 28 - October 1,\r\n               2020},\r\n  pages     = {79--87},\r\n  publisher = {Eurographics Association},\r\n  year      = {2020},\r\n  url       = {https://doi.org/10.2312/vmv.20201190},\r\n  doi       = {10.2312/vmv.20201190},\r\n  timestamp = {Tue, 06 Oct 2020 16:51:11 +0200},\r\n  biburl    = {https://dblp.org/rec/conf/vmv/NgoL20.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}","notes":"","funding":""},{"Title":"Caarvida: Visual Analytics for Test Drive Videos","Submission Target":"AVI ","Date":"2020-09-28","Type":"Full Paper","First Author":"Alexander Achberger","Other Authors":"Rene Cutura, Oguzhan Türksoy, Michael Sedlmair","Key (e.g. for file names)":"achberger2020caarvida","Publisher URL (official)":"https://doi.org/10.1145/3399715.3399862","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.","bibtex":"@inproceedings{10.1145/3399715.3399862,\r\nauthor = {Achberger, Alexander and Cutura, Ren\\'{e} and T\\\"{u}rksoy, Oguzhan and Sedlmair, Michael},\r\ntitle = {Caarvida: Visual Analytics for Test Drive Videos},\r\nyear = {2020},\r\nisbn = {9781450375351},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3399715.3399862},\r\ndoi = {10.1145/3399715.3399862},\r\nabstract = {We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.},\r\nbooktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},\r\narticleno = {6},\r\nnumpages = {9},\r\nkeywords = {visual analytics, object detection, automotive, information visualization, human computer interaction},\r\nlocation = {Salerno, Italy},\r\nseries = {AVI '20}\r\n}","notes":"","funding":""},{"Title":"Comparing and Exploring High-Dimensional Data with Dimensionality Reduction Algorithms and Matrix Visualizations","Submission Target":"AVI ","Date":"2020-09-28","Type":"Full Paper","First Author":"Rene Cutura","Other Authors":"Michaël Aupetit, Jean-Daniel Fekete, Michael Sedlmair","Key (e.g. for file names)":"cutura2020comparing","Publisher URL (official)":"https://doi.org/10.1145/3399715.3399875","url2":"","PDF URL (public)":"https://renecutura.eu/pdfs/Compadre.pdf","Video":"https://youtu.be/UPkH7rc0ulU","Video2":"","Supplemental":"","Acknowledgements":"This work was supported by the BMVIT ICT of the Future program via the ViSciPub project (no. 867378) and handled by the FFG.","Abstract":"We propose Compadre, a tool for visual analysis for comparing distances of high-dimensional (HD) data and their low-dimensional projections. At the heart is a matrix visualization to represent the discrepancy between distance matrices, linked side-by-side with 2D scatterplot projections of the data. Using different examples and datasets, we illustrate how this approach fosters (1) evaluating dimensionality reduction techniques w.r.t. how well they project the HD data, (2) comparing them to each other side-by-side, and (3) evaluate important data features through subspace comparison. We also present a case study, in which we analyze IEEE VIS authors from 1990 to 2018, and gain new insights on the relationships between coauthors, citations, and keywords. The coauthors are projected as accurately with UMAP as with t-SNE but the projections show different insights. The structure of the citation subspace is very different from the coauthor subspace. The keyword subspace is noisy yet consistent among the three IEEE VIS sub-conferences.","bibtex":"@inproceedings{cutura2020comparing,\r\n  title={Comparing and exploring high-dimensional data with dimensionality reduction algorithms and matrix visualizations},\r\n  author={Cutura, Rene and Aupetit, Micha{\\\"e}l and Fekete, Jean-Daniel and Sedlmair, Michael},\r\n  booktitle={Proc. Intl. Conf. on Advanced Visual Interfaces (AVI)},\r\n  pages={1--9},\r\n  year={2020},\r\n  doi={10.1145/3399715.3399875}}","notes":"","funding":""},{"Title":"ClaVis: An Interactive Visual Comparison System for Classifiers","Submission Target":"AVI","Date":"2020-09-28","Type":"Full Paper","First Author":"Frank Heyen","Other Authors":"Tanja Munz, Michael Neumann, Daniel Ortega, Ngoc Thang Vu, Daniel Weiskopf, Michael Sedlmair","Key (e.g. for file names)":"heyen2020clavis","Publisher URL (official)":"https://doi.org/10.1145/3399715.3399814","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://github.com/fheyen/clavis","Acknowledgements":"Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (A08) and under Germany’s Excellence Strategy – EXC-2075 – 39074001","Abstract":"We propose ClaVis, a visual analytics system for comparative analysis of classification models. ClaVis allows users to visually compare the performance and behavior of tens to hundreds of classifiers trained with different hyperparameter configurations. Our approach is plugin-based and classifier-agnostic and allows users to add their own datasets and classifier implementations. It provides multiple visualizations, including a multivariate ranking, a similarity map, a scatterplot that reveals correlations between parameters and scores, and a training history chart. We demonstrate the effectivity of our approach in multiple case studies for training classification models in the domain of natural language processing.","bibtex":"@inproceedings{10.1145/3399715.3399814,\r\nauthor = {Heyen, Frank and Munz, Tanja and Neumann, Michael and Ortega, Daniel and Vu, Ngoc Thang and Weiskopf, Daniel and Sedlmair, Michael},\r\ntitle = {ClaVis: An Interactive Visual Comparison System for Classifiers},\r\nyear = {2020},\r\nisbn = {9781450375351},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3399715.3399814},\r\ndoi = {10.1145/3399715.3399814},\r\nabstract = {We propose ClaVis, a visual analytics system for comparative analysis of classification\r\nmodels. ClaVis allows users to visually compare the performance and behavior of tens\r\nto hundreds of classifiers trained with different hyperparameter configurations. Our\r\napproach is plugin-based and classifier-agnostic and allows users to add their own\r\ndatasets and classifier implementations. It provides multiple visualizations, including\r\na multivariate ranking, a similarity map, a scatterplot that reveals correlations\r\nbetween parameters and scores, and a training history chart. We demonstrate the effectivity\r\nof our approach in multiple case studies for training classification models in the\r\ndomain of natural language processing.},\r\nbooktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},\r\narticleno = {9},\r\nnumpages = {9},\r\nkeywords = {visual analytics, Visualization, machine learning, classifier comparison},\r\nlocation = {Salerno, Italy},\r\nseries = {AVI '20}\r\n}","notes":"","funding":""},{"Title":"Shortcut Gestures for Mobile Text Editing on Fully Touch Sensitive Smartphones","Submission Target":"TOCHI","Date":"2020-08-17","Type":"Full Paper","First Author":"Huy Viet Le","Other Authors":"Sven Mayer, Maximilian Weiß, Jonas Vogelsang, Henrike Weingärtner, Niels Henze","Key (e.g. for file names)":"le2020shortcut","Publisher URL (official)":"https://doi.org/10.1145/3396233","url2":"https://dl.acm.org/doi/abs/10.1145/3396233","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3396233","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"While advances in mobile text entry enable smartphone users to type almost as fast as on hardware keyboards, text-heavy activities are still not widely adopted. One reason is the lack of shortcut mechanisms. In this article, we determine shortcuts for text-heavy activities, elicit shortcut gestures, implement them for a fully touch-sensitive smartphone, and conduct an evaluation with potential users. We found that experts perform around 800 keyboard shortcuts per day, which are not available on smartphones. Interviews revealed the lack of shortcuts as a major limitation that prevents mobile text editing. Therefore, we elicited gestures for the 22 most important shortcuts for smartphones that are touch-sensitive on the whole device surface. We implemented the gestures for a fully touch-sensitive smartphone using deep learning and evaluated them in realistic scenarios to gather feedback. We show that the developed prototype is perceived as intuitive and faster than recent commercial approaches.","bibtex":"@article{10.1145/3396233,\r\nauthor = {Le, Huy Viet and Mayer, Sven and Wei\\ss{}, Maximilian and Vogelsang, Jonas and Weing\\\"{a}rtner, Henrike and Henze, Niels},\r\ntitle = {Shortcut Gestures for Mobile Text Editing on Fully Touch Sensitive Smartphones},\r\nyear = {2020},\r\nissue_date = {October 2020},\r\npublisher = {ACM},\r\nvolume = {27},K218\r\nnumber = {5},\r\nissn = {1073-0516},\r\nurl = {https://doi.org/10.1145/3396233},\r\ndoi = {10.1145/3396233},\r\nabstract = {While advances in mobile text entry enable smartphone users to type almost as fast as on hardware keyboards, text-heavy activities are still not widely adopted. One reason is the lack of shortcut mechanisms. In this article, we determine shortcuts for text-heavy activities, elicit shortcut gestures, implement them for a fully touch-sensitive smartphone, and conduct an evaluation with potential users. We found that experts perform around 800 keyboard shortcuts per day, which are not available on smartphones. Interviews revealed the lack of shortcuts as a major limitation that prevents mobile text editing. Therefore, we elicited gestures for the 22 most important shortcuts for smartphones that are touch-sensitive on the whole device surface. We implemented the gestures for a fully touch-sensitive smartphone using deep learning and evaluated them in realistic scenarios to gather feedback. We show that the developed prototype is perceived as intuitive and faster than recent commercial approaches.},\r\njournal = {ACM Trans. Comput.-Hum. Interact.},\r\nmonth = {aug},\r\narticleno = {33},\r\nnumpages = {38},\r\nkeywords = {Shortcuts, text editing, gestures, smartphone, keyboard}\r\n}","notes":"","funding":""},{"Title":"Mixed Reality based Collaboration for Design Processes","Submission Target":"i-com","Date":"2020-08-06","Type":"Journal Paper","First Author":"Natalie Hube","Other Authors":"Mathias Müller, Esther Lapczyna, Jan Wojdziak","Key (e.g. for file names)":"hube2020mixed","Publisher URL (official)":"https://doi.org/10.1515/icom-2020-0012","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Due to constantly and rapidly growing digitization, requirements for international cooperation are changing. Tools for collaborative work such as video telephony are already an integral part of today’s communication across companies. However, these tools are not sufficient to represent the full physical presence of an employee or a product as well as its components in another location, since the representation of information in a two-dimensional way and the resulting limited communication loses concrete objectivity. Thus, we present a novel object-centered approach that compromises of Augmented and Virtual Reality technology as well as design suggestions for remote collaboration. Furthermore, we identify current key areas for future research and specify a design space for the use of Augmented and Virtual Reality remote collaboration in the manufacturing process in the automotive industry.","bibtex":"@article{hube2020mixed,\r\n  title={Mixed Reality based Collaboration for Design Processes},\r\n  author={Hube, Natalie and M{\\\"u}ller, Mathias and Lapczyna, Esther and Wojdziak, Jan},\r\n  journal={i-com},\r\n  volume={19},\r\n  number={2},\r\n  pages={123--137},\r\n  year={2020},\r\n  publisher={De Gruyter Oldenbourg}\r\n}","notes":"","funding":""},{"Title":"2019 IEEE Scientific Visualization Contest Winner: Visual Analysis of Structure Formation in Cosmic Evolution","Submission Target":"CG&A","Date":"2020-06-25","Type":"","First Author":"Karsten Schatz","Other Authors":"Christoph Müller, Patrick Gralka, Moritz Heinemann, Alexander Straub, Christoph Schulz, Matthias Braun, Tobias Rau, Michael Becher, Steffen Frey,  Guido Reina, Michael Sedlmair, others","Key (e.g. for file names)":"schatz2020scivis","Publisher URL (official)":"https://doi.org/10.1109/MCG.2020.3004613","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Simulations of cosmic evolution are a means to explain the formation of the universe as we see it today. The resulting data of such simulations comprise numerous physical quantities, which turns their analysis into a complex task. Here, we analyze such high-dimensional and time-varying particle data using various visualization techniques from the fields of particle visualization, flow visualization, volume visualization, and information visualization. Our approach employs specialized filters to extract and highlight the development of so-called active galactic nuclei and filament structures formed by the particles. Additionally, we calculate X-ray emission of the evolving structures in a preprocessing step to complement visual analysis. Our approach is integrated into a single visual analytics framework to allow for analysis of star formation at interactive frame rates. Finally, we lay out the methodological aspects of our work that led to success at the 2019 IEEE SciVis Contest.","bibtex":"@article{schatz2020scivis,\r\n   title={2019 {IEEE} {S}cientific {V}isualization Contest Winner: Visual Analysis of Structure Formation in Cosmic Evolution},\r\n   author={Schatz, Karsten and M{\\\"u}ller, Christoph and Gralka, Patrick and Heinemann, Moritz and Straub, Alexander and Schulz, Christoph and Braun, Matthias and Rau, Tobias and Becher, Michael and Frey, Steffen and Reina, Guido and Sedlmair, Michael and others},\r\n   journal={IEEE Computer Graphics and Applications (CG\\&A)},\r\n   volume={41},\r\n    number={6},\r\n    pages={101--110},\r\ndoi={10.1109/MCG.2020.3004613},\r\nyear = {2021}\r\n}","notes":"","funding":""},{"Title":"Evaluation of Gaze Depth Estimation from Eye Tracking in Augmented Reality","Submission Target":"ETRA","Date":"2020-06-02","Type":"","First Author":"Seyda Öney","Other Authors":"Nils Rodrigues, Michael Becher, Guido Reina, Thomas Ertl, Michael Sedlmair, Daniel Weiskopf","Key (e.g. for file names)":"oeney2020etra","Publisher URL (official)":"https://doi.org/10.1145/3379156.3391835","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Gaze tracking in 3D has the potential to improve interaction with objects and visualizations in augmented reality. However, previous research showed that subjective perception of distance varies between real and virtual surroundings. We wanted to determine whether objectively measured 3D gaze depth through eye tracking also exhibits differences between entirely real and augmented environments. To this end, we conducted an experiment (N = 25) in which we used Microsoft HoloLens with a binocular eye tracking add-on from Pupil Labs. Participants performed a task that required them to look at stationary real and virtual objects while wearing a HoloLens device. We were not able to find significant differences in the gaze depth measured by eye tracking. Finally, we discuss our findings and their implications for gaze interaction in immersive analytics, and the quality of the collected gaze data. ","bibtex":"@inproceedings{10.1145/3379156.3391835,\r\nauthor = {Oney, Seyda and Rodrigues, Nils and Becher, Michael and Ertl, Thomas and Reina, Guido and Sedlmair, Michael and Weiskopf, Daniel},\r\ntitle = {Evaluation of Gaze Depth Estimation from Eye Tracking in Augmented Reality},\r\nyear = {2020},\r\nisbn = {9781450371346},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3379156.3391835},\r\ndoi = {10.1145/3379156.3391835},\r\nabstract = {Gaze tracking in 3D has the potential to improve interaction with objects and visualizations in augmented reality. However, previous research showed that subjective perception of distance varies between real and virtual surroundings. We wanted to determine whether objectively measured 3D gaze depth through eye tracking also exhibits differences between entirely real and augmented environments. To this end, we conducted an experiment (N = 25) in which we used Microsoft HoloLens with a binocular eye tracking add-on from Pupil Labs. Participants performed a task that required them to look at stationary real and virtual objects while wearing a HoloLens device. We were not able to find significant differences in the gaze depth measured by eye tracking. Finally, we discuss our findings and their implications for gaze interaction in immersive analytics, and the quality of the collected gaze data. },\r\nbooktitle = {ACM Symposium on Eye Tracking Research and Applications},\r\narticleno = {49},\r\nnumpages = {5},\r\nkeywords = {eye tracking, visualization, Augmented reality, depth perception, immersive analytics, user study},\r\nlocation = {Stuttgart, Germany},\r\nseries = {ETRA '20 Short Papers}\r\n}","notes":"","funding":""},{"Title":"Eye vs. Head: Comparing Gaze Methods for Interaction in AR","Submission Target":"ETRA","Date":"2020-06-02","Type":"","First Author":"Nelusa Pathmanathan","Other Authors":"Michael Becher, Nils Rodrigues, Guido Reina, Thomas Ertl, Daniel Weiskopf, Michael Sedlmair","Key (e.g. for file names)":"pathmanathan2020etra","Publisher URL (official)":"https://doi.org/10.1145/3379156.3391829","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Visualization in virtual 3D environments can provide a natural way for users to explore data. Often, arm and short head movements are required for interaction in augmented reality, which can be tiring and strenuous though. In an effort toward more user-friendly interaction, we developed a prototype that allows users to manipulate virtual objects using a combination of eye gaze and an external clicker device. Using this prototype, we performed a user study comparing four different input methods of which head gaze plus clicker was preferred by most participants.","bibtex":"@inproceedings{10.1145/3379156.3391829,\r\nauthor = {Pathmanathan, Nelusa and Becher, Michael and Rodrigues, Nils and Reina, Guido and Ertl, Thomas and Weiskopf, Daniel and Sedlmair, Michael},\r\ntitle = {Eye vs.&nbsp;Head: Comparing Gaze Methods for Interaction in Augmented Reality},\r\nyear = {2020},\r\nisbn = {9781450371346},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3379156.3391829},\r\ndoi = {10.1145/3379156.3391829},\r\nabstract = {Visualization in virtual 3D environments can provide a natural way for users to explore data. Often, arm and short head movements are required for interaction in augmented reality, which can be tiring and strenuous though. In an effort toward more user-friendly interaction, we developed a prototype that allows users to manipulate virtual objects using a combination of eye gaze and an external clicker device. Using this prototype, we performed a user study comparing four different input methods of which head gaze plus clicker was preferred by most participants.},\r\nbooktitle = {ACM Symposium on Eye Tracking Research and Applications},\r\narticleno = {50},\r\nnumpages = {5},\r\nkeywords = {Immersive analytics, interaction, visualization, eye tracking, augmented reality},\r\nlocation = {Stuttgart, Germany},\r\nseries = {ETRA '20 Short Papers}\r\n}","notes":"","funding":""},{"Title":"Comparing Input Modalities for Shape Drawing Tasks","Submission Target":"ETVIS","Date":"2020-06-02","Type":"Workshop / Short Paper","First Author":"Annalena Streichert","Other Authors":"Katrin Angerbauer, Magdalena Schwarzl, Michael Sedlmair","Key (e.g. for file names)":"streichert2020etra","Publisher URL (official)":"https://doi.org/10.1145/3379156.3391830","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N= 20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods.","bibtex":"@inproceedings{10.1145/3379156.3391830,\r\nauthor = {Streichert, Annalena and Angerbauer, Katrin and Schwarzl, Magdalena and Sedlmair, Michael},\r\ntitle = {Comparing Input Modalities for Shape Drawing Tasks},\r\nyear = {2020},\r\nisbn = {9781450371346},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3379156.3391830},\r\ndoi = {10.1145/3379156.3391830},\r\nabstract = {With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N=20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods.},\r\nbooktitle = {ACM Symposium on Eye Tracking Research and Applications},\r\narticleno = {51},\r\nnumpages = {5},\r\nkeywords = {Immersive analytics, input modalities, interaction},\r\nlocation = {Stuttgart, Germany},\r\nseries = {ETRA '20 Short Papers}\r\n}","notes":"","funding":""},{"Title":"Toward Agile Situated Visualization: An Exploratory User Study","Submission Target":"CHI","Date":"2020-04-25","Type":"Extended Abstract","First Author":"Leonel Merino","Other Authors":"Boris Sotomayor-Gómez, Xingyao Yu, Ronie Salgado, Alexandre Bergel, Michael Sedlmair, Daniel Weiskopf","Key (e.g. for file names)":"merino2020toward","Publisher URL (official)":"https://doi.org/10.1145/3334480.3383017","url2":"","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3334480.3383017","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median ≥ 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.","bibtex":"@inproceedings{10.1145/3334480.3383017,\r\nauthor = {Merino, Leonel and Sotomayor-G\\'{o}mez, Boris and Yu, Xingyao and Salgado, Ronie and Bergel, Alexandre and Sedlmair, Michael and Weiskopf, Daniel},\r\ntitle = {Toward Agile Situated Visualization: An Exploratory User Study},\r\nyear = {2020},\r\nisbn = {9781450368193},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3334480.3383017},\r\ndoi = {10.1145/3334480.3383017},\r\nabstract = {We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median ≥ 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.},\r\nbooktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},\r\npages = {1–7},\r\nnumpages = {7},\r\nkeywords = {situated visualization, augmented reality, user study},\r\nlocation = {Honolulu, HI, USA},\r\nseries = {CHI EA '20}\r\n}","notes":"","funding":""},{"Title":"Assessing 2D and 3D Heatmaps for Comparative Analysis: An Empirical Study","Submission Target":"CHI","Date":"2020-04-21","Type":"Full Paper","First Author":"Matthias Kraus","Other Authors":"Katrin Angerbauer, Juri Buchmüller, Daniel Schweitzer, Daniel A Keim, Michael Sedlmair, Johannes Fuchs","Key (e.g. for file names)":"kraus2020chi","Publisher URL (official)":"https://doi.org/10.1145/3313831.3376675","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Heatmaps are a popular visualization technique that encode 2D density distributions using color or brightness. Experimental studies have shown though that both of these visual variables are inaccurate when reading and comparing numeric data values. A potential remedy might be to use 3D heatmaps by introducing height as a third dimension to encode the data. Encoding abstract data in 3D, however, poses many problems, too. To better understand this tradeoff, we conducted an empirical study (N=48) to evaluate the user performance of 2D and 3D heatmaps for comparative analysis tasks. We test our conditions on a conventional 2D screen, but also in a virtual reality environment to allow for real stereoscopic vision. Our main results show that 3D heatmaps are superior in terms of error rate when reading and comparing single data items. However, for overview tasks, the well-established 2D heatmap performs better.","bibtex":"@inbook{10.1145/3313831.3376675,\r\nauthor = {Kraus, Matthias and Angerbauer, Katrin and Buchm\\\"{u}ller, Juri and Schweitzer, Daniel and Keim, Daniel A. and Sedlmair, Michael and Fuchs, Johannes},\r\ntitle = {Assessing 2D and 3D Heatmaps for Comparative Analysis: An Empirical Study},\r\nyear = {2020},\r\nisbn = {9781450367080},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3313831.3376675},\r\nabstract = {Heatmaps are a popular visualization technique that encode 2D density distributions using color or brightness. Experimental studies have shown though that both of these visual variables are inaccurate when reading and comparing numeric data values. A potential remedy might be to use 3D heatmaps by introducing height as a third dimension to encode the data. Encoding abstract data in 3D, however, poses many problems, too. To better understand this tradeoff, we conducted an empirical study (N=48) to evaluate the user performance of 2D and 3D heatmaps for comparative analysis tasks. We test our conditions on a conventional 2D screen, but also in a virtual reality environment to allow for real stereoscopic vision. Our main results show that 3D heatmaps are superior in terms of error rate when reading and comparing single data items. However, for overview tasks, the well-established 2D heatmap performs better.},\r\nbooktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\r\npages = {1–14},\r\nnumpages = {14}\r\n}","notes":"","funding":""},{"Title":"A View on the Viewer: Gaze-Adaptive Captions for Videos","Submission Target":"CHI","Date":"2020-04-21","Type":"Full Paper","First Author":"Kuno Kurzhals","Other Authors":"Fabian Göbel, Katrin Angerbauer, Michael Sedlmair, Martin Raubal","Key (e.g. for file names)":"kurzhals2020chi","Publisher URL (official)":"https://doi.org/10.1145/3313831.3376266","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.","bibtex":"@inbook{10.1145/3313831.3376266,\r\nauthor = {Kurzhals, Kuno and G\\\"{o}bel, Fabian and Angerbauer, Katrin and Sedlmair, Michael and Raubal, Martin},\r\ntitle = {A View on the Viewer: Gaze-Adaptive Captions for Videos},\r\nyear = {2020},\r\nisbn = {9781450367080},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3313831.3376266},\r\nabstract = {Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.},\r\nbooktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\r\npages = {1–12},\r\nnumpages = {12}\r\n}","notes":"","funding":""},{"Title":"Visual Analysis of Billiard Dynamics Simulation Ensembles","Submission Target":"VISIGRAPP","Date":"2020-02-27","Type":"Full Paper","First Author":"Stefan Boshe-Plois","Other Authors":"Quynh Quang Ngo, Peter Albers, Lars Linsen","Key (e.g. for file names)":"boshe-plois2020visual","Publisher URL (official)":"https://doi.org/10.5220/0008956201850192","url2":"","PDF URL (public)":"https://www.scitepress.org/Papers/2020/89562/89562.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Mathematical billiards assume a table of a certain shape and dynamical rules for handling collisions. Some trajectories exhibit distinguished patterns. Detecting such trajectories manually for a given billiard is cumbersome, especially, when assuming an ensemble of billiards with different parameter settings. We propose a visual analysis approach for simulation ensembles of billiard dynamics based on phase-space visualizations and multi-dimensional scaling. We apply our methods to the well-studied approach of dynamical billiards for validation and to the novel approach of symplectic billiards for new observations.","bibtex":"@inproceedings{DBLP:conf/grapp/Boshe-PloisNAL20,\r\n  author    = {Stefan Boshe{-}Plois and\r\n               Quynh Quang Ngo and\r\n               Peter Albers and\r\n               Lars Linsen},\r\n  editor    = {Andreas Kerren and\r\n               Christophe Hurter and\r\n               Jos{\\'{e}} Braz},\r\n  title     = {Visual Analysis of Billiard Dynamics Simulation Ensembles},\r\n  booktitle = {Proceedings of the 15th International Joint Conference on Computer\r\n               Vision, Imaging and Computer Graphics Theory and Applications, {VISIGRAPP}\r\n               2020, Volume 3: IVAPP, Valletta, Malta, February 27-29, 2020},\r\n  pages     = {185--192},\r\n  publisher = {{SCITEPRESS}},\r\n  year      = {2020},\r\n  url       = {https://doi.org/10.5220/0008956201850192},\r\n  doi       = {10.5220/0008956201850192},\r\n  timestamp = {Thu, 16 Apr 2020 15:03:30 +0200},\r\n  biburl    = {https://dblp.org/rec/conf/grapp/Boshe-PloisNAL20.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}","notes":"","funding":""},{"Title":"SepEx: Visual Analysis of Class Separation Measures","Submission Target":"EuroVA","Date":"2020-01-01","Type":"","First Author":"Jürgen Bernard","Other Authors":"Marco Hutter, Matthias Zeppelzauer, Michael Sedlmair, Tamara Munzner","Key (e.g. for file names)":"bernard2020eurova","Publisher URL (official)":"https://doi.org/10.2312/eurova.20201079","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Class separation is an important concept in machine learning and visual analytics. However, the comparison of class separation for datasets with varying dimensionality is non-trivial, given a) the various possible structural characteristics of datasets and b) the plethora of separation measures that exist. Building upon recent findings in visualization research about the qualitative and quantitative evaluation of class separation for 2D dimensionally reduced data using scatterplots, this research addresses the visual analysis of class separation measures for high-dimensional data. We present SepEx, an interactive visualization approach for the assessment and comparison of class separation measures for multiple datasets. SepEx supports analysts with the comparison of multiple separation measures over many high-dimensional datasets, the effect of dimensionality reduction on measure outputs by supporting nD to 2D comparison, and the comparison of the effect of different dimensionality reduction methods on measure outputs. We demonstrate SepEx in a scenario on 100 two-class 5D datasets with a linearly increasing amount of separation between the classes, illustrating both similarities and nonlinearities across 11 measures.","bibtex":"@inproceedings {10.2312:eurova.20201079,\r\nbooktitle = {EuroVis Workshop on Visual Analytics (EuroVA)},\r\neditor = {Turkay, Cagatay and Vrotsou, Katerina},\r\ntitle = {{SepEx: Visual Analysis of Class Separation Measures}},\r\nauthor = {Bernard, Jürgen and Hutter, Marco and Zeppelzauer, Matthias and Sedlmair, Michael and Munzner, Tamara},\r\nyear = {2020},\r\npublisher = {The Eurographics Association},\r\nISSN = {2664-4487},\r\nISBN = {978-3-03868-116-8},\r\nDOI = {10.2312/eurova.20201079}\r\n}","notes":"","funding":""},{"Title":"Improving the Robustness of Scagnostics","Submission Target":"TVCG","Date":"2020-01-01","Type":"","First Author":"Yunhai Wang","Other Authors":"Zeyu Wang, Tingting Liu, Michael Correll, Zhanglin Cheng, Oliver Deussen, Michael Sedlmair","Key (e.g. for file names)":"wang2020scag","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2019.2934796","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.","bibtex":"@ARTICLE{8807247,\r\n  author={Wang, Yunhai and Wang, Zeyu and Liu, Tingting and Correll, Michael and Cheng, Zhanglin and Deussen, Oliver and Sedlmair, Michael},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Improving the Robustness of Scagnostics}, \r\n  year={2020},\r\n  volume={26},\r\n  number={1},\r\n  pages={759-769},\r\n  doi={10.1109/TVCG.2019.2934796}}\r\n","notes":"","funding":""},{"Title":"Toward Perception-based Evaluation of Clustering Techniques for Visual Analytics","Submission Target":"VIS","Date":"2019-12-19","Type":"Short Paper","First Author":"Michael Aupetit","Other Authors":"Michael Sedlmair, Mostafa Abbas, Abdelkader Baggag, Halima Bensmail","Key (e.g. for file names)":"aupetit2019vis","Publisher URL (official)":"https://doi.org/10.1109/VISUAL.2019.8933620","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Automatic clustering techniques play a central role in Visual Analytics by helping analysts to discover interesting patterns in high-dimensional data. Evaluating these clustering techniques, however, is difficult due to the lack of universal ground truth. Instead, clustering approaches are usually evaluated based on a subjective visual judgment of low-dimensional scatterplots of different datasets. As clustering is an inherent human-in-the-loop task, we propose a more systematic way of evaluating clustering algorithms based on quantification of human perception of clusters in 2D scatterplots. The core question we are asking is in how far existing clustering techniques align with clusters perceived by humans. To do so, we build on a dataset from a previous study [1], in which 34 human subjects la-beled 1000 synthetic scatterplots in terms of whether they could see one or more than one cluster. Here, we use this dataset to benchmark state-of-the-art clustering techniques in terms of how far they agree with these human judgments. More specifically, we assess 1437 variants of K-means, Gaussian Mixture Models, CLIQUE, DBSCAN, and Agglomerative Clustering techniques on these benchmarks data. We get unexpected results. For instance, CLIQUE and DBSCAN are at best in slight agreement on this basic cluster counting task, while model-agnostic Agglomerative clustering can be up to a substantial agreement with human subjects depending on the variants. We discuss how to extend this perception-based clustering benchmark approach, and how it could lead to the design of perception-based clustering techniques that would better support more trustworthy and explainable models of cluster patterns.","bibtex":"@INPROCEEDINGS{8933620,\r\n  author={Aupetit, Michaël and Sedlmair, Michael and Abbas, Mostafa M. and Baggag, Abdelkader and Bensmail, Halima},\r\n  booktitle={2019 IEEE Visualization Conference (VIS)}, \r\n  title={Toward Perception-Based Evaluation of Clustering Techniques for Visual Analytics}, \r\n  year={2019},\r\n  pages={141-145},\r\n  doi={10.1109/VISUAL.2019.8933620}}\r\n","notes":"","funding":""},{"Title":"Network Visualization","Submission Target":"In Network Science - An Aerial View, Springer","Date":"2019-11-20","Type":"Book Chapter","First Author":"Ulrik Brandes","Other Authors":"Michael Sedlmair","Key (e.g. for file names)":"brandes2019netvis","Publisher URL (official)":"https://doi.org/10.1007/978-3-030-26814-5_2","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Data visualization is the art and science of mapping data to graphical variables. In this context, networks give rise to unique difficulties because of inherent dependencies among their elements. We provide a high-level overview of the main challenges and common techniques to address them. They are illustrated with examples from two application domains, social networks and automotive engineering. The chapter concludes with opportunities for future work in network visualization.","bibtex":"@Inbook{Brandes2019,\r\nauthor=\"Brandes, Ulrik\r\nand Sedlmair, Michael\",\r\neditor=\"Biagini, Francesca\r\nand Kauermann, G{\\\"o}ran\r\nand Meyer-Brandis, Thilo\",\r\ntitle=\"Network Visualization\",\r\nbookTitle=\"Network Science: An Aerial View\",\r\nyear=\"2019\",\r\npublisher=\"Springer International Publishing\",\r\naddress=\"Cham\",\r\npages=\"5--21\",\r\nabstract=\"Data visualization is the art and science of mapping data to graphical variables. In this context, networks give rise to unique difficulties because of inherent dependencies among their elements. We provide a high-level overview of the main challenges and common techniques to address them. They are illustrated with examples from two application domains, social networks and automotive engineering. The chapter concludes with opportunities for future work in network visualization.\",\r\nisbn=\"978-3-030-26814-5\",\r\ndoi=\"10.1007/978-3-030-26814-5_2\",\r\nurl=\"https://doi.org/10.1007/978-3-030-26814-5_2\"\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"The Impact of Work Distribution on in Situ Visualization: A Case Study","Submission Target":"ISAV","Date":"2019-11-18","Type":"Workshop Paper","First Author":"Tobias Rau","Other Authors":"Patrick Gralka, Oliver Fernandes, Guido Reina, Steffen Frey, Thomas Ertl","Key (e.g. for file names)":"rau2019the","Publisher URL (official)":"https://doi.org/10.1145/3364228.3364233","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Large-scale computer simulations generate data at rates that necessitate visual analysis tools to run in situ. The distribution of work on and across nodes of a supercomputer is crucial to utilize compute resources as efficiently as possible. In this paper, we study two work distribution problems in the context of in situ visualization and jointly assess the performance impact of different variants. First, especially for simulations involving heterogeneous loads across their domain, dynamic load balancing can significantly reduce simulation run times. However, the adjustment of the domain partitioning associated with this also has a direct impact on visualization performance. The exact impact of this side effect is largely unclear a priori as generally different criteria are used for balancing simulation and visualization load. Second, on node level, the adequate allocation of threads to simulation or visualization tasks minimizes the performance drain of the simulation while also enabling timely visualization results. In our case study, we jointly study both work distribution aspects with the visualization framework MegaMol coupled in situ on node level to the molecular dynamics simulation ls1 Mardyn on Stampede2 at TACC.","bibtex":"@inproceedings{rau2019insitu,\r\n  author = {Rau, Tobias and Gralka, Patrick and Fernandes, Oliver and Reina, Guido and Frey, Steffen and Ertl, Thomas},\r\n  booktitle = {Proceedings of the Workshop on In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization},\r\n  doi = {10.1145/3364228.3364233},\r\n  isbn = {978-1-4503-7723-2},\r\n  numpages = {6},\r\n  pages = {17--22},\r\n  publisher = {ACM},\r\n  series = {ISAV '19},\r\n  title = {The Impact of Work Distribution on in Situ Visualization: A Case Study},\r\n  url = {http://doi.acm.org/10.1145/3364228.3364233},\r\n  year = 2019\r\n}","notes":"","funding":""},{"Title":"FIESTA: A Free Roaming Collaborative Immersive Analytics System","Submission Target":"ISS","Date":"2019-11-10","Type":"","First Author":"Benjamin Lee","Other Authors":"Maxime Cordeil, Arnaud Prouzeau, Tim Dwyer","Key (e.g. for file names)":"lee2019fiesta","Publisher URL (official)":"https://doi.org/10.1145/3343055.3360746","url2":"https://dl.acm.org/doi/abs/10.1145/3343055.3360746","PDF URL (public)":"","Video":"https://dl.acm.org/doi/abs/10.1145/3343055.3360746#sec-supp","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present FIESTA, a prototype system for collaborative immersive analytics (CIA). In contrast to many existing CIA prototypes, FIESTA allows users to collaboratively work together wherever and however they wish---untethered from mandatory physical display devices. Users can freely move around in a shared room-sized environment, author and generate immersive data visualisations, position them in the space around them, and share and communicate their insights to one another. Certain visualisation tasks are also supported to facilitate this process, such as details on demand and brushing and linking.","bibtex":"@inproceedings{10.1145/3343055.3360746,\r\nauthor = {Lee, Benjamin and Cordeil, Maxime and Prouzeau, Arnaud and Dwyer, Tim},\r\ntitle = {FIESTA: A Free Roaming Collaborative Immersive Analytics System},\r\nyear = {2019},\r\nisbn = {9781450368919},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3343055.3360746},\r\ndoi = {10.1145/3343055.3360746},\r\nabstract = {We present FIESTA, a prototype system for collaborative immersive analytics (CIA). In contrast to many existing CIA prototypes, FIESTA allows users to collaboratively work together wherever and however they wish---untethered from mandatory physical display devices. Users can freely move around in a shared room-sized environment, author and generate immersive data visualisations, position them in the space around them, and share and communicate their insights to one another. Certain visualisation tasks are also supported to facilitate this process, such as details on demand and brushing and linking.},\r\nbooktitle = {Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces},\r\npages = {335--338},\r\nnumpages = {4},\r\nkeywords = {virtual reality, collaborative immersive analytics, immersive analytics, collaboration, data visualisation},\r\nlocation = {Daejeon, Republic of Korea},\r\nseries = {ISS '19}\r\n}","notes":"","funding":""},{"Title":"Visual Analysis of Structure Formation in Cosmic Evolution","Submission Target":"SciVis","Date":"2019-10-20","Type":"","First Author":"Karsten Schatz","Other Authors":"Christoph Müller, Patrick Gralka, Moritz Heinemann, Alexander Straub, Christoph Schulz, Matthias Braun, Tobias Rau, Michael Becher, Patrick Diehl, Dominic Marcello, Juhan Frank, Thomas Müller, Steffen Frey, Guido Reina, Daniel Weiskopf, Thomas Ertl","Key (e.g. for file names)":"schatz2019visual","Publisher URL (official)":"https://doi.org/10.1109/scivis47405.2019.8968855","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The IEEE SciVis 2019 Contest targets the visual analysis of structure formation in the cosmic evolution of the universe from when the universe was five million years old up to now. In our submission, we analyze high-dimensional data to get an overview, then investigate the impact of Active Galactic Nuclei (AGNs) using various visualization techniques, for instance, an adapted filament filtering method for detailed analysis and particle flow in the vicinity of filaments. Based on feedback from domain scientists on these initial visualizations, we also analyzed X-ray emissions and star formation areas. The conversion of star-forming gas to stars and the resulting increasing molecular weight of the particles could be observed.","bibtex":"@inproceedings{schatz2019visual,\r\n  abstract = {The IEEE SciVis 2019 Contest targets the visual analysis of structure formation in the cosmic evolution of the universe from when the universe was five million years old up to now. In our submission, we analyze high-dimensional data to get an overview, then investigate the impact of Active Galactic Nuclei (AGNs) using various visualization techniques, for instance, an adapted filament filtering method for detailed analysis and particle flow in the vicinity of filaments. Based on feedback from domain scientists on these initial visualizations, we also analyzed X-ray emissions and star formation areas. The conversion of star-forming gas to stars and the resulting increasing molecular weight of the particles could be observed.},\r\n  author = {Schatz, Karsten and Müller, Christoph and Gralka, Patrick and Heinemann, Moritz and Straub, Alexander and Schulz, Christoph and Braun, Matthias and Rau, Tobias and Becher, Michael and Diehl, Patrick and Marcello, Dominic and Frank, Juhan and Müller, Thomas and Frey, Steffen and Reina, Guido and Weiskopf, Daniel and Ertl, Thomas},\r\n  booktitle = {2019 IEEE Scientific Visualization Conference (SciVis)},\r\n  doi = {10.1109/scivis47405.2019.8968855},\r\n  pages = {33-41},\r\n  title = {Visual Analysis of Structure Formation in Cosmic Evolution},\r\n  url = {https://dx.doi.org/10.1109/scivis47405.2019.8968855},\r\n  year = 2019\r\n}","notes":"","funding":""},{"Title":"Notification in VR: The Effect of Notification Placement, Task, and Environment","Submission Target":"CHI PLAY","Date":"2019-10-17","Type":"Full Paper","First Author":"Rufat Rzayev","Other Authors":"Sven Mayer, Christian Krauter, Niels Henze","Key (e.g. for file names)":"rzayev2019notification","Publisher URL (official)":"https://doi.org/10.1145/3311350.3347190","url2":"https://dl.acm.org/doi/10.1145/3311350.3347190","PDF URL (public)":"","Video":"https://dl.acm.org/doi/10.1145/3311350.3347190","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Virtual reality (VR) is commonly used for entertainment applications but is also increasingly employed for a large number of use cases such as digital prototyping or training workers. Here, VR is key to present an immersive secondary world. VR enables experiences that are close to reality, regardless of time and place. However, highly immersive VR can result in missing digital information from the real world, such as important notifications. For efficient notification presentation in VR, it is necessary to understand how notifications should be integrated in VR without breaking the immersion. Thus, we conducted a study with 24 participants to investigate notification placement in VR while playing games, learning, and solving problems. We compared placing notifications using a Head-Up Display, On-Body, Floating, and In-Situ in open, semi-open, and closed VR environments. We found significant effects of notification placement and task on how notifications are perceived in VR. Insights from our study inform the design of VR applications that support digital notifications.","bibtex":"@inproceedings{rzayev2019notification,\r\n  title        = {Notification in VR: The Effect of Notification Placement, Task and Environment},\r\n  shorttitle   = {Notification in VR},\r\n  author       = {Rzayev, Rufat and Mayer, Sven and Krauter, Christian and Henze, Niels},\r\n  year         = 2019,\r\n  month        = oct,\r\n  booktitle    = {Proc. Symp. Computer-Human Interaction in Play},\r\n  series       = {CHI Play},\r\n  publisher    = {ACM},\r\n  pages        = {199--211},\r\n  doi          = {10.1145/3311350.3347190},\r\n  isbn         = {978-1-4503-6688-5},\r\n  bibtex_show  = {true},\r\n  abstract     = {Virtual reality (VR) is commonly used for entertainment applications but is also increasingly employed for a large number of use cases such as digital prototyping or training workers. Here, VR is key to present an immersive secondary world. VR enables experiences that are close to reality, regardless of time and place. However, highly immersive VR can result in missing digital information from the real world, such as important notifications. For efficient notification presentation in VR, it is necessary to understand how notifications should be integrated in VR without breaking the immersion. Thus, we conducted a study with 24 participants to investigate notification placement in VR while playing games, learning, and solving problems. We compared placing notifications using a Head-Up Display, On-Body, Floating, and In-Situ in open, semi-open, and closed VR environments. We found significant effects of notification placement and task on how notifications are perceived in VR. Insights from our study inform the design of VR applications that support digital notifications.},\r\n  preview      = {rzayev2019Notification_teaser.png},\r\n  pdf          = {rzayev2019Notification.pdf},\r\n  video        = {http://doi.org/10.1145/3311350.3347190}\r\n}","notes":"","funding":""},{"Title":"Visual Analytics of Simulation Ensembles for Network Dynamics","Submission Target":"VMV","Date":"2019-10-02","Type":"Full Paper","First Author":"Quynh Quang Ngo","Other Authors":"Marc-Thorsten Hütt, Lars Linsen","Key (e.g. for file names)":"ngo2019visual","Publisher URL (official)":"https://doi.org/10.2312/vmv.20191322","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"A central question in the field of Network Science is to analyze the role of a given network topology on the dynamical behavior captured by time-varying simulations executed on the network. These dynamical systems are also influenced by global simulation parameters. We present a visual analytics approach that supports the investigation of the impact of the parameter settings, i.e., how parameter choices change the role of network topology on the simulations' dynamics. To answer this question, we are analyzing ensembles of simulation runs with different parameter settings executed on a given network topology. We relate the nodes' topological structures to their dynamical similarity in a 2D plot based on an interactively defined hierarchy of topological properties and a 1D embedding for the dynamical similarity. We evaluate interactively defined topological groups with respect to matching dynamical behavior, which we visually encode as graphs of the function of the considered simulation parameter. Interactive filtering and coordinated views allow for a detailed analysis of the parameter space with respect to topology-dynamics relations. Our visual analytics approach is applied to scenarios for excitable dynamics on synthetic and real brain connectome networks.","bibtex":"@inproceedings{DBLP:conf/vmv/NgoHL19,\r\n  author    = {Quynh Quang Ngo and\r\n               Marc{-}Thorsten H{\\\"{u}}tt and\r\n               Lars Linsen},\r\n  editor    = {Hans{-}J{\\\"{o}}rg Schulz and\r\n               Matthias Teschner and\r\n               Michael Wimmer},\r\n  title     = {Visual Analytics of Simulation Ensembles for Network Dynamics},\r\n  booktitle = {24th International Symposium on Vision, Modeling, and Visualization,\r\n               {VMV} 2019, Rostock, Germany, September 30 - October 2, 2019},\r\n  pages     = {89--97},\r\n  publisher = {Eurographics Association},\r\n  year      = {2019},\r\n  url       = {https://doi.org/10.2312/vmv.20191322},\r\n  doi       = {10.2312/vmv.20191322},\r\n  timestamp = {Wed, 26 May 2021 11:51:06 +0200},\r\n  biburl    = {https://dblp.org/rec/conf/vmv/NgoHL19.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}","notes":"","funding":""},{"Title":"Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience","Submission Target":"INTERSPEECH","Date":"2019-09-15","Type":"Poster","First Author":"Katrin Angerbauer","Other Authors":"Heike Adel, Ngoc Thang Vu","Key (e.g. for file names)":"angerbauer2019interspeech","Publisher URL (official)":"https://doi.org/10.21437/Interspeech.2019-1750","url2":"","PDF URL (public)":"https://www.isca-speech.org/archive/pdfs/interspeech_2019/angerbauer19_interspeech.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Understanding spoken language can be impeded through factors like noisy environments, hearing impairments or lack of proficiency. Subtitles can help in those cases. However, for fast speech or limited screen size, it might be advantageous to compress the subtitles to their most relevant content. Therefore, we address automatic sentence compression in this paper. We propose a neural network model based on an encoder-decoder approach with the possibility of integrating the desired compression ratio. Using this model, we conduct a user study to investigate the effects of compressed subtitles on user experience. Our results show that compressed subtitles can suffice for comprehension but may pose additional cognitive load.","bibtex":"@inproceedings{angerbauer19_interspeech,\r\n  author={Katrin Angerbauer and Heike Adel and Ngoc Thang Vu},\r\n  title={{Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience}},\r\n  year=2019,\r\n  booktitle={Proc. Interspeech 2019},\r\n  pages={594--598},\r\n  doi={10.21437/Interspeech.2019-1750}\r\n}","notes":"","funding":""},{"Title":"Collecting and Structuring Information in the Information Collage","Submission Target":"arXiv","Date":"2019-09-02","Type":"Technical Report","First Author":"Sebastian Sippl","Other Authors":"Michael Sedlmair, Manuela Waldner","Key (e.g. for file names)":"sippl2019tr","Publisher URL (official)":"https://doi.org/10.48550/arXiv.1909.00608","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Knowledge workers, such as scientists, journalists, or consultants, adaptively seek, gather, and consume information. These processes are often inefficient as existing user interfaces provide limited possibilities to combine information from various sources and different formats into a common knowledge representation. In this paper, we present the concept of an information collage (IC) -- a web browser extension combining manual spatial organization of gathered information fragments and automatic text analysis for interactive content exploration and expressive visual summaries. We used IC for case studies with knowledge workers from different domains and longer-term field studies over a period of one month. We identified three different ways how users collect and structure information and provide design recommendations how to support these observed usage strategies. ","bibtex":"@article{DBLP:journals/corr/abs-1909-00608,\r\n  author    = {Sebastian Sippl and\r\n               Michael Sedlmair and\r\n               Manuela Waldner},\r\n  title     = {Collecting and Structuring Information in the Information Collage},\r\n  journal   = {CoRR},\r\n  volume    = {abs/1909.00608},\r\n  year      = {2019},\r\n  url       = {http://arxiv.org/abs/1909.00608},\r\n  eprinttype = {arXiv},\r\n  eprint    = {1909.00608},\r\n  timestamp = {Mon, 16 Sep 2019 17:27:14 +0200},\r\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-00608.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}","notes":"","funding":""},{"Title":"Speculative Execution for Guided Visual Analytics","Submission Target":"VIS","Date":"2019-08-07","Type":"Workshop Paper","First Author":"Fabian Sperrle","Other Authors":"Jürgen Bernard, Michael Sedlmair, Daniel Keim, Mennatallah El-Assady","Key (e.g. for file names)":"sperrle2018speculative","Publisher URL (official)":"https://doi.org/10.48550/arXiv.1908.02627","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We propose the concept of Speculative Execution for Visual Analytics and discuss its effectiveness for model exploration and optimization. Speculative Execution enables the automatic generation of alternative, competing model configurations that do not alter the current model state unless explicitly confirmed by the user. These alternatives are computed based on either user interactions or model quality measures and can be explored using delta-visualizations. By automatically proposing modeling alternatives, systems employing Speculative Execution can shorten the gap between users and models, reduce the confirmation bias and speed up optimization processes. In this paper, we have assembled five application scenarios showcasing the potential of Speculative Execution, as well as a potential for further research. ","bibtex":"@article{DBLP:journals/corr/abs-1908-02627,\r\n  author    = {Fabian Sperrle and\r\n               J{\\\"{u}}rgen Bernard and\r\n               Michael Sedlmair and\r\n               Daniel A. Keim and\r\n               Mennatallah El{-}Assady},\r\n  title     = {Speculative Execution for Guided Visual Analytics},\r\n  journal   = {CoRR},\r\n  volume    = {abs/1908.02627},\r\n  year      = {2019},\r\n  url       = {http://arxiv.org/abs/1908.02627},\r\n  eprinttype = {arXiv},\r\n  eprint    = {1908.02627},\r\n  timestamp = {Fri, 09 Aug 2019 12:15:56 +0200},\r\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-02627.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}","notes":"","funding":""},{"Title":"netflower: Dynamic Network Visualization for Data Journalists","Submission Target":"CGF","Date":"2019-07-11","Type":"","First Author":"Christina Stoiber","Other Authors":"Alexander Rind, Florian Grassinger, Robert Gutounig, Eva Goldgruber, Michael Sedlmair, Stefan Emrich, Wolfgang Aigner","Key (e.g. for file names)":"stoiber2019eurovis","Publisher URL (official)":"https://doi.org/10.1111/cgf.13721","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Journalists need visual interfaces that cater to the exploratory nature of their investigative activities. In this paper, we report on a four-year design study with data journalists. The main result is netflower, a visual exploration tool that supports journalists in investigating quantitative flows in dynamic network data for story-finding. The visual metaphor is based on Sankey diagrams and has been extended to make it capable of processing large amounts of input data as well as network change over time. We followed a structured, iterative design process including requirement analysis and multiple design and prototyping iterations in close cooperation with journalists. To validate our concept and prototype, a workshop series and two diary studies were conducted with journalists. Our findings indicate that the prototype can be picked up quickly by journalists and valuable insights can be achieved in a few hours. The prototype can be accessed at: http://netflower.fhstp.ac.at/","bibtex":"@article{https://doi.org/10.1111/cgf.13721,\r\nauthor = {Stoiber, C. and Rind, A. and Grassinger, F. and Gutounig, R. and Goldgruber, E. and Sedlmair, M. and Emrich, Š. and Aigner, W.},\r\ntitle = {netflower: Dynamic Network Visualization for Data Journalists},\r\njournal = {Computer Graphics Forum},\r\nvolume = {38},\r\nnumber = {3},\r\npages = {699-711},\r\nkeywords = {CCS Concepts, • Human-centered computing → Visualization design and evaluation methods},\r\ndoi = {https://doi.org/10.1111/cgf.13721},\r\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13721},\r\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13721},\r\nabstract = {Abstract Journalists need visual interfaces that cater to the exploratory nature of their investigative activities. In this paper, we report on a four-year design study with data journalists. The main result is netflower, a visual exploration tool that supports journalists in investigating quantitative flows in dynamic network data for story-finding. The visual metaphor is based on Sankey diagrams and has been extended to make it capable of processing large amounts of input data as well as network change over time. We followed a structured, iterative design process including requirement analysis and multiple design and prototyping iterations in close cooperation with journalists. To validate our concept and prototype, a workshop series and two diary studies were conducted with journalists. Our findings indicate that the prototype can be picked up quickly by journalists and valuable insights can be achieved in a few hours. The prototype can be accessed at: http://netflower.fhstp.ac.at/},\r\nyear = {2019}\r\n}","notes":"","funding":""},{"Title":"ClustMe: A Visual Quality Measure for Ranking Monochrome Scatterplots based on Cluster Patterns","Submission Target":"CGF","Date":"2019-07-10","Type":"","First Author":"Mostafa Abbas","Other Authors":"Michael Aupetit, Michael Sedlmair, Halima Bensmail","Key (e.g. for file names)":"abbas2019eurovis","Publisher URL (official)":"https://doi.org/10.1111/cgf.13684","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We propose ClustMe, a new visual quality measure to rank monochrome scatterplots based on cluster patterns. ClustMe is based on data collected from a human-subjects study, in which 34 participants judged synthetically generated cluster patterns in 1000 scatterplots. We generated these patterns by carefully varying the free parameters of a simple Gaussian Mixture Model with two components, and asked the participants to count the number of clusters they could see (1 or more than 1). Based on the results, we form ClustMe by selecting the model that best predicts these human judgments among 7 different state-of-the-art merging techniques (Demp). To quantitatively evaluate ClustMe, we conducted a second study, in which 31 human subjects ranked 435 pairs of scatterplots of real and synthetic data in terms of cluster patterns complexity. We use this data to compare ClustMe's performance to 4 other state-of-the-art clustering measures, including the well-known Clumpiness scagnostics. We found that of all measures, ClustMe is in strongest agreement with the human rankings.","bibtex":"@article{https://doi.org/10.1111/cgf.13684,\r\nauthor = {Abbas, Mostafa M. and Aupetit, Michaël and Sedlmair, Michael and Bensmail, Halima},\r\ntitle = {ClustMe: A Visual Quality Measure for Ranking Monochrome Scatterplots based on Cluster Patterns},\r\njournal = {Computer Graphics Forum},\r\nvolume = {38},\r\nnumber = {3},\r\npages = {225-236},\r\nkeywords = {CCS Concepts, • Human-centered computing → Visual analytics; Empirical studies in visualization, • Computing methodologies → Cluster analysis; Mixture modeling},\r\ndoi = {https://doi.org/10.1111/cgf.13684},\r\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13684},\r\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13684},\r\nabstract = {Abstract We propose ClustMe, a new visual quality measure to rank monochrome scatterplots based on cluster patterns. ClustMe is based on data collected from a human-subjects study, in which 34 participants judged synthetically generated cluster patterns in 1000 scatterplots. We generated these patterns by carefully varying the free parameters of a simple Gaussian Mixture Model with two components, and asked the participants to count the number of clusters they could see (1 or more than 1). Based on the results, we form ClustMe by selecting the model that best predicts these human judgments among 7 different state-of-the-art merging techniques (Demp). To quantitatively evaluate ClustMe, we conducted a second study, in which 31 human subjects ranked 435 pairs of scatterplots of real and synthetic data in terms of cluster patterns complexity. We use this data to compare ClustMe's performance to 4 other state-of-the-art clustering measures, including the well-known Clumpiness scagnostics. We found that of all measures, ClustMe is in strongest agreement with the human rankings.},\r\nyear = {2019}\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Voronoi-Based Foveated Volume Rendering","Submission Target":"EuroVis","Date":"2019-07-03","Type":"Short Paper","First Author":"Valentin Bruder","Other Authors":"Christoph Schulz, Ruben Bauer, Steffen Frey, Daniel Weiskopf, Thomas Ertl","Key (e.g. for file names)":"bruder2019voronoi-based","Publisher URL (official)":"https://doi.org/10.2312/evs.20191172","url2":"","PDF URL (public)":"https://diglib.eg.org/bitstream/handle/10.2312/evs20191172/067-071.pdf?sequence=1&isAllowed=y","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Foveal vision is located in the center of the field of view with a rich impression of detail and color, whereas peripheral vision occurs on the side with more fuzzy and colorless perception. This visual acuity fall-off can be used to achieve higher frame rates by adapting rendering quality to the human visual system. Volume raycasting has unique characteristics, preventing a direct transfer of many traditional foveated rendering techniques. We present an approach that utilizes the visual acuity fall-off to accelerate volume rendering based on Linde-Buzo-Gray sampling and natural neighbor interpolation. First, we measure gaze using a stationary 1200 Hz eye-tracking system. Then, we adapt our sampling and reconstruction strategy to that gaze. Finally, we apply a temporal smoothing filter to attenuate undersampling artifacts since peripheral vision is particularly sensitive to contrast changes and movement. Our approach substantially improves rendering performance with barely perceptible changes in visual quality. We demonstrate the usefulness of our approach through performance measurements on various data sets.","bibtex":"@inproceedings {10.2312:evs.20191172,\r\nbooktitle = {EuroVis 2019 - Short Papers},\r\neditor = {Johansson, Jimmy and Sadlo, Filip and Marai, G. Elisabeta},\r\ntitle = {{Voronoi-Based Foveated Volume Rendering}},\r\nauthor = {Bruder, Valentin and Schulz, Christoph and Bauer, Ruben and Frey, Steffen and Weiskopf, Daniel and Ertl, Thomas},\r\nyear = {2019},\r\npublisher = {The Eurographics Association},\r\nISBN = {978-3-03868-090-1},\r\nDOI = {10.2312/evs.20191172}\r\n}","notes":"","funding":""},{"Title":"Visual Ensemble Analysis to Study the Influence of Hyper-parameters on Training Deep Neural Networks","Submission Target":"EuroVis","Date":"2019-06-03","Type":"Workshop Paper","First Author":"Sagad Hamid","Other Authors":"Adrian Derstroff, Sören Klemm, Quynh Quang Ngo, Xiaoyi Jiang, Lars Linsen","Key (e.g. for file names)":"hamid2019visual","Publisher URL (official)":"https://doi.org/10.2312/mlvis.20191160","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"A good deep neural network design allows for efficient training and high accuracy. The training step requires a suitable choice of several hyper-parameters. Limited knowledge exists on how the hyper-parameters impact the training process, what is the interplay of multiple hyper-parameters, and what is the interrelation of hyper-parameters and network topology. In this paper, we present a structured analysis towards these goals by investigating an ensemble of training runs.We propose a visual ensemble analysis based on hyper-parameter space visualizations, performance visualizations, and visualizing correlations of topological structures. As a proof of concept, we apply our approach to deep convolutional neural networks.","bibtex":"@inproceedings{DBLP:conf/mlvis-ws/HamidDKNJL19,\r\n  author    = {Sagad Hamid and\r\n               Adrian Derstroff and\r\n               S{\\\"{o}}ren Klemm and\r\n               Quynh Quang Ngo and\r\n               Xiaoyi Jiang and\r\n               Lars Linsen},\r\n  editor    = {Daniel Archambault and\r\n               Ian T. Nabney and\r\n               Jaakko Peltonen},\r\n  title     = {Visual Ensemble Analysis to Study the Influence of Hyper-parameters\r\n               on Training Deep Neural Networks},\r\n  booktitle = {2nd Workshop on Machine Learning Methods in Visualisation for Big\r\n               Data, MLVis@EuroVis 2019, Porto, Portugal, June 3, 2019},\r\n  pages     = {19--23},\r\n  publisher = {Eurographics Association},\r\n  year      = {2019},\r\n  url       = {https://doi.org/10.2312/mlvis.20191160},\r\n  doi       = {10.2312/mlvis.20191160},\r\n  timestamp = {Wed, 10 Feb 2021 15:14:52 +0100},\r\n  biburl    = {https://dblp.org/rec/conf/mlvis-ws/HamidDKNJL19.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}","notes":"","funding":""},{"Title":"Effect of Orientation on Unistroke Touch Gestures","Submission Target":"CHI","Date":"2019-05-02","Type":"Full Paper","First Author":"Sven Mayer","Other Authors":"Valentin Schwind, Huy Viet Le, Dominik Weber, Jonas Vogelsang, Johannes Wolf, Niels Henze","Key (e.g. for file names)":"mayer2019effect","Publisher URL (official)":"https://doi.org/10.1145/3290605.3300928","url2":"https://dl.acm.org/doi/abs/10.1145/3290605.3300928","PDF URL (public)":"https://dl.acm.org/doi/pdf/10.1145/3290605.3300928","Video":"","Video2":"https://www.youtube.com/watch?v=Ev30no1uSUU&embeds_referring_euri=https%3A%2F%2Fdl.acm.org%2F&source_ve_path=OTY3MTQ&feature=emb_imp_woyt","Supplemental":"https://dl.acm.org/doi/abs/10.1145/3290605.3300928","Acknowledgements":"","Abstract":"As touchscreens are the most successful input method of current mobile devices, touch gestures became a widely used input technique. While gestures provide users with advantages to express themselves, they also introduce challenges regarding accuracy and memorability. In this paper, we investigate the effect of a gesture's orientation on how well the gesture can be performed. We conducted a study in which participants performed systematically rotated unistroke gestures. For straight lines as well as for compound lines, we found that users tend to align gestures with the primary axes. We show that the error can be described by a Clausen function with R² = .93. Based on our findings, we suggest design implications and highlight the potential for recognizing flick gestures, visualizing gestures and improving recognition of compound gestures.","bibtex":"@inproceedings{10.1145/3290605.3300928,\r\nauthor = {Mayer, Sven and Schwind, Valentin and Le, Huy Viet and Weber, Dominik and Vogelsang, Jonas and Wolf, Johannes and Henze, Niels},\r\ntitle = {Effect of Orientation on Unistroke Touch Gestures},\r\nyear = {2019},\r\nisbn = {9781450359702},\r\npublisher = {ACM},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3290605.3300928},\r\ndoi = {10.1145/3290605.3300928},\r\nabstract = {As touchscreens are the most successful input method of current mobile devices, touch gestures became a widely used input technique. While gestures provide users with advantages to express themselves, they also introduce challenges regarding accuracy and memorability. In this paper, we investigate the effect of a gesture's orientation on how well the gesture can be performed. We conducted a study in which participants performed systematically rotated unistroke gestures. For straight lines as well as for compound lines, we found that users tend to align gestures with the primary axes. We show that the error can be described by a Clausen function with R² = .93. Based on our findings, we suggest design implications and highlight the potential for recognizing flick gestures, visualizing gestures and improving recognition of compound gestures.},\r\nbooktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},\r\npages = {1–9},\r\nnumpages = {9},\r\nkeywords = {touch unistroke gestures, mobile device, user study, gesture set, orientation, design guidelines, touch input},\r\nlocation = {Glasgow, Scotland Uk},\r\nseries = {CHI '19}\r\n}","notes":"","funding":""},{"Title":"Neighborhood Perception in Bar Charts","Submission Target":"CHI","Date":"2019-05-01","Type":"","First Author":"Mingqian Zhao","Other Authors":"Huamin Qu, Michael Sedlmair","Key (e.g. for file names)":"zhao2019barperception","Publisher URL (official)":"https://doi.org/10.1145/3290605.3300462","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper, we report three user experiments that investigate in how far the perception of a bar in a bar chart changes based on the height of its neighboring bars. We hypothesized that the perception of the very same bar, for instance, might differ when it is surrounded by the top highest vs. the top lowest bars. Our results show that such neighborhood effects exist: a target bar surrounded by high neighbor bars, is perceived to be lower as the same bar surrounded with low neighbors. Yet, the effect size of this neighborhood effect is small compared to other data-inherent effects: the judgment accuracy largely depends on the target bar rank, number of data items, and other data characteristics of the dataset. Based on the findings, we discuss design implications for perceptually optimizing bar charts.","bibtex":"@inproceedings{10.1145/3290605.3300462,\r\nauthor = {Zhao, Mingqian and Qu, Huamin and Sedlmair, Michael},\r\ntitle = {Neighborhood Perception in Bar Charts},\r\nyear = {2019},\r\nisbn = {9781450359702},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3290605.3300462},\r\ndoi = {10.1145/3290605.3300462},\r\nabstract = {In this paper, we report three user experiments that investigate in how far the perception of a bar in a bar chart changes based on the height of its neighboring bars. We hypothesized that the perception of the very same bar, for instance, might differ when it is surrounded by the top highest vs. the top lowest bars. Our results show that such neighborhood effects exist: a target bar surrounded by high neighbor bars, is perceived to be lower as the same bar surrounded with low neighbors. Yet, the effect size of this neighborhood effect is small compared to other data-inherent effects: the judgment accuracy largely depends on the target bar rank, number of data items, and other data characteristics of the dataset. Based on the findings, we discuss design implications for perceptually optimizing bar charts.},\r\nbooktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},\r\npages = {1–12},\r\nnumpages = {12},\r\nkeywords = {visualization, empirical study that tells us about people},\r\nlocation = {Glasgow, Scotland Uk},\r\nseries = {CHI '19}\r\n}","notes":"","funding":""},{"Title":"A Framework for Pervasive Visual Deficiency Simulation","Submission Target":"VR","Date":"2019-03-23","Type":"","First Author":"Christoph Schulz","Other Authors":"Nils Rodrigues, Marco Amann, Daniel Baumgartner, Arman Mielke, Christian Baumann, Michael Sedlmair, Daniel Weiskopf","Key (e.g. for file names)":"schulz2019deficiencySim","Publisher URL (official)":"https://doi.org/10.1109/VR44988.2019.9044164","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present a framework for rapid prototyping of pervasive visual deficiency simulation in the context of graphical interfaces, virtual reality, and augmented reality. Our framework facilitates the emulation of various visual deficiencies for a wide range of applications, which allows users with normal vision to experience combinations of conditions such as myopia, hyperopia, presbyopia, cataract, nyctalopia, protanopia, deuteranopia, tritanopia, and achromatopsia. Our framework provides an infrastructure to encourage researchers to evaluate visualization and other display techniques regarding visual deficiencies, and opens up the field of visual disease simulation to a broader audience. The benefits of our framework are easy integration, configuration, fast prototyping, and portability to new emerging hardware. To demonstrate the applicability of our framework, we showcase a desktop application and an Android application that transform commodity hardware into glasses for visual deficiency simulation. We expect that this work promotes a greater understanding of visual impairments, leads to better product design for the visually impaired, and forms a basis for research to compensate for these impairments as everyday help.","bibtex":"@INPROCEEDINGS{9044164,\r\n  author={Schulz, Christoph and Rodrigues, Nils and Amann, Marco and Baumgartner, Daniel and Mielke, Arman and Baumann, Christian and Sedlmair, Michael and Weiskopf, Daniel},\r\n  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, \r\n  title={A Framework for Pervasive Visual Deficiency Simulation}, \r\n  year={2019},\r\n  pages={1-6},\r\n  doi={10.1109/VR44988.2019.9044164}}\r\n","notes":"","funding":""},{"Title":"MegaMol - a comprehensive prototyping framework for visualizations","Submission Target":"The European Physical Journal (Special Topics)","Date":"2019-03-08","Type":"","First Author":"Patrick Gralka","Other Authors":"Michael Becher, Matthias Braun, Florian Frieß,\r\nChristoph Müller, Tobias Rau, Karsten Schatz, Christoph Schulz,\r\nMichael Krone, Guido Reina, Thomas Ertl","Key (e.g. for file names)":"gralka2019megamol","Publisher URL (official)":"https://doi.org/10.1140/epjst/e2019-800167-5","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present MegaMol, a low-overhead prototyping framework for interactive visualization of large scientific data sets. We give a brief summary of related work for context and then focus on a comprehensive overview of the core architecture of the framework. This is followed by the existing and novel features and techniques in MegaMol that define its current functionality. MegaMol has originally been developed to support the visualization and analysis of particle-based data sets that, for instance, come from molecular dynamics simulations. Meanwhile, the software has evolved beyond that. New algorithms and techniques have been implemented to handle many diverse tasks, including information visualization. Additionally, improvements have been made on the software engineering side to make MegaMol more accessible for domain scientists, like an easy-to-handle scripting interface.","bibtex":"@article{Gralka2019Megamol,\r\n  abstract = {We present MegaMol, a low-overhead prototyping framework for interactive visualization of large scientific data sets. We give a brief summary of related work for context and then focus on a comprehensive overview of the core architecture of the framework. This is followed by the existing and novel features and techniques in MegaMol that define its current functionality. MegaMol has originally been developed to support the visualization and analysis of particle-based data sets that, for instance, come from molecular dynamics simulations. Meanwhile, the software has evolved beyond that. New algorithms and techniques have been implemented to handle many diverse tasks, including information visualization. Additionally, improvements have been made on the software engineering side to make MegaMol more accessible for domain scientists, like an easy-to-handle scripting interface.},\r\n  author = {Gralka, Patrick and Becher, Michael and Braun, Matthias and Frieß, Florian and Müller, Christoph and Rau, Tobias and Schatz, Karsten and Schulz, Christoph and Krone, Michael and Reina, Guido and Ertl, Thomas},\r\n  doi = {10.1140/epjst/e2019-800167-5},\r\n  issn = {1951-6401},\r\n  journal = {The European Physical Journal (Special Topics)},\r\n  number = 14,\r\n  pages = {1817--1829},\r\n  title = {MegaMol -- a comprehensive prototyping framework for visualizations},\r\n  url = {https://doi.org/10.1140/epjst/e2019-800167-5},\r\n  volume = {227: Particle Methods in Natural Science and Engineering},\r\n  year = 2019\r\n}","notes":"","funding":""},{"Title":"Lessons Learned from Users Reading Highlighted Abstracts in a Digital Library","Submission Target":"CHIIR","Date":"2019-03-08","Type":"Short Paper","First Author":"Dagmar Kern","Other Authors":"Daniel Hienert, Katrin Angerbauer, Tilman Dingler, Pia Borlund","Key (e.g. for file names)":"kern2019lessons","Publisher URL (official)":"https://doi.org/10.1145/3295750.3298950","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Finding relevant documents is essential for researchers of all disciplines. We investigated an approach for supporting searchers in their relevance decision in a digital library by automatically highlighting the most important keywords in abstracts. We conducted an eye-tracking study with 25 subjects and observed very different search and reading behavior which lead to diverse results. Some of the participants liked that highlighted abstracts accelerate their relevance decision, while others found that they disturb the reading flow. What many agree on is that the quality of highlighting is crucial for trust and system credibility.","bibtex":"@inproceedings{10.1145/3295750.3298950,\r\nauthor = {Kern, Dagmar and Hienert, Daniel and Angerbauer, Katrin and Dingler, Tilman and Borlund, Pia},\r\ntitle = {Lessons Learned from Users Reading Highlighted Abstracts in a Digital Library},\r\nyear = {2019},\r\nisbn = {9781450360258},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3295750.3298950},\r\ndoi = {10.1145/3295750.3298950},\r\nabstract = {Finding relevant documents is essential for researchers of all disciplines. We investigated an approach for supporting searchers in their relevance decision in a digital library by automatically highlighting the most important keywords in abstracts. We conducted an eye-tracking study with 25 subjects and observed very different search and reading behavior which lead to diverse results. Some of the participants liked that highlighted abstracts accelerate their relevance decision, while others found that they disturb the reading flow. What many agree on is that the quality of highlighting is crucial for trust and system credibility.},\r\nbooktitle = {Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},\r\npages = {271–275},\r\nnumpages = {5},\r\nkeywords = {user study, highlighting, reading behavior, relevance judgment},\r\nlocation = {Glasgow, Scotland UK},\r\nseries = {CHIIR '19}\r\n}","notes":"","funding":""},{"Title":"Visual Analysis of Degree-of-Interest Functions to Support Selection Strategies for Instance Labeling","Submission Target":"EuroVA","Date":"2019-01-01","Type":"","First Author":"Jürgen Bernard","Other Authors":"Marco Hutter, Christian Ritter, Markus Lehmann, Michael Sedlmair, Matthias Zeppelzauer","Key (e.g. for file names)":"bernard2019eurova","Publisher URL (official)":"https://doi.org/10.2312/eurova.20191116","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Manually labeling data sets is a time-consuming and expensive task that can be accelerated by interactive machine learning and visual analytics approaches. At the core of these approaches are strategies for the selection of candidate instances to label. We introduce degree-of-interest (DOI) functions as atomic building blocks to formalize candidate selection strategies. We introduce a taxonomy of DOI functions and an approach for the visual analysis of DOI functions, which provide novel complementary views on labeling strategies and DOIs, support their in-depth analysis and facilitate their interpretation. Our method shall support the generation of novel and better explanation of existing labeling strategies in future.","bibtex":"@inproceedings {10.2312:eurova.20191116,\r\nbooktitle = {EuroVis Workshop on Visual Analytics (EuroVA)},\r\neditor = {Landesberger, Tatiana von and Turkay, Cagatay},\r\ntitle = {{Visual Analysis of Degree-of-Interest Functions to Support Selection Strategies for Instance Labeling}},\r\nauthor = {Bernard, Jürgen and Hutter, Marco and Ritter, Christian and Lehmann, Markus and Sedlmair, Michael and Zeppelzauer, Matthias},\r\nyear = {2019},\r\npublisher = {The Eurographics Association},\r\nISBN = {978-3-03868-087-1},\r\nDOI = {10.2312/eurova.20191116}\r\n}","notes":"","funding":""},{"Title":"LTMA: Layered Topic Matching for the Comparative Exploration, Evaluation, and Refinement of Topic Modeling Results","Submission Target":"BDVA","Date":"2018-11-15","Type":"","First Author":"Mennatallah El-Assady","Other Authors":"Fabian Sperrle, Rita Sevastjanova, Michael Sedlmair, Daniel Keim","Key (e.g. for file names)":"elassady2018ltma","Publisher URL (official)":"https://doi.org/10.1109/BDVA.2018.8534018","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present LTMA, a Layered Topic Matching approach for the unsupervised comparative analysis of topic modeling results. Due to the vast number of available modeling algorithms, an efficient and effective comparison of their results is detrimental to a data- and task-driven selection of a model. LTMA automates this comparative analysis by providing topic matching based on two layers (document-overlap and keyword-similarity), creating a novel topic-match data structure. This data structure builds a basis for model exploration and optimization, thus, allowing for an efficient evaluation of their performance in the context of a given type of text data and task. This is especially important for text types where an annotated gold standard dataset is not readily available and, therefore, quantitative evaluation methods are not applicable. We confirm the usefulness of our technique based on three use cases, namely: (1) the automatic comparative evaluation of topic models, (2) the visual exploration of topic modeling differences, and (3) the optimization of topic modeling results through combining matches.","bibtex":"@INPROCEEDINGS{8534018,\r\n  author={El-Assady, Mennatallah and Sperrle, Fabian and Sevastjanova, Rita and Sedlmair, Michael and Keim, Daniel},\r\n  booktitle={2018 International Symposium on Big Data Visual and Immersive Analytics (BDVA)}, \r\n  title={LTMA: Layered Topic Matching for the Comparative Exploration, Evaluation, and Refinement of Topic Modeling Results}, \r\n  year={2018},\r\n  pages={1-10},\r\n  doi={10.1109/BDVA.2018.8534018}}\r\n","notes":"","funding":""},{"Title":"Effect of Using HMDs for One Hour on Preteens Visual Fatigue","Submission Target":"ISMAR","Date":"2018-10-16","Type":"Poster","First Author":"Xingyao Yu","Other Authors":"Dongdong Weng, Jie Guo, Haiyan Jiang, Yihua Bao","Key (e.g. for file names)":"yu2018effect","Publisher URL (official)":"https://doi.org/10.1109/ISMAR-Adjunct.2018.00042","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We designed a within-subject experiment to compare visual discomfort to preteen users caused by using head-mounted displays (HMD) and tablet computers for an hour. 18 participants younger than 13 years old were recruited to fulfill a series of similar painting tasks under both display conditions. Visual fatigue was measured with visual analog scale before and after experiment and during the break of experiment. The results indicated that HMD had a trend to bring higher visual fatigue than tablet computer during the exposure of 1 hour. Although the symptoms of visual discomfort disappeared after resting, there is need for preteen-specific head-mounted displays.","bibtex":"@INPROCEEDINGS{8699249,\r\n  author={Yu, Xingyao and Weng, Dongdong and Guo, Jie and Jiang, Haiyan and Bao, Yihua},\r\n  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, \r\n  title={Effect of Using HMDs for One Hour on Preteens Visual Fatigue}, \r\n  year={2018},\r\n  pages={93-96},\r\n  doi={10.1109/ISMAR-Adjunct.2018.00042}}\r\n","notes":"","funding":""},{"Title":"Studying Biases in Visualization Research: Framework and Methods","Submission Target":"Cognitive Biases in Visualizations, Springer","Date":"2018-09-28","Type":"Book Chapter","First Author":"André Calero Valdez","Other Authors":"Martina Ziefle, Michael Sedlmair","Key (e.g. for file names)":"calero2018bias","Publisher URL (official)":"https://doi.org/10.1007/978-3-319-95831-6_2","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this chapter, we propose and discuss a lightweight framework to help organize research questions that arise around biases in visualization and visual analysis. We contrast our framework against the cognitive bias codex by Buster Benson. The framework is inspired by Norman’s Human Action Cycle and classifies biases into three levels: perceptual biases, action biases, and social biases. For each of the levels of cognitive processing, we discuss examples of biases from the cognitive science literature and speculate how they might also be important to the area of visualization. In addition, we put forward a methodological discussion on how biases might be studied on all three levels, and which pitfalls and threats to validity exist. We hope that the framework will help spark new ideas and guide researchers that study the important topic of biases in visualization.","bibtex":"@Inbook{CaleroValdez2018,\r\nauthor=\"Calero Valdez, Andr{\\'e}\r\nand Ziefle, Martina\r\nand Sedlmair, Michael\",\r\neditor=\"Ellis, Geoffrey\",\r\ntitle=\"Studying Biases in Visualization Research: Framework and Methods\",\r\nbookTitle=\"Cognitive Biases in Visualizations\",\r\nyear=\"2018\",\r\npublisher=\"Springer International Publishing\",\r\naddress=\"Cham\",\r\npages=\"13--27\",\r\nabstract=\"In this chapter, we propose and discuss a lightweight framework to help organize research questions that arise around biases in visualization and visual analysis. We contrast our framework against the cognitive bias codex by Buster Benson. The framework is inspired by Norman's Human Action Cycle and classifies biases into three levels: perceptual biases, action biases, and social biases. For each of the levels of cognitive processing, we discuss examples of biases from the cognitive science literature and speculate how they might also be important to the area of visualization. In addition, we put forward a methodological discussion on how biases might be studied on all three levels, and which pitfalls and threats to validity exist. We hope that the framework will help spark new ideas and guide researchers that study the important topic of biases in visualization.\",\r\nisbn=\"978-3-319-95831-6\",\r\ndoi=\"10.1007/978-3-319-95831-6_2\",\r\nurl=\"https://doi.org/10.1007/978-3-319-95831-6_2\"\r\n}","notes":"","funding":""},{"Title":"Immersive VisualAudioDesign: Spectral Editing in VR","Submission Target":"Audio Mostly","Date":"2018-09-12","Type":"Full Paper","First Author":"Lars Engeln","Other Authors":"Natalie Hube, Rainer Groh","Key (e.g. for file names)":"engeln2018immersive","Publisher URL (official)":"https://doi.org/10.1145/3243274.3243279","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"VisualAudioDesign (VAD) is an attempt to design audio in a visual way. The frequency-domain visualized as a spectrogram construed as pixel data can be manipulated with image filters. Thereby, an approach is described to get away from direct DSP parameter manipulation to a more comprehensible sound design. Virtual Reality (VR) offers immersive insights into data and embodied interaction in the virtual environment. VAD and VR combined enrich spectral editing with a natural work-flow. Therefore, a design paper prototype for interaction with audio data in an virtual environment was used and examined.","bibtex":"@inproceedings{10.1145/3243274.3243279,\r\nauthor = {Engeln, Lars and Hube, Natalie and Groh, Rainer},\r\ntitle = {Immersive VisualAudioDesign: Spectral Editing in VR},\r\nyear = {2018},\r\nisbn = {9781450366090},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3243274.3243279},\r\ndoi = {10.1145/3243274.3243279},\r\nabstract = {VisualAudioDesign (VAD) is an attempt to design audio in a visual way. The frequency-domain visualized as a spectrogram construed as pixel data can be manipulated with image filters. Thereby, an approach is described to get away from direct DSP parameter manipulation to a more comprehensible sound design. Virtual Reality (VR) offers immersive insights into data and embodied interaction in the virtual environment. VAD and VR combined enrich spectral editing with a natural work-flow. Therefore, a design paper prototype for interaction with audio data in an virtual environment was used and examined.},\r\nbooktitle = {Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion},\r\narticleno = {38},\r\nnumpages = {4},\r\nkeywords = {Immersive Analytics, VisualAudioDesign, Virtual Reality, Paper Prototype, Spectral Editing, Immersive Audio},\r\nlocation = {Wrexham, United Kingdom},\r\nseries = {AM'18}\r\n}","notes":"","funding":""},{"Title":"Structure-aware Fisheye Views for Efficient Large Graph Exploration","Submission Target":"TVCG","Date":"2018-08-21","Type":"","First Author":"Yunhai Wang","Other Authors":"Yanyan Wang, Haifeng Zhang, Yinqi Sun, Chi-Wing Fu, Michael Sedlmair, Baoquan Chen, Oliver Deussen","Key (e.g. for file names)":"wang2019fisheye","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2018.2864911","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Traditional fisheye views for exploring large graphs introduce substantial distortions that often lead to a decreased readability of paths and other interesting structures. To overcome these problems, we propose a framework for structure-aware fisheye views. Using edge orientations as constraints for graph layout optimization allows us not only to reduce spatial and temporal distortions during fisheye zooms, but also to improve the readability of the graph structure. Furthermore, the framework enables us to optimize fisheye lenses towards specific tasks and design a family of new lenses: polyfocal, cluster, and path lenses. A GPU implementation lets us process large graphs with up to 15,000 nodes at interactive rates. A comprehensive evaluation, a user study, and two case studies demonstrate that our structure-aware fisheye views improve layout readability and user performance.","bibtex":"@ARTICLE{8440835,\r\n  author={Wang, Yunhai and Wang, Yanyan and Zhang, Haifeng and Sun, Yinqi and Fu, Chi-Wing and Sedlmair, Michael and Chen, Baoquan and Deussen, Oliver},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Structure-aware Fisheye Views for Efficient Large Graph Exploration}, \r\n  year={2019},\r\n  volume={25},\r\n  number={1},\r\n  pages={566-575},\r\n  doi={10.1109/TVCG.2018.2864911}}\r\n","notes":"","funding":""},{"Title":"Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots","Submission Target":"TVCG","Date":"2018-08-20","Type":"","First Author":"Yunhai Wang","Other Authors":"Xin Chen, Tong Ge, Chen Bao, Michael Sedlmair, Chi-Wing Fu, Oliver Deussen, Baoquan Chen","Key (e.g. for file names)":"wang2019color","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2018.2864912","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Appropriate choice of colors significantly aids viewers in understanding the structures in multiclass scatterplots and becomes more important with a growing number of data points and groups. An appropriate color mapping is also an important parameter for the creation of an aesthetically pleasing scatterplot. Currently, users of visualization software routinely rely on color mappings that have been pre-defined by the software. A default color mapping, however, cannot ensure an optimal perceptual separability between groups, and sometimes may even lead to a misinterpretation of the data. In this paper, we present an effective approach for color assignment based on a set of given colors that is designed to optimize the perception of scatterplots. Our approach takes into account the spatial relationships, density, degree of overlap between point clusters, and also the background color. For this purpose, we use a genetic algorithm that is able to efficiently find good color assignments. We implemented an interactive color assignment system with three extensions of the basic method that incorporates top K suggestions, user-defined color subsets, and classes of interest for the optimization. To demonstrate the effectiveness of our assignment technique, we conducted a numerical study and a controlled user study to compare our approach with default color assignments; our findings were verified by two expert studies. The results show that our approach is able to support users in distinguishing cluster numbers faster and more precisely than default assignment methods.","bibtex":"@ARTICLE{8440853,\r\n  author={Wang, Yunhai and Chen, Xin and Ge, Tong and Bao, Chen and Sedlmair, Michael and Fu, Chi-Wing and Deussen, Oliver and Chen, Baoquan},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots}, \r\n  year={2019},\r\n  volume={25},\r\n  number={1},\r\n  pages={820-829},\r\n  doi={10.1109/TVCG.2018.2864912}}\r\n","notes":"","funding":""},{"Title":"Towards User-Centered Active Learning Algorithms","Submission Target":"CGF","Date":"2018-07-11","Type":"","First Author":"Jürgen Bernard","Other Authors":"Matthias Zeppelzauer, Markus Lehmann, Martin Müller, Michael Sedlmair","Key (e.g. for file names)":"bernard2018eurovis","Publisher URL (official)":"https://doi.org/10.1111/cgf.13406","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The labeling of data sets is a time-consuming task, which is, however, an important prerequisite for machine learning and visual analytics. Visual-interactive labeling (VIAL) provides users an active role in the process of labeling, with the goal to combine the potentials of humans and machines to make labeling more efficient. Recent experiments showed that users apply different strategies when selecting instances for labeling with visual-interactive interfaces. In this paper, we contribute a systematic quantitative analysis of such user strategies. We identify computational building blocks of user strategies, formalize them, and investigate their potentials for different machine learning tasks in systematic experiments. The core insights of our experiments are as follows. First, we identified that particular user strategies can be used to considerably mitigate the bootstrap (cold start) problem in early labeling phases. Second, we observed that they have the potential to outperform existing active learning strategies in later phases. Third, we analyzed the identified core building blocks, which can serve as the basis for novel selection strategies. Overall, we observed that data-based user strategies (clusters, dense areas) work considerably well in early phases, while model-based user strategies (e.g., class separation) perform better during later phases. The insights gained from this work can be applied to develop novel active learning approaches as well as to better guide users in visual interactive labeling.","bibtex":"@article{https://doi.org/10.1111/cgf.13406,\r\nauthor = {Bernard, Jürgen and Zeppelzauer, Matthias and Lehmann, Markus and Müller, Martin and Sedlmair, Michael},\r\ntitle = {Towards User-Centered Active Learning Algorithms},\r\njournal = {Computer Graphics Forum},\r\nvolume = {37},\r\nnumber = {3},\r\npages = {121-132},\r\nkeywords = {Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation},\r\ndoi = {https://doi.org/10.1111/cgf.13406},\r\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13406},\r\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13406},\r\nabstract = {Abstract The labeling of data sets is a time-consuming task, which is, however, an important prerequisite for machine learning and visual analytics. Visual-interactive labeling (VIAL) provides users an active role in the process of labeling, with the goal to combine the potentials of humans and machines to make labeling more efficient. Recent experiments showed that users apply different strategies when selecting instances for labeling with visual-interactive interfaces. In this paper, we contribute a systematic quantitative analysis of such user strategies. We identify computational building blocks of user strategies, formalize them, and investigate their potentials for different machine learning tasks in systematic experiments. The core insights of our experiments are as follows. First, we identified that particular user strategies can be used to considerably mitigate the bootstrap (cold start) problem in early labeling phases. Second, we observed that they have the potential to outperform existing active learning strategies in later phases. Third, we analyzed the identified core building blocks, which can serve as the basis for novel selection strategies. Overall, we observed that data-based user strategies (clusters, dense areas) work considerably well in early phases, while model-based user strategies (e.g., class separation) perform better during later phases. The insights gained from this work can be applied to develop novel active learning approaches as well as to better guide users in visual interactive labeling.},\r\nyear = {2018}\r\n}","notes":"","funding":""},{"Title":"Hypersliceplorer: Interactive Visualization of Shapes in Multiple Dimensions","Submission Target":"CGF","Date":"2018-07-10","Type":"","First Author":"Thomas Torsney-Weir","Other Authors":"Torsten Möller, Michael Sedlmair, Mike Kirby","Key (e.g. for file names)":"torsneyweir2018eurovis","Publisher URL (official)":"https://doi.org/10.1111/cgf.13415","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper we present Hypersliceplorer, an algorithm for generating 2D slices of multi-dimensional shapes defined by a simplical mesh. Often, slices are generated by using a parametric form and then constraining parameters to view the slice. In our case, we developed an algorithm to slice a simplical mesh of any number of dimensions with a two-dimensional slice. In order to get a global appreciation of the multi-dimensional object, we show multiple slices by sampling a number of different slicing points and projecting the slices into a single view per dimension pair. These slices are shown in an interactive viewer which can switch between a global view (all slices) and a local view (single slice). We show how this method can be used to study regular polytopes, differences between spaces of polynomials, and multi-objective optimization surfaces.","bibtex":"@article{https://doi.org/10.1111/cgf.13415,\r\nauthor = {Torsney-Weir, T. and Möller, T. and Sedlmair, M. and Kirby, R. M.},\r\ntitle = {Hypersliceplorer: Interactive visualization of shapes in multiple dimensions},\r\njournal = {Computer Graphics Forum},\r\nvolume = {37},\r\nnumber = {3},\r\npages = {229-240},\r\nkeywords = {Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation},\r\ndoi = {https://doi.org/10.1111/cgf.13415},\r\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13415},\r\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13415},\r\nabstract = {Abstract In this paper we present Hypersliceplorer, an algorithm for generating 2D slices of multi-dimensional shapes defined by a simplical mesh. Often, slices are generated by using a parametric form and then constraining parameters to view the slice. In our case, we developed an algorithm to slice a simplical mesh of any number of dimensions with a two-dimensional slice. In order to get a global appreciation of the multi-dimensional object, we show multiple slices by sampling a number of different slicing points and projecting the slices into a single view per dimension pair. These slices are shown in an interactive viewer which can switch between a global view (all slices) and a local view (single slice). We show how this method can be used to study regular polytopes, differences between spaces of polynomials, and multi-objective optimization surfaces.},\r\nyear = {2018}\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Towards augmented reality in quality assurance processes","Submission Target":"MMSys","Date":"2018-06-12","Type":"Workshop Paper","First Author":"Natalie Hube","Other Authors":"Mathias Müller, Jan Wojdziak, Franziska Hannß, Rainer Groh","Key (e.g. for file names)":"hube2018towards","Publisher URL (official)":"https://doi.org/10.1145/3210438.3210442","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Augmented reality (AR) has gained exceptional importance in supporting task performance. Particularly, in quality assurance (QA) processes in the automotive sector AR offers a diversity of use cases. In this paper we propose an interface design which projects information as a digital canvas on the surface of vehicle components. Based on a requirement analysis, we discuss design aspects and describe our application in applying the quality assurance process of a luxury automaker. The application includes a personal view on spatial information embedded in a guided interaction process as a design solution that can be applied to enhance QA processes.","bibtex":"@inproceedings{10.1145/3210438.3210442,\r\nauthor = {Hube, Natalie and M\\\"{u}ller, Mathias and Wojdziak, Jan and Hann\\ss{}, Franziska and Groh, Rainer},\r\ntitle = {Towards Augmented Reality in Quality Assurance Processes},\r\nyear = {2018},\r\nisbn = {9781450357715},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3210438.3210442},\r\ndoi = {10.1145/3210438.3210442},\r\nabstract = {Augmented reality (AR) has gained exceptional importance in supporting task performance. Particularly, in quality assurance (QA) processes in the automotive sector AR offers a diversity of use cases. In this paper we propose an interface design which projects information as a digital canvas on the surface of vehicle components. Based on a requirement analysis, we discuss design aspects and describe our application in applying the quality assurance process of a luxury automaker. The application includes a personal view on spatial information embedded in a guided interaction process as a design solution that can be applied to enhance QA processes.},\r\nbooktitle = {Proceedings of the 10th International Workshop on Immersive Mixed and Virtual Environment Systems},\r\npages = {16–21},\r\nnumpages = {6},\r\nkeywords = {Hand-held Devices, Augmented Reality, Design Decisions, Concept and Implementation},\r\nlocation = {Amsterdam, Netherlands},\r\nseries = {MMVE '18}\r\n}","notes":"","funding":""},{"Title":"Evaluating Visual Data Analysis Systems: A Discussion Report","Submission Target":"HILDA","Date":"2018-06-10","Type":"Workshop Paper","First Author":"Leilani Battle","Other Authors":"Marco Angelini, Carsten Binnig, Tiziana Catarci, Philipp Eichmann, Jean-Daniel Fekete, Giuseppe Santucci, Michael Sedlmair, Wesley Willett","Key (e.g. for file names)":"ballte2018hilda","Publisher URL (official)":"https://doi.org/10.1145/3209900.3209901","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Visual data analysis is a key tool for helping people to make sense of and interact with massive data sets. However, existing evaluation methods (e.g., database benchmarks, individual user studies) fail to capture the key points that make systems for visual data analysis (or visual data systems) challenging to design. In November 2017, members of both the Database and Visualization communities came together in a Dagstuhl seminar to discuss the grand challenges in the intersection of data analysis and interactive visualization.\r\n\r\nIn this paper, we report on the discussions of the working group on the evaluation of visual data systems, which addressed questions centered around developing better evaluation methods, such as \"How do the different communities evaluate visual data systems?\" and \"What we could learn from each other to develop evaluation techniques that cut across areas?\". In their discussions, the group brainstormed initial steps towards new joint evaluation methods and developed a first concrete initiative --- a trace repository of various real-world workloads and visual data systems --- that enables researchers to derive evaluation setups (e.g., performance benchmarks, user studies) under more realistic assumptions, and enables new evaluation perspectives (e.g., broader meta analysis across analysis contexts, reproducibility and comparability across systems).\r\n","bibtex":"@inproceedings{10.1145/3209900.3209901,\r\nauthor = {Battle, Leilani and Angelini, Marco and Binnig, Carsten and Catarci, Tiziana and Eichmann, Philipp and Fekete, Jean-Daniel and Santucci, Giuseppe and Sedlmair, Michael and Willett, Wesley},\r\ntitle = {Evaluating Visual Data Analysis Systems: A Discussion Report},\r\nyear = {2018},\r\nisbn = {9781450358279},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3209900.3209901},\r\ndoi = {10.1145/3209900.3209901},\r\nabstract = {Visual data analysis is a key tool for helping people to make sense of and interact with massive data sets. However, existing evaluation methods (e.g., database benchmarks, individual user studies) fail to capture the key points that make systems for visual data analysis (or visual data systems) challenging to design. In November 2017, members of both the Database and Visualization communities came together in a Dagstuhl seminar to discuss the grand challenges in the intersection of data analysis and interactive visualization.In this paper, we report on the discussions of the working group on the evaluation of visual data systems, which addressed questions centered around developing better evaluation methods, such as \"How do the different communities evaluate visual data systems?\" and \"What we could learn from each other to develop evaluation techniques that cut across areas?\". In their discussions, the group brainstormed initial steps towards new joint evaluation methods and developed a first concrete initiative --- a trace repository of various real-world workloads and visual data systems --- that enables researchers to derive evaluation setups (e.g., performance benchmarks, user studies) under more realistic assumptions, and enables new evaluation perspectives (e.g., broader meta analysis across analysis contexts, reproducibility and comparability across systems).},\r\nbooktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},\r\narticleno = {4},\r\nnumpages = {6},\r\nlocation = {Houston, TX, USA},\r\nseries = {HILDA'18}\r\n}","notes":"","funding":""},{"Title":"Learning from the Best – Visual Analysis of a Quasi-Optimal Data Labeling Strategy","Submission Target":"EuroVis","Date":"2018-06-04","Type":"Short Paper","First Author":"Jürgen Bernard","Other Authors":"Marco Hutter, Markus Lehmann, Martin Müller, Matthias Zeppelzauer, Michael Sedlmair","Key (e.g. for file names)":"bernard2018eurovis_short","Publisher URL (official)":"https://dl.acm.org/doi/abs/10.5555/3290776.3290797","url2":"","PDF URL (public)":"https://www.vis.uni-stuttgart.de/documentcenter/staff/sedlmaml/papers/bernard2018eurovis_short_suppl.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"An overarching goal of active learning strategies is to reduce the human effort when labeling datasets and training machine learning methods. In this work, we focus on the analysis of a (theoretical) quasi-optimal, ground-truth-based strategy for labeling instances, which we refer to as the upper limit of performance (ULoP). Our long-term goal is to improve existing active learning strategies and to narrow the gap between current strategies and the outstanding performance of ULoP. In an observational study conducted on five datasets, we leverage visualization methods to better understand how and why ULoP selects instances. Results show that the strategy of ULoP is not constant (as in most state-of-the-art active learning strategies) but changes within the labeling process. We identify three phases that are common to most observed labeling processes, partitioning the labeling process into (1) a Discovery Phase, (2) a Consolidation Phase, and (3) a Fine Tuning Phase.","bibtex":"@inproceedings{bernard2018leraning,\r\nauthor = {Bernard, J\\\"{u}rgen and Hutter, Marco and Lehmann, Markus and M\\\"{u}ller, Martin and Zeppelzauer, Matthias and Sedlmair, Michael},\r\ntitle = {Learning from the Best: Visual Analysis of a Quasi-Optimal Data Labeling Strategy},\r\nyear = {2018},\r\npublisher = {Eurographics Association},\r\nabstract = {An overarching goal of active learning strategies is to reduce the human effort when labeling datasets and training machine learning methods. In this work, we focus on the analysis of a (theoretical) quasi-optimal, ground-truth-based strategy for labeling instances, which we refer to as the upper limit of performance (ULoP). Our long-term goal is to improve existing active learning strategies and to narrow the gap between current strategies and the outstanding performance of ULoP. In an observational study conducted on five datasets, we leverage visualization methods to better understand how and why ULoP selects instances. Results show that the strategy of ULoP is not constant (as in most state-of-the-art active learning strategies) but changes within the labeling process. We identify three phases that are common to most observed labeling processes, partitioning the labeling process into (1) a Discovery Phase, (2) a Consolidation Phase, and (3) a Fine Tuning Phase.},\r\nbooktitle = {Proceedings of the Eurographics/IEEE VGTC Conference on Visualization: Short Papers},\r\npages = {95–99},\r\nnumpages = {5},\r\nseries = {EuroVis '18}\r\n}","notes":"","funding":""},{"Title":"Risk Fixers and Sweet Spotters: A Study of the Different Approaches to Using Visual Sensitivity Analysis in an Investment Scenario","Submission Target":"EuroVis","Date":"2018-06-04","Type":"Short Paper","First Author":"Thomas Torsney-Weir","Other Authors":"Shahrzad Afroozeh, Michael Sedlmair, and Torsten Möller","Key (e.g. for file names)":"torsneyweir2018eurovis_short","Publisher URL (official)":"https://dl.acm.org/doi/abs/10.5555/3290776.3290801","url2":"http://eprints.cs.univie.ac.at/5649/","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present an empirical study that illustrates how individual users' decision making preferences and biases influence visualization design choices. Twenty-three participants, in a lab study, were shown two interactive financial portfolio optimization interfaces which allowed them to adjust the return for the portfolio and view how the risk changes. One interface showed the sensitivity of the risk to changes in the return and one did not have this feature. Our study highlights two classes of users. One which preferred the interface with the sensitivity feature and one group that does not prefer the sensitivity feature. We named these two groups the ``risk fixers'' and the ``sweet spotters'' due to the analysis method they used. The ``risk fixers'' selected a level of risk which they were comfortable with while the ``sweet spotters'' tried to find a point right before the risk increased greatly. Our study shows that exposing the sensitivity of investment parameters will impact the investment decision process and increase confidence for these ``sweet spotters.'' We also discuss the implications for design.","bibtex":"@inproceedings{cs5649,\r\n       booktitle = {EuroVis 2018 - Short Papers},\r\n           title = {Risk fixers and sweet spotters: A study of the different approaches to using visual sensitivity analysis in an investment scenario},\r\n          author = {Thomas Torsney-Weir and Shahrzad Afroozeh and Michael Sedlmair and Torsten M{\\\"o}ller},\r\n            year = {2018},\r\n       publisher = {The Eurographics Association},\r\n        abstract = {We present an empirical study that illustrates how individual users' decision\r\nmaking preferences and biases influence visualization design choices.\r\nTwenty-three participants, in a lab study,\r\nwere shown two interactive financial portfolio optimization\r\ninterfaces which allowed them to adjust the return for the portfolio and\r\nview how the risk changes. One interface showed the sensitivity of the\r\nrisk to changes in the return and one did not have this feature. Our\r\nstudy highlights two classes of users. One which preferred the\r\ninterface with the sensitivity feature and one group that does not\r\nprefer the sensitivity feature. We named these two groups the ``risk\r\nfixers'' and the ``sweet spotters'' due to the analysis method they\r\nused. The ``risk fixers'' selected a level of risk which they were\r\ncomfortable with while the ``sweet spotters'' tried to find a point\r\nright before the risk increased greatly. Our study shows that exposing\r\nthe sensitivity of investment parameters will impact the investment\r\ndecision process and increase confidence for these ``sweet spotters.''\r\nWe also discuss the implications for design.},\r\n             url = {http://eprints.cs.univie.ac.at/5649/}\r\n}","notes":"","funding":""},{"Title":"A Virtual Environment Gesture Interaction System for People with Dementia","Submission Target":"DIS","Date":"2018-05-30","Type":"","First Author":"Alexander Bejan","Other Authors":"Markus Wieland, Patrizia Murko, Christophe Kunze","Key (e.g. for file names)":"bejan2018a","Publisher URL (official)":"https://doi.org/10.1145/3197391.3205440","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"As dementia will most likely become an impactful challenge for our future society, it is imperative to maintain the well-being of the diverse group of people with dementia (PwD). Thus, appropriate interventions that effectively trigger identity-stabilizing memories, and at the same time encourage sensorimotor activities, have to be designed and implemented. To that end, we present a novel natural user interface (NUI) system combined with a reminiscence-provoking virtual 3D environment (VE). With it, PwD can delve into memories while interacting with the VE over dementia-fitted gestures. The results of the preliminary evaluations are promising, as they show that most PwD get immersed and cheerfully engage in gesture interactions after a short settling-in period.","bibtex":"@inproceedings{10.1145/3197391.3205440,\r\nauthor = {Bejan, Alexander and Wieland, Markus and Murko, Patrizia and Kunze, Christophe},\r\ntitle = {A Virtual Environment Gesture Interaction System for People with Dementia},\r\nyear = {2018},\r\nisbn = {9781450356312},\r\npublisher = {ACM},\r\nurl = {https://doi.org/10.1145/3197391.3205440},\r\ndoi = {10.1145/3197391.3205440},\r\nabstract = {As dementia will most likely become an impactful challenge for our future society, it is imperative to maintain the well-being of the diverse group of people with dementia (PwD). Thus, appropriate interventions that effectively trigger identity-stabilizing memories, and at the same time encourage sensorimotor activities, have to be designed and implemented. To that end, we present a novel natural user interface (NUI) system combined with a reminiscence-provoking virtual 3D environment (VE). With it, PwD can delve into memories while interacting with the VE over dementia-fitted gestures. The results of the preliminary evaluations are promising, as they show that most PwD get immersed and cheerfully engage in gesture interactions after a short settling-in period.},\r\nbooktitle = {Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems},\r\npages = {225–230},\r\nnumpages = {6},\r\nkeywords = {reminiscence therapy, joyful, dementia, natural user interfaces, virtual environment, fun moments, memory triggering, gesture interaction, virtual reality},\r\nseries = {DIS '18 Companion}\r\n}","notes":"","funding":""},{"Title":"Facilitating exploration on exhibitions with augmented reality","Submission Target":"AVI","Date":"2018-05-29","Type":"Poster Paper","First Author":"Natalie Hube","Other Authors":"Mathias Müller, Rainer Groh","Key (e.g. for file names)":"hube2018facilitating","Publisher URL (official)":"https://doi.org/10.1145/3206505.3206585","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"At exhibitions, visitors are usually in a completely unknown environment. Although visitors generally are informed about the topic before a visit, interests are still difficult to extract from the mass of exhibition stands and offers. In this paper we describe a concept using head-coupled AR together with recommender mechanisms for exhibitions. We present a conceptual development for a first prototype with focus on navigational aspects as well as explicit and implicit recommendations to generate input data for visually displayed recommendations.","bibtex":"@inproceedings{10.1145/3206505.3206585,\r\nauthor = {Hube, Natalie and M\\\"{u}ller, Mathias and Groh, Rainer},\r\ntitle = {Facilitating Exploration on Exhibitions with Augmented Reality},\r\nyear = {2018},\r\nisbn = {9781450356169},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3206505.3206585},\r\ndoi = {10.1145/3206505.3206585},\r\nabstract = {At exhibitions, visitors are usually in a completely unknown environment. Although visitors generally are informed about the topic before a visit, interests are still difficult to extract from the mass of exhibition stands and offers. In this paper we describe a concept using head-coupled AR together with recommender mechanisms for exhibitions. We present a conceptual development for a first prototype with focus on navigational aspects as well as explicit and implicit recommendations to generate input data for visually displayed recommendations.},\r\nbooktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},\r\narticleno = {64},\r\nnumpages = {3},\r\nkeywords = {information visualization, human computer interaction, augmented reality, recommender systems},\r\nlocation = {Castiglione della Pescaia, Grosseto, Italy},\r\nseries = {AVI '18}\r\n}","notes":"","funding":""},{"Title":"The Data in Your Hands: Exploring Novel Interaction Techniques and Data Visualization Approaches for Immersive Data Analytics","Submission Target":"AVI ","Date":"2018-05-29","Type":"Workshop Paper","First Author":"Natalie Hube","Other Authors":"Mathias Müller","Key (e.g. for file names)":"hube2018the","Publisher URL (official)":"","url2":"","PDF URL (public)":"http://ceur-ws.org/Vol-2108/paper2.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper, we describe a concept for visualization and interaction with a large data set in an virtual environment. The core idea uses the traditional flat 2D representation as a base visualization but lets the user transform it into a spatial 3D visualizations on demand. Our visualization and interaction concept targets data analysts to use it for exploration and analysis, utilizing virtual reality to gain insight into complex data sets. The concept is based on the use of Parallel Sets for the representation of categorical data. By extending the conventional 2D Parallel Sets with a third dimension, correlations between path variables and the related number of items belonging to a specific node can be visualized. Furthermore, the concept uses virtual reality controllers in combination with a head-mounted display to control additional views. The purpose of the paper is to describe the core concepts and challenges for this type of spatial visualization and the related interaction design, including the use of gestures for direct manuipulation and a hand-attached menu for complex actions","bibtex":"","notes":"","funding":""},{"Title":"VisCoDeR: A Tool for Visually Comparing Dimensionality Reduction Algorithms","Submission Target":"ESANN","Date":"2018-04-25","Type":"Full Paper","First Author":"Rene Cutura","Other Authors":"Stefan Holzer, Michaël Aupetit, Michael Sedlmair","Key (e.g. for file names)":"cutura2018viscoder","Publisher URL (official)":"","url2":"","PDF URL (public)":"https://www.esann.org/sites/default/files/proceedings/legacy/es2018-74.pdf","Video":"https://youtu.be/gg2pgv0xwmc","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We propose VisCoDeR, a tool that leverages comparative visualization to support learning and analyzing different dimensionality reduction (DR) methods. VisCoDeR fosters two modes. The Discover mode allows qualitatively comparing several DR results by juxtaposing and linking the resulting scatterplots. The Explore mode allows for analyzing hundreds of differently parameterized DR results in a quantitative way. We present use cases that show that our approach helps to understand similarities and differences between DR algorithms.","bibtex":"@inproceedings{cutura2018viscoder,\r\n  title={{VisCoDeR}: A Tool for Visually Comparing Dimensionality Reduction Algorithms},\r\n  author={Cutura, Rene and Holzer, Stefan and Aupetit, Micha{\\\"e}l and Sedlmair, Michael},\r\n  booktitle={Euro. Symp. on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},\r\n  pages={641--646},\r\n  year={2018}\r\n}","notes":"","funding":""},{"Title":"Pac-Many: Movement Behavior when Playing Collaborative and Competitive Games on Large Displays","Submission Target":"CHI","Date":"2018-04-21","Type":"Full Paper","First Author":"Sven Mayer","Other Authors":"Lars Lischke, Jens Emil Grønbæk, Zhanna Sarsenbayeva, Jonas Vogelsang, Paweł W. Woźniak, Niels Henze, Giulio Jacucci","Key (e.g. for file names)":"mayer2018pac-many","Publisher URL (official)":"https://doi.org/10.1145/3173574.3174113","url2":"https://dl.acm.org/doi/abs/10.1145/3173574.3174113","PDF URL (public)":"","Video":"","Video2":"https://www.youtube.com/watch?v=rRGdP6JV0Ug&embeds_referring_euri=https%3A%2F%2Fdl.acm.org%2F&source_ve_path=OTY3MTQ&feature=emb_imp_woyt","Supplemental":"https://dl.acm.org/doi/abs/10.1145/3173574.3174113","Acknowledgements":"","Abstract":"Previous work has shown that large high resolution displays (LHRDs) can enhance collaboration between users. As LHRDs allow free movement in front of the screen, an understanding of movement behavior is required to build successful interfaces for these devices. This paper presents Pac-Many; a multiplayer version of the classical computer game Pac-Man to study group dynamics when using LHRDs. We utilized smartphones as game controllers to enable free movement while playing the game. In a lab study, using a 4m × 1m LHRD, 24 participants (12 pairs) played Pac-Many in collaborative and competitive conditions. The results show that players in the collaborative condition divided screen space evenly. In contrast, competing players stood closer together to avoid benefits for the other player. We discuss how the nature of the task is important when designing and analyzing collaborative interfaces for LHRDs. Our work shows how to account for the spatial aspects of interaction with LHRDs to build immersive experiences.","bibtex":"@inproceedings{10.1145/3173574.3174113,\r\nauthor = {Mayer, Sven and Lischke, Lars and Gr\\o{}nb\\ae{}k, Jens Emil and Sarsenbayeva, Zhanna and Vogelsang, Jonas and Wo\\'{z}niak, Pawe\\l{} W. and Henze, Niels and Jacucci, Giulio},\r\ntitle = {Pac-Many: Movement Behavior When Playing Collaborative and Competitive Games on Large Displays},\r\nyear = {2018},\r\nisbn = {9781450356206},\r\npublisher = {ACM},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3173574.3174113},\r\ndoi = {10.1145/3173574.3174113},\r\nabstract = {Previous work has shown that large high resolution displays (LHRDs) can enhance collaboration between users. As LHRDs allow free movement in front of the screen, an understanding of movement behavior is required to build successful interfaces for these devices. This paper presents Pac-Many; a multiplayer version of the classical computer game Pac-Man to study group dynamics when using LHRDs. We utilized smartphones as game controllers to enable free movement while playing the game. In a lab study, using a 4m \\texttimes{} 1m LHRD, 24 participants (12 pairs) played Pac-Many in collaborative and competitive conditions. The results show that players in the collaborative condition divided screen space evenly. In contrast, competing players stood closer together to avoid benefits for the other player. We discuss how the nature of the task is important when designing and analyzing collaborative interfaces for LHRDs. Our work shows how to account for the spatial aspects of interaction with LHRDs to build immersive experiences.},\r\nbooktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},\r\npages = {1–10},\r\nnumpages = {10},\r\nkeywords = {large high resolution displays, gaming, co-located, multiplayer, large tiled display, collaborative},\r\nlocation = {Montreal QC, Canada},\r\nseries = {CHI '18}\r\n}","notes":"","funding":""},{"Title":"More than Bags of Words: Sentiment Analysis with Word Embeddings","Submission Target":"CMM","Date":"2018-04-10","Type":"","First Author":"Elena Rudkowsky","Other Authors":"Martin Haselmayer, Matthias Wastian, Marcelo Jenny, Stefan Emrich, Michael Sedlmair","Key (e.g. for file names)":"rudkowsky2018cmm","Publisher URL (official)":"https://doi.org/10.1080/19312458.2018.1455817","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Moving beyond the dominant bag-of-words approach to sentiment analysis we introduce an alternative procedure based on distributed word embeddings. The strength of word embeddings is the ability to capture similarities in word meaning. We use word embeddings as part of a supervised machine learning procedure which estimates levels of negativity in parliamentary speeches. The procedure’s accuracy is evaluated with crowdcoded training sentences; its external validity through a study of patterns of negativity in Austrian parliamentary speeches. The results show the potential of the word embeddings approach for sentiment analysis in the social sciences.","bibtex":"@article{doi:10.1080/19312458.2018.1455817,\r\nauthor = {Elena Rudkowsky and Martin Haselmayer and Matthias Wastian and Marcelo Jenny and Štefan Emrich and Michael Sedlmair},\r\ntitle = {More than Bags of Words: Sentiment Analysis with Word Embeddings},\r\njournal = {Communication Methods and Measures},\r\nvolume = {12},\r\nnumber = {2-3},\r\npages = {140-157},\r\nyear  = {2018},\r\npublisher = {Routledge},\r\ndoi = {10.1080/19312458.2018.1455817},\r\nURL = { \r\n        https://doi.org/10.1080/19312458.2018.1455817\r\n},\r\neprint = { \r\n        https://doi.org/10.1080/19312458.2018.1455817\r\n    }\r\n}","notes":"","funding":""},{"Title":"VIAL – A Unified Process for Visual-Interactive Labeling","Submission Target":"EuroVA","Date":"2018-03-19","Type":"","First Author":"Jürgen Bernard","Other Authors":"Matthias Zeppelzauer, Michael Sedlmair, Wolfgang Aigner","Key (e.g. for file names)":"bernard2018vial","Publisher URL (official)":"https://doi.org/10.1007/s00371-018-1500-3","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The assignment of labels to data instances is a fundamental prerequisite for many machine learning tasks. Moreover, labeling is a frequently applied process in visual interactive analysis approaches and visual analytics. However, the strategies for creating labels usually differ between these two fields. This raises the question whether synergies between the different approaches can be attained. In this paper, we study the process of labeling data instances with the user in the loop, from both the machine learning and visual interactive perspective. Based on a review of differences and commonalities, we propose the “visual interactive labeling” (VIAL) process that unifies both approaches. We describe the six major steps of the process and discuss their specific challenges. Additionally, we present two heterogeneous usage scenarios from the novel VIAL perspective, one on metric distance learning and one on object detection in videos. Finally, we discuss general challenges to VIAL and point out necessary work for the realization of future VIAL approaches.","bibtex":"@article{bernard2018vial,\r\n  title={VIAL: a unified process for visual interactive labeling},\r\n  author={Bernard, J{\\\"u}rgen and Zeppelzauer, Matthias and Sedlmair, Michael and Aigner, Wolfgang},\r\n  journal={The Visual Computer},\r\n  volume={34},\r\n  number={9},\r\n  pages={1189--1207},\r\n  year={2018},\r\n  publisher={Springer}\r\n}","notes":"","funding":""},{"Title":"Data Journalism - Guidelines and Best Practices for Getting Started","Submission Target":"Booklet, FFG VALiD (project no. 845598)","Date":"2018-01-01","Type":"Booklet","First Author":"Wolfgang Aigner","Other Authors":"Eva Goldgruber, Florian Grassinger, Robert Gutounig, Alexander Rind, Michael Sedlmair, Christina Stoiber","Key (e.g. for file names)":"aigner2018valid","Publisher URL (official)":"http://www.validproject.at","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://github.com/VALIDproject/ddj-booklet","Acknowledgements":"","Abstract":"","bibtex":"","notes":"","funding":""},{"Title":"Bike Sharing Atlas: Visual Analysis of Bike-Sharing Networks","Submission Target":"IJT","Date":"2018-01-01","Type":"","First Author":"Michael Oppermann","Other Authors":"Torsten Möller, Michael Sedlmair","Key (e.g. for file names)":"oppermann2017bikesharingatlas","Publisher URL (official)":"http://eprints.cs.univie.ac.at/5855/","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper, we introduce an interactive visualization system, bikesharingatlas.org, that supports the explorative data analysis of more than 468 bike-sharing networks worldwide. The system leverages a multi-coordinated view approach and innovative interaction techniques can help, for instance, to expose capacity bottlenecks, commuting patterns, and other network characteristics. Our broader goal is to illustrate how visual analysis can be used for exploring distributed, heterogeneous data from smart cities. Based on our collaboration with different target users, we present usage scenarios that show the potential of our approach to understanding bike-sharing and urban commuting behaviors.","bibtex":"@article{cs5855,\r\n            year = {2018},\r\n           month = {January},\r\n            issn = {2207-6433},\r\n          author = {Michael Oppermann and Torsten M{\\\"o}ller and Michael Sedlmair},\r\n           title = {BikeSharingAtlas: Visual Analysis of Bike-Sharing Networks},\r\n         journal = {International Journal of Transportation},\r\n             url = {http://eprints.cs.univie.ac.at/5855/}\r\n}","notes":"","funding":""},{"Title":"What You See Is What You Can Change: Human-Centered Machine Learning By Interactive Visualization","Submission Target":"Neurocomputing","Date":"2017-12-13","Type":"","First Author":"Dominik Sacha","Other Authors":"Michael Sedlmair, Leishi Zhang, John A Lee, Jaakko Peltonen, Daniel Weiskopf, Stephen C North, Daniel A Keim","Key (e.g. for file names)":"sacha2017b","Publisher URL (official)":"https://doi.org/10.1016/j.neucom.2017.01.105","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Visual analytics (VA) systems help data analysts solve complex problems interactively, by integrating automated data analysis and mining, such as machine learning (ML) based methods, with interactive visualizations. We propose a conceptual framework that models human interactions with ML components in the VA process, and that puts the central relationship between automated algorithms and interactive visualizations into sharp focus. The framework is illustrated with several examples and we further elaborate on the interactive ML process by identifying key scenarios where ML methods are combined with human feedback through interactive visualization. We derive five open research challenges at the intersection of ML and visualization research, whose solution should lead to more effective data analysis.","bibtex":"@article{SACHA2017164,\r\ntitle = {What you see is what you can change: Human-centered machine learning by interactive visualization},\r\njournal = {Neurocomputing},\r\nvolume = {268},\r\npages = {164-175},\r\nyear = {2017},\r\nnote = {Advances in artificial neural networks, machine learning and computational intelligence},\r\nissn = {0925-2312},\r\ndoi = {https://doi.org/10.1016/j.neucom.2017.01.105},\r\nurl = {https://www.sciencedirect.com/science/article/pii/S0925231217307609},\r\nauthor = {Dominik Sacha and Michael Sedlmair and Leishi Zhang and John A. Lee and Jaakko Peltonen and Daniel Weiskopf and Stephen C. North and Daniel A. Keim},\r\nkeywords = {Machine learning, Information visualization, Interaction, Visual analytics},\r\n}","notes":"","funding":""},{"Title":"The Back End is Only One Part of the Picture: Mobile-Aware Application Performance Monitoring and Problem Diagnosis","Submission Target":"VALUETOOLS","Date":"2017-12-05","Type":"Full Paper","First Author":"Katrin Angerbauer","Other Authors":"Dušan Okanović,  André van Hoorn, Christoph Heger","Key (e.g. for file names)":"angerbauer2017valuetools","Publisher URL (official)":"https://doi.org/10.1145/3150928.3150939","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The success of modern businesses relies on the quality of their supporting application systems. Continuous application performance management is mandatory to enable efficient problem detection, diagnosis, and resolution during production. In today's age of ubiquitous computing, large fractions of users access application systems from mobile devices, such as phones and tablets. For detecting, diagnosing, and resolving performance and availability problems, an end-to-end view, i.e., traceability of requests starting on the (mobile) clients' devices, is becoming increasingly important. In this paper, we propose an approach for end-to-end monitoring of applications from the users' mobile devices to the back end, and diagnosing root-causes of detected performance problems. We extend our previous work on diagnosing performance anti-patterns from execution traces by new metrics and rules. The evaluation of this work shows that our approach successfully detects and diagnoses performance anti-patterns in applications with iOS-based mobile clients. While there are threats to validity to our experiment, our research is a promising starting point for future work.","bibtex":"@inproceedings{10.1145/3150928.3150939,\r\nauthor = {Angerbauer, Katrin and Okanovi\\'{c}, Du\\v{s}an and van Hoorn, Andr\\'{e} and Heger, Christoph},\r\ntitle = {The Back End is Only One Part of the Picture: Mobile-Aware Application Performance Monitoring and Problem Diagnosis},\r\nyear = {2017},\r\nisbn = {9781450363464},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/3150928.3150939},\r\ndoi = {10.1145/3150928.3150939},\r\nabstract = {The success of modern businesses relies on the quality of their supporting application systems. Continuous application performance management is mandatory to enable efficient problem detection, diagnosis, and resolution during production. In today's age of ubiquitous computing, large fractions of users access application systems from mobile devices, such as phones and tablets. For detecting, diagnosing, and resolving performance and availability problems, an end-to-end view, i.e., traceability of requests starting on the (mobile) clients' devices, is becoming increasingly important. In this paper, we propose an approach for end-to-end monitoring of applications from the users' mobile devices to the back end, and diagnosing root-causes of detected performance problems. We extend our previous work on diagnosing performance anti-patterns from execution traces by new metrics and rules. The evaluation of this work shows that our approach successfully detects and diagnoses performance anti-patterns in applications with iOS-based mobile clients. While there are threats to validity to our experiment, our research is a promising starting point for future work.},\r\nbooktitle = {Proceedings of the 11th EAI International Conference on Performance Evaluation Methodologies and Tools},\r\npages = {82–89},\r\nnumpages = {8},\r\nkeywords = {performance anti-patterns, application performance monitoring, iOS},\r\nlocation = {Venice, Italy},\r\nseries = {VALUETOOLS 2017}\r\n}","notes":"","funding":""},{"Title":"A Framework for Studying Biases in Visualization Research","Submission Target":"DECISIVe","Date":"2017-10-01","Type":"","First Author":"André Calero Valdez","Other Authors":"Martina Ziefle, Michael Sedlmair","Key (e.g. for file names)":"calero-valdez2017framework","Publisher URL (official)":"http://eprints.cs.univie.ac.at/5258/","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this position paper, we propose and discuss a lightweight framework to help organize research questions that arise around biases in visualization and visual analysis. We contrast our framework against cognitive bias codex by Buster Benson. The framework is inspired by Norman’s Human Action Cycle [23] and classifies biases into three levels: perceptual biases, action biases, and social biases. For each of the levels of cognitive processing, we discuss examples of biases from the cognitive science literature, and speculate how they might also be important to the area of visualization. In addition, we put forward a methodological discussion on how biases might be studied on all three levels, and which pitfalls and threats to validity exist. We hope that the framework will help spark new ideas and discussions on how to proceed studying the important topic of biases in visualization.","bibtex":"@inproceedings{cs5258,\r\n          author = {Andr{\\'e} Caldero Valdez and Martina Ziefle and Michael Sedlmair},\r\n            year = {2017},\r\n       booktitle = {DECISIVe 2017},\r\n           title = {A Framework for Studying Biases in Visualization Research},\r\n             url = {http://eprints.cs.univie.ac.at/5258/}\r\n}\r\n","notes":"","funding":""},{"Title":"Text Priming-Effects of Text Visualizations on Readers Prior to Reading","Submission Target":"INTERACT","Date":"2017-09-25","Type":"Full Paper","First Author":"Tilman Dingler","Other Authors":"Dagmar Kern, Katrin Angerbauer, Albrecht Schmidt","Key (e.g. for file names)":"dingler2017text","Publisher URL (official)":"https://doi.org/10.1007/978-3-319-67687-6_23","url2":"","PDF URL (public)":"https://hal.inria.fr/hal-01717225/document","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Living in our information society poses the challenge of having to deal with a plethora of information. While most content is represented through text, keyword extraction and visualization techniques allow the processing and adjustment of text presentation to the readers’ individual requirements and preferences. In this paper, we investigate four types of text visualizations and their feasibility to give readers an overview before they actually engage with a text: word clouds, highlighting, mind maps, and image collages. In a user study with 50 participants, we assessed the effects of such visualizations on reading comprehension, reading time, and subjective impressions. Results show that (1) mind maps best support readers in getting the gist of a text, (2) they also give better subjective impressions on text content and structure, and (3) highlighting keywords in a text before reading helps to reduce reading time. We discuss a set of guidelines to inform the design of automated systems for creating text visualizations for reader support.","bibtex":"@InProceedings{10.1007/978-3-319-67687-6_23,\r\nauthor=\"Dingler, Tilman\r\nand Kern, Dagmar\r\nand Angerbauer, Katrin\r\nand Schmidt, Albrecht\",\r\neditor=\"Bernhaupt, Regina\r\nand Dalvi, Girish\r\nand Joshi, Anirudha\r\nand K. Balkrishan, Devanuj\r\nand O'Neill, Jacki\r\nand Winckler, Marco\",\r\ntitle=\"Text Priming - Effects of Text Visualizations on Readers Prior to Reading\",\r\nbooktitle=\"Human-Computer Interaction -- INTERACT 2017\",\r\nyear=\"2017\",\r\npublisher=\"Springer International Publishing\",\r\naddress=\"Cham\",\r\npages=\"345--365\",\r\nabstract=\"Living in our information society poses the challenge of having to deal with a plethora of information. While most content is represented through text, keyword extraction and visualization techniques allow the processing and adjustment of text presentation to the readers' individual requirements and preferences. In this paper, we investigate four types of text visualizations and their feasibility to give readers an overview before they actually engage with a text: word clouds, highlighting, mind maps, and image collages. In a user study with 50 participants, we assessed the effects of such visualizations on reading comprehension, reading time, and subjective impressions. Results show that (1) mind maps best support readers in getting the gist of a text, (2) they also give better subjective impressions on text content and structure, and (3) highlighting keywords in a text before reading helps to reduce reading time. We discuss a set of guidelines to inform the design of automated systems for creating text visualizations for reader support.\",\r\nisbn=\"978-3-319-67687-6\"\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Priming and Anchoring Effects in Visualization","Submission Target":"TVCG","Date":"2017-08-30","Type":"","First Author":"André Calero Valdez","Other Authors":"Martina Ziefle, Michael Sedlmair","Key (e.g. for file names)":"calero-valdez2017priming","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2017.2744138","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We investigate priming and anchoring effects on perceptual tasks in visualization. Priming or anchoring effects depict the phenomena that a stimulus might influence subsequent human judgments on a perceptual level, or on a cognitive level by providing a frame of reference. Using visual class separability in scatterplots as an example task, we performed a set of five studies to investigate the potential existence of priming and anchoring effects. Our findings show that - under certain circumstances - such effects indeed exist. In other words, humans judge class separability of the same scatterplot differently depending on the scatterplot(s) they have seen before. These findings inform future work on better understanding and more accurately modeling human perception of visual patterns.","bibtex":"@ARTICLE{8022891,\r\n  author={Valdez, André Calero and Ziefle, Martina and Sedlmair, Michael},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Priming and Anchoring Effects in Visualization}, \r\n  year={2018},\r\n  volume={24},\r\n  number={1},\r\n  pages={584-594},\r\n  doi={10.1109/TVCG.2017.2744138}}\r\n","notes":"","funding":""},{"Title":"Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study","Submission Target":"TVCG","Date":"2017-08-29","Type":"","First Author":"Jürgen Bernard","Other Authors":"Marco Hutter, Matthias Zeppelzauer, Dieter Fellner, Michael Sedlmair","Key (e.g. for file names)":"bernard2017labeling","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2017.2744818","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Labeling data instances is an important task in machine learning and visual analytics. Both fields provide a broad set of labeling strategies, whereby machine learning (and in particular active learning) follows a rather model-centered approach and visual analytics employs rather user-centered approaches (visual-interactive labeling). Both approaches have individual strengths and weaknesses. In this work, we conduct an experiment with three parts to assess and compare the performance of these different labeling strategies. In our study, we (1) identify different visual labeling strategies for user-centered labeling, (2) investigate strengths and weaknesses of labeling strategies for different labeling tasks and task complexities, and (3) shed light on the effect of using different visual encodings to guide the visual-interactive labeling process. We further compare labeling of single versus multiple instances at a time, and quantify the impact on efficiency. We systematically compare the performance of visual interactive labeling with that of active learning. Our main findings are that visual-interactive labeling can outperform active learning, given the condition that dimension reduction separates well the class distributions. Moreover, using dimension reduction in combination with additional visual encodings that expose the internal state of the learning model turns out to improve the performance of visual-interactive labeling.","bibtex":"@ARTICLE{8019851,\r\n  author={Bernard, Jürgen and Hutter, Marco and Zeppelzauer, Matthias and Fellner, Dieter and Sedlmair, Michael},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study}, \r\n  year={2018},\r\n  volume={24},\r\n  number={1},\r\n  pages={298-308},\r\n  doi={10.1109/TVCG.2017.2744818}}\r\n","notes":"","funding":""},{"Title":"EdWordle: Consistency-preserving Word Cloud Editing","Submission Target":"TVCG","Date":"2017-08-29","Type":"","First Author":"Yunhai Wang","Other Authors":"Xiaowei Chu, Chen Bao, Lifeng Zhu, Oliver Deussen, Baoquan Chen, Michael Sedlmair","Key (e.g. for file names)":"wang2017edwordle","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2017.2745859","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present EdWordle, a method for consistently editing word clouds. At its heart, EdWordle allows users to move and edit words while preserving the neighborhoods of other words. To do so, we combine a constrained rigid body simulation with a neighborhood-aware local Wordle algorithm to update the cloud and to create very compact layouts. The consistent and stable behavior of EdWordle enables users to create new forms of word clouds such as storytelling clouds in which the position of words is carefully edited. We compare our approach with state-of-the-art methods and show that we can improve user performance, user satisfaction, as well as the layout itself.","bibtex":"@ARTICLE{8017586,\r\n  author={Wang, Yunhai and Chu, Xiaowei and Bao, Chen and Zhu, Lifeng and Deussen, Oliver and Chen, Baoquan and Sedlmair, Michael},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={EdWordle: Consistency-Preserving Word Cloud Editing}, \r\n  year={2018},\r\n  volume={24},\r\n  number={1},\r\n  pages={647-656},\r\n  doi={10.1109/TVCG.2017.2745859}}\r\n","notes":"","funding":""},{"Title":"Revisiting Stress Majorization as a Unified Framework for Interactive Constrained Graph Visualization","Submission Target":"TVCG","Date":"2017-08-29","Type":"","First Author":"Yunhai Wang","Other Authors":"Yanyan Wang,  Yinqi Sun, Lifeng Zhu, Kecheng Lu, Chi-Wing Fu, Michael Sedlmair, Oliver Deussen, Baoquan Chen","Key (e.g. for file names)":"wang2017graph","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2017.2745919","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present an improved stress majorization method that incorporates various constraints, including directional constraints without the necessity of solving a constraint optimization problem. This is achieved by reformulating the stress function to impose constraints on both the edge vectors and lengths instead of just on the edge lengths (node distances). This is a unified framework for both constrained and unconstrained graph visualizations, where we can model most existing layout constraints, as well as develop new ones such as the star shapes and cluster separation constraints within stress majorization. This improvement also allows us to parallelize computation with an efficient GPU conjugant gradient solver, which yields fast and stable solutions, even for large graphs. As a result, we allow the constraint-based exploration of large graphs with 10K nodes - an approach which previous methods cannot support.","bibtex":"@ARTICLE{8017634,\r\n  author={Wang, Yunhai and Wang, Yanyan and Sun, Yinqi and Zhu, Lifeng and Lu, Kecheng and Fu, Chi-Wing and Sedlmair, Michael and Deussen, Oliver and Chen, Baoquan},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Revisiting Stress Majorization as a Unified Framework for Interactive Constrained Graph Visualization}, \r\n  year={2018},\r\n  volume={24},\r\n  number={1},\r\n  pages={489-499},\r\n  doi={10.1109/TVCG.2017.2745919}}\r\n","notes":"","funding":""},{"Title":"A zoomable product browser for elastic displays","Submission Target":"xCoAX","Date":"2017-07-06","Type":"Full Paper","First Author":"Mathias Müller","Other Authors":"Mandy Keck, Thomas Gründer, Natalie Hube, Rainer Groh","Key (e.g. for file names)":"müller2017a","Publisher URL (official)":"","url2":"","PDF URL (public)":"http://2017.xcoax.org/pdf/xcoax2017-Muller.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper, we present an interaction and visualization concept for elastic displays. The interaction concept was inspired by the search process of a rummage table to explore a large set of product data. The basic approach uses a similarity-based search pattern—based on a small set of items, the user refines the search result by examining similar items and exchanging them with items from the current result. A physically-based approach is used to interact with the data by deforming the surface of the elastic display. The presented visualization concept uses glyphs to directly compare items at a glance. Zoomable UI techniques controlled by the deformation of the elastic surface allow to display different levels of detail for each item.","bibtex":"","notes":"","funding":""},{"Title":"Combining Cluster and Outlier Analysis with Visual Analytics","Submission Target":"EuroVA","Date":"2017-06-12","Type":"","First Author":"Jürgen Bernard","Other Authors":"Eduard Dobermann, Michael Sedlmair, Dieter Fellner","Key (e.g. for file names)":"bernard2017a","Publisher URL (official)":"https://doi.org/10.2312/eurova.20171114","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Cluster and outlier analysis are two important tasks. Due to their nature these tasks seem to be opposed to each other, i.e., data objects either belong to a cluster structure or a sparsely populated outlier region. In this work, we present a visual analytics tool that allows the combined analysis of clusters and outliers. Users can add multiple clustering and outlier analysis algorithms, compare results visually, and combine the algorithms’ results. The usefulness of the combined analysis is demonstrated using the example of labeling unknown data sets. The usage scenario also shows that identified clusters and outliers can share joint areas of the data space.","bibtex":"@inproceedings{10.2312/eurova.20171114,\r\nauthor = {Bernard, J\\\"{u}rgen and Dobermann, Eduard and Sedlmair, Michael and Fellner, D. W.},\r\ntitle = {Combining Cluster and Outlier Analysis with Visual Analytics},\r\nyear = {2017},\r\npublisher = {Eurographics Association},\r\naddress = {Goslar, DEU},\r\nurl = {https://doi.org/10.2312/eurova.20171114},\r\ndoi = {10.2312/eurova.20171114},\r\nabstract = {Cluster and outlier analysis are two important tasks. Due to their nature these tasks seem to be opposed to each other, i.e., data objects either belong to a cluster structure or a sparsely populated outlier region. In this work, we present a visual analytics tool that allows the combined analysis of clusters and outliers. Users can add multiple clustering and outlier analysis algorithms, compare results visually, and combine the algorithms' results. The usefulness of the combined analysis is demonstrated using the example of labeling unknown data sets. The usage scenario also shows that identified clusters and outliers can share joint areas of the data space.},\r\nbooktitle = {Proceedings of the EuroVis Workshop on Visual Analytics},\r\npages = {19–23},\r\nnumpages = {5},\r\nlocation = {Barcelona, Spain},\r\nseries = {EuroVA '17}\r\n}","notes":"","funding":""},{"Title":"A Unified Process for Visual-Interactive Labeling","Submission Target":"EuroVA","Date":"2017-06-12","Type":"","First Author":"Jürgen Bernard","Other Authors":"Matthias Zeppelzauer, Michael Sedlmair, Wolfgang Aigner","Key (e.g. for file names)":"bernard2017b","Publisher URL (official)":"https://doi.org/10.2312/eurova.20171123","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Assigning labels to data instances is a prerequisite for many machine learning tasks. Similarly, labeling is applied in visual-interactive analysis approaches. However, the strategies for creating labels often differ in the two fields. In this paper, we study the process of labeling data instances with the user in the loop, from both the machine learning and visual-interactive perspective. Based on a review of differences and commonalities, we propose the 'Visual-Interactive Labeling' (VIAL) process, conflating the strengths of both. We describe the six major steps of the process and highlight their related challenges.","bibtex":"@inproceedings{10.2312/eurova.20171123,\r\nauthor = {Bernard, J\\\"{u}rgen and Zeppelzauer, Matthias and Sedlmair, Michael and Aigner, Wolfgang},\r\ntitle = {A Unified Process for Visual-Interactive Labeling},\r\nyear = {2017},\r\npublisher = {Eurographics Association},\r\naddress = {Goslar, DEU},\r\nurl = {https://doi.org/10.2312/eurova.20171123},\r\ndoi = {10.2312/eurova.20171123},\r\nabstract = {Assigning labels to data instances is a prerequisite for many machine learning tasks. Similarly, labeling is applied in visual-interactive analysis approaches. However, the strategies for creating labels often differ in the two fields. In this paper, we study the process of labeling data instances with the user in the loop, from both the machine learning and visual-interactive perspective. Based on a review of differences and commonalities, we propose the 'Visual-Interactive Labeling' (VIAL) process, conflating the strengths of both. We describe the six major steps of the process and highlight their related challenges.},\r\nbooktitle = {Proceedings of the EuroVis Workshop on Visual Analytics},\r\npages = {73–77},\r\nnumpages = {5},\r\nlocation = {Barcelona, Spain},\r\nseries = {EuroVA '17}\r\n}","notes":"","funding":""},{"Title":"Additional On-Demand Dimension for Data Visualization","Submission Target":"EuroVis","Date":"2017-06-12","Type":"Short Paper","First Author":"Natalie Hube","Other Authors":"Mathias Müller, Rainer Groh","Key (e.g. for file names)":"hube2017additional","Publisher URL (official)":"https://doi.org/10.2312/eurovisshort.20171151","url2":"","PDF URL (public)":"https://diglib.eg.org/bitstream/handle/10.2312/eurovisshort20171151/163-167.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper, we present a concept to interactively extend an 2d visualization by an additional on-demand dimension. We use categorical data in a multidimensional information space applied in a travel search scenario. Parallel sets are used as the basis for the visualization concept, since this is particularly suitable for the visualization of categorical data. The on-demand dimension expands the vertical axis of a parallel coordinate graph into depth axis and is intended to increase comparability of path variables with respect to the number of elements belonging to the respective parameter axis instead of direct comparability of individual paths and keep relations between the parallel sets. The presented implementation suits as foundation for further studies about the usefulness of a dynamic, on demand extension a of 2d visualizations into spatial visualizations. Furthermore, we present some additional approaches about the usage of the increased visualization space.","bibtex":"@inproceedings {10.2312:eurovisshort.20171151,\r\nbooktitle = {EuroVis 2017 - Short Papers},\r\neditor = {Barbora Kozlikova and Tobias Schreck and Thomas Wischgoll},\r\ntitle = {{Additional On-Demand Dimension for Data Visualization}},\r\nauthor = {Hube, Natalie and Müller, Mathias and Groh, Rainer},\r\nyear = {2017},\r\npublisher = {The Eurographics Association},\r\nISBN = {978-3-03868-043-7},\r\nDOI = {10.2312/eurovisshort.20171151}\r\n}","notes":"","funding":""},{"Title":"Visual Analytics of Global Parameters in Simulation Ensembles of ODE-based Excitable Network Dynamics","Submission Target":"EuroVis","Date":"2017-06-12","Type":"Poster","First Author":"Quynh Quang Ngo","Other Authors":"Marc-Thorsten Hütt, Lars Linsen","Key (e.g. for file names)":"ngo2017visual","Publisher URL (official)":"https://doi.org/10.2312/eurp.20171179","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The role of network topology on the dynamics in simulations that are executed on the network is a central question in the field of network science. However, the influence of the topology is affected by the global dynamical simulation parameters. To investigate this impact of the parameter settings, multiple simulation runs are executed with different settings. Moreover, since the outcome of a single simulation run also depends on the randomly chosen start configurations, multiple runs with the same settings are carried out, as well. We present a visual approach to analyze the role of topology in such an ensemble of simulation ensembles. We use the dynamics of an excitable network implemented in the form of a coupled ordinary differential equation (ODE) following the FitzHugh-Nagumo (FHN) model and modular network topologies.","bibtex":"@inproceedings{DBLP:conf/vissym/NgoHL17,\r\n  author    = {Quynh Quang Ngo and\r\n               Marc{-}Thorsten H{\\\"{u}}tt and\r\n               Lars Linsen},\r\n  editor    = {Anna Puig and\r\n               Tobias Isenberg},\r\n  title     = {Visual Analytics of Global Parameters in Simulation Ensembles of ODE-based\r\n               Excitable Network Dynamics},\r\n  booktitle = {19th Eurographics Conference on Visualization, EuroVis 2017 - Posters,\r\n               Barcelona, Spain, June 12-16, 2017},\r\n  pages     = {101--103},\r\n  publisher = {Eurographics Association},\r\n  year      = {2017},\r\n  url       = {https://doi.org/10.2312/eurp.20171179},\r\n  doi       = {10.2312/eurp.20171179},\r\n  timestamp = {Wed, 01 Jul 2020 13:35:20 +0200},\r\n  biburl    = {https://dblp.org/rec/conf/vissym/NgoHL17.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}","notes":"","funding":""},{"Title":"Sliceplorer: 1D Slices for Multi-dimensional Continuous Functions","Submission Target":"CGF","Date":"2017-06-01","Type":"","First Author":"Thomas Torsney-Weir","Other Authors":"Michael Sedlmair, Torsten Möller","Key (e.g. for file names)":"torsneyweir2017sliceplorer","Publisher URL (official)":"https://doi.org/10.1111/cgf.13177","url2":"","PDF URL (public)":"","Video":"https://www.youtube.com/watch?v=VSHadt-jB-s","Video2":"","Supplemental":"http://sliceplorer.cs.univie.ac.at/evaluation/index.html","Acknowledgements":"","Abstract":"Multi-dimensional continuous functions are commonly visualized with 2D slices or topological views. Here, we explore 1D slices as an alternative approach to show such functions. Our goal with 1D slices is to combine the benefits of topological views, that is, screen space efficiency, with those of slices, that is a close resemblance of the underlying function. We compare 1D slices to 2D slices and topological views, first, by looking at their performance with respect to common function analysis tasks. We also demonstrate 3 usage scenarios: the 2D sinc function, neural network regression, and optimization traces. Based on this evaluation, we characterize the advantages and drawbacks of each of these approaches, and show how interaction can be used to overcome some of the shortcomings.","bibtex":"@article{10.1111/cgf.13177,\r\nauthor = {Torsney-Weir, T. and Sedlmair, M. and M\\\"{o}ller, T.},\r\ntitle = {Sliceplorer: 1D Slices for Multi-Dimensional Continuous Functions},\r\nyear = {2017},\r\nissue_date = {June 2017},\r\npublisher = {The Eurographs Association &amp; John Wiley &amp; Sons, Ltd.},\r\naddress = {Chichester, GBR},\r\nvolume = {36},\r\nnumber = {3},\r\nissn = {0167-7055},\r\nurl = {https://doi.org/10.1111/cgf.13177},\r\ndoi = {10.1111/cgf.13177},\r\nabstract = {Multi-dimensional continuous functions are commonly visualized with 2D slices or topological views. Here, we explore 1D slices as an alternative approach to show such functions. Our goal with 1D slices is to combine the benefits of topological views, that is, screen space efficiency, with those of slices, that is a close resemblance of the underlying function. We compare 1D slices to 2D slices and topological views, first, by looking at their performance with respect to common function analysis tasks. We also demonstrate 3 usage scenarios: the 2D sinc function, neural network regression, and optimization traces. Based on this evaluation, we characterize the advantages and drawbacks of each of these approaches, and show how interaction can be used to overcome some of the shortcomings.},\r\njournal = {Comput. Graph. Forum},\r\nmonth = {jun},\r\npages = {167–177},\r\nnumpages = {11},\r\nkeywords = {Categories and Subject Descriptors according to ACM CCS, I.3.3 [Computer Graphics]: Picture/Image Generation-Line and curve generation}\r\n}","notes":"","funding":""},{"Title":"Supervised Sentiment Analysis of Parliamentary Speeches and News Reports","Submission Target":"ICA","Date":"2017-05-25","Type":"","First Author":"Elena Rudkowsky","Other Authors":"Martin Haselmayer, Matthias Wastian, Marcelo Jenny, Stefan Emrich, Michael Sedlmair","Key (e.g. for file names)":"rudkowsy2017sentiment","Publisher URL (official)":"","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper, we use several supervised machine learning approaches and compare their success in predicting the sentiment of Austrian parliamentary speeches and news reports (German language). Prediction results in learning- based sentiment analysis vary strongly. They depend on the choice of algorithm and its parameterization, the quality and quantity of available training data as well as the selection of appropriate input feature representations. Our training data contains human-annotated sentiment scores at the phrase and sentence level. Going beyond the dominant bag-of-words modeling approach in traditional natural language processing, we also test sentiment analysis for neural network-based distributed representations of words. The latter reflect syntactic as well as semantic relatedness, but require huge amounts of training examples. We test both approaches with heterogeneous textual data, compare their success rates and provide conclusions on how to improve the sentiment analysis of political communication.\r\n","bibtex":"@article{rudkowskysupervised,\r\n  title={Supervised Sentiment Analysis of Parliamentary Speeches and News Reports},\r\n  author={Rudkowsky, Elena and Haselmayer, Martin and Wastian, Matthias and Jenny, Marcelo and Emrich, {\\v{S}}tefan and Sedlmair, Michael}\r\n}\r\n","notes":"","funding":""},{"Title":"A Perception-Driven Approach to Supervised Dimensionality Reduction for Visualization","Submission Target":"TVCG","Date":"2017-05-05","Type":"","First Author":"Yunhai Wang","Other Authors":"Kang Feng, Xiaowei Chu, Jian Zhang, Chi-Wing Fu, Michael Sedlmair, Xiaohui Yu, Baoquan Chen","Key (e.g. for file names)":"wang2017tvcg","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2017.2701829","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Dimensionality reduction (DR) is a common strategy for visual analysis of labeled high-dimensional data. Low-dimensional representations of the data help, for instance, to explore the class separability and the spatial distribution of the data. Widely-used unsupervised DR methods like PCA do not aim to maximize the class separation, while supervised DR methods like LDA often assume certain spatial distributions and do not take perceptual capabilities of humans into account. These issues make them ineffective for complicated class structures. Towards filling this gap, we present a perception-driven linear dimensionality reduction approach that maximizes the perceived class separation in projections. Our approach builds on recent developments in perception-based separation measures that have achieved good results in imitating human perception. We extend these measures to be density-aware and incorporate them into a customized simulated annealing algorithm, which can rapidly generate a near optimal DR projection. We demonstrate the effectiveness of our approach by comparing it to state-of-the-art DR methods on 93 datasets, using both quantitative measure and human judgments. We also provide case studies with class-imbalanced and unlabeled data.","bibtex":"@ARTICLE{7920403,\r\n  author={Wang, Yunhai and Feng, Kang and Chu, Xiaowei and Zhang, Jian and Fu, Chi-Wing and Sedlmair, Michael and Yu, Xiaohui and Chen, Baoquan},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={A Perception-Driven Approach to Supervised Dimensionality Reduction for Visualization}, \r\n  year={2018},\r\n  volume={24},\r\n  number={5},\r\n  pages={1828-1840},\r\n  doi={10.1109/TVCG.2017.2701829}}\r\n","notes":"","funding":""},{"Title":"Incivility in Austrian parliamentary debates: A supervised sentiment analysis of parliamentary speeches","Submission Target":"ECPR","Date":"2017-04-25","Type":"","First Author":"Marcelo Jenny","Other Authors":"Martin Haselmayer, Elena Rudkowsky, Matthias Wastian, Stefan Emrich, Michael Sedlmair","Key (e.g. for file names)":"jenny2017incivility","Publisher URL (official)":"","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Incivility of political communication has become a major topic in public and scientific discourse (e.g. Herbst 2010; Berry and Sobieraj 2013), and it is often seen as a cause of increasing political polarization, lower electoral turnout and voter disaffection with politics and democracy in general (Jamieson 1992; Kahn and Kenny 1999; Mutz and Reeves 2005, Mutz 2007; Brooks and Geer 2007; Lau and Rovner 2009; Harcourt 2012). However, there is no agreement on the definition or measurement of incivility. Our paper presents an automated sentiment analysis to identify uncivil language and to measure the level of (in)civility in parliamentary speeches. Substantively, we study incivility in the Austrian national parliament during the last two decades (1996-2013) and explore some of the political, institutional and individual factors that affect the level of incivility shown in parliamentary debates. We check whether government/opposition status, the parliamentary role, the type of debate and closeness to the next election has an effect on the level of civility observed in parliament.","bibtex":"@inproceedings{jenny2017incivility,\r\n  title={Incivility in Austrian Parliamentary Debates: A supervised sentiment analysis of parliamentary speeches},\r\n  author={Jenny, Marcelo and Haselmayer, Martin and Rudkowsky, Elena and Wastian, Matthias and Emrich, Stefan and Sedlmair, Michael},\r\n  booktitle={European Consortium for Political Research Joint Workshops, Nottingham, UK},\r\n  year={2017}\r\n}","notes":"","funding":""},{"Title":"From Visualization Research to Public Presentation - Design and Realization of a Scientific Exhibition","Submission Target":"SIGRAD","Date":"2017-01-01","Type":"","First Author":"Michael Krone","Other Authors":"Karsten Schatz, Nora Hieronymus, Christoph Müller, Michael Becher, Tina Barthelmes, April Cooper, Steffen Currle, Patrick Gralka, Marcel Hlawatsch, Lisa Pietrzyk, Tobias Rau, Guido Reina, Rene Trefft, Thomas Ertl","Key (e.g. for file names)":"krone2017from","Publisher URL (official)":"https://ep.liu.se/en/conference-article.aspx?series=ecp&issue=143&Article_No=3","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":" In this paper, we present the design considerations of a scientific exhibition we recently realized. The exhibition presented the work of two large research projects related to computer simulations, which include scientific visualization as an essential part of the involved research. Consequently, visualization was also of central importance for our exhibition. It was not only used to illustrate the complex simulation data to convey information about the results from the application domains, but we also wanted to teach visitors about visualization itself. Therefore, explaining the purpose and the challenges of visualization research was a significant part of the exhibition. We describe how we developed an engaging experience of a highly theoretic topic using the same visualization tools we developed for the application scientists and how we integrated the venue into our design. Finally, we discuss our insights from the project as well as visitor feedback.","bibtex":"@inproceedings{krone2017sigrad,\r\n  author = {Krone, Michael and Schatz, Karsten and Hieronymus, Nora and Müller, Christoph and Becher, Michael and Barthelmes, Tina and Cooper, April and Currle, Steffen and Gralka, Patrick and Hlawatsch, Marcel and Pietrzyk, Lisa and Rau, Tobias and Reina, Guido and Trefft, Rene and Ertl, Thomas},\r\n  booktitle = {Proceedings of SIGRAD 2017},\r\n  title = {From Visualization Research to Public Presentation - Design and Realization of a Scientific Exhibition},\r\n  year = 2017\r\n}","notes":"","funding":""},{"Title":"Challenges and Opportunities using Software-defined Visualization in MegaMol","Submission Target":"IXPUG","Date":"2017-01-01","Type":"","First Author":"Tobias Rau","Other Authors":"Michael Krone, Guido Reina, Thomas Ertl","Key (e.g. for file names)":"rau2017challenges","Publisher URL (official)":"","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper we describe how we integrated the OSPRay ray tracing engine into the traditionally GPU-centric MegaMol visualization framework. Since OSPRay is purely CPU-based, this adds software-defined visualization to MegaMol. This enables us to use MegaMol for in-situ rendering on HPC systems that lack GPUs. Furthermore, we designed the integration so that the new OSPRay rendering can be used alongside the classical OpenGL-based rendering. We describe how the ray tracing paradigm, where the whole scene has to be available during rendering, changes the module graph of MegaMol. The performance of the OSPRay ray tracing is shown to be at least competitive with classical GPU-accelerated rendering methods for particle rendering available in MegaMol.","bibtex":"@inproceedings{rau17ospray,\r\n  author = {Rau, Tobias and Krone, Michael and Reina, Guido and Ertl, Thomas},\r\n  booktitle = {7th Workshop on Visual Analytics, Information Visualization and Scientific Visualization},\r\n  title = {Challenges and Opportunities using Software-defined Visualization in MegaMol},\r\n  year = {2017}\r\n}","notes":"","funding":""},{"Title":"A Team-Approach to Putting Learner-Centered Principles to Practice in a Large Course on Human-Computer Interaction","Submission Target":"FIE","Date":"2016-12-01","Type":"","First Author":"Renate Motschnig","Other Authors":"Michael Sedlmair, Svenja Schröder, Torsten Möller","Key (e.g. for file names)":"motschnig2016fie","Publisher URL (official)":"https://doi.org/10.1109/FIE.2016.7757576","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present a case study on how a team of instructors put learner-centered principles into practice in a large undergraduate course on Human-Computer Interaction (HCI) that was run in 4 parallel groups of about 50 students. The course stands on the crossroads between software engineering, business, and research in so far as student-teams apply human-centered design techniques to develop mobile apps, test them with real end-users, read research papers and regularly reflect upon their experience. As a proof of the course-concept, selected results from formative and summative assessments are presented. The summative results show that students rated the course as one of the best of the 87 computer science courses run in the summer term of 2015 at the University of Vienna. The primary goal of this paper is to provide instructors intrigued by learner-centered approaches with ideas for their own practice. In particular, this paper is of interest to those who teach Human-Computer Interaction and to those who seek inspiration on mapping their course to the 14 learner-centered principles.","bibtex":"@INPROCEEDINGS{7757576,\r\n  author={Motschnig, Renate and Sedlmair, Michael and Schröder, Svenja and Möller, Torsten},\r\n  booktitle={2016 IEEE Frontiers in Education Conference (FIE)}, \r\n  title={A team-approach to putting learner-centered principles to practice in a large course on Human-Computer Interaction}, \r\n  year={2016},\r\n  pages={1-9},\r\n  doi={10.1109/FIE.2016.7757576}}\r\n","notes":"","funding":""},{"Title":"Human-Centered Machine Learning Through Interactive Visualization: Review and Open Challenges","Submission Target":"ESANN","Date":"2016-10-24","Type":"","First Author":"Dominik Sacha","Other Authors":"Michael Sedlmair, Leishi Zhang, John Lee, Daniel Weiskopf, Stephen North, Daniel A Keim","Key (e.g. for file names)":"sacha2016esann","Publisher URL (official)":"https://www.esann.org/sites/default/files/proceedings/legacy/es2016-166.pdf","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The goal of visual analytics (VA) systems is to solve complex problems by integrating automated data analysis methods, such as machine learning (ML) algorithms, with interactive visualizations. We propose a conceptual framework that models human interactions with ML components in the VA process, and makes the crucial interplay between automated algorithms and interactive visualizations more concrete. The framework is illustrated through several examples. We derive three open research challenges at the intersection of ML and visualization research that will lead to more effective data analysis.","bibtex":"@inproceedings{sacha2016human,\r\n  title={Human-centered machine learning through interactive visualization},\r\n  author={Sacha, Dominik and Sedlmair, Michael and Zhang, Leishi and Lee, John Aldo and Weiskopf, Daniel and North, Stephen and Keim, Daniel},\r\n  year={2016},\r\n  organization={ESANN}\r\n}\r\n","notes":"","funding":""},{"Title":"Design Study Contributions Come in Different Guises: Seven Guiding Scenarios","Submission Target":"VIS","Date":"2016-10-24","Type":"Workshop Paper","First Author":"Michael Sedlmair","Other Authors":"","Key (e.g. for file names)":"sedlmair2016beliv","Publisher URL (official)":"https://doi.org/10.1145/2993901.2993913","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Design studies are projects in which visualization researchers seek to design visualization tools that help solving challenging real-world problems faced by domain experts. While design studies have become a vital component of visualization research, reflecting on actionable contributions from them often poses challenges. The goal of this paper is to better characterize different contributions that can result from design study projects. Towards this goal, a set of seven guiding scenarios for characterizing design study contributions is proposed. The scenarios are meant to help authors identify and depict design study contributions that are interesting and actionable for other visualization researchers. They are also meant to provide better guidance in evaluating design study contributions in the reviewing process.","bibtex":"@inproceedings{10.1145/2993901.2993913,\r\nauthor = {Sedlmair, Michael},\r\ntitle = {Design Study Contributions Come in Different Guises: Seven Guiding Scenarios},\r\nyear = {2016},\r\nisbn = {9781450348188},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/2993901.2993913},\r\ndoi = {10.1145/2993901.2993913},\r\nabstract = {Design studies are projects in which visualization researchers seek to design visualization tools that help solving challenging real-world problems faced by domain experts. While design studies have become a vital component of visualization research, reflecting on actionable contributions from them often poses challenges. The goal of this paper is to better characterize different contributions that can result from design study projects. Towards this goal, a set of seven guiding scenarios for characterizing design study contributions is proposed. The scenarios are meant to help authors identify and depict design study contributions that are interesting and actionable for other visualization researchers. They are also meant to provide better guidance in evaluating design study contributions in the reviewing process.},\r\nbooktitle = {Proceedings of the Sixth Workshop on Beyond Time and Errors on Novel Evaluation Methods for Visualization},\r\npages = {152–161},\r\nnumpages = {10},\r\nkeywords = {visualization, contribution, position paper, Design study},\r\nlocation = {Baltimore, MD, USA},\r\nseries = {BELIV '16}\r\n}","notes":"","funding":""},{"Title":"vispubdata.org: A Metadata Collection about IEEE Visualization (VIS) Publications","Submission Target":"TVCG","Date":"2016-10-05","Type":"","First Author":"Petra Isenberg","Other Authors":"Florian Heimerl, Steffen Koch, Tobias Isenberg, Panpan Xu, Chad Stolper, Michael Sedlmair, Jian Chen, Torsten Möller, John Stasko","Key (e.g. for file names)":"isenberg2017tvcg","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2016.2615308","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"https://vispubdata.org","Acknowledgements":"","Abstract":"We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.","bibtex":"@ARTICLE{7583708,\r\n  author={Isenberg, Petra and Heimerl, Florian and Koch, Steffen and Isenberg, Tobias and Xu, Panpan and Stolper, Charles D. and Sedlmair, Michael and Chen, Jian and Möller, Torsten and Stasko, John},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications}, \r\n  year={2017},\r\n  volume={23},\r\n  number={9},\r\n  pages={2199-2206},\r\n  doi={10.1109/TVCG.2016.2615308}}\r\n","notes":"","funding":""},{"Title":"Reduce Simulator Sickness by Overwritten Symbol in Smartphone-Based VR System","Submission Target":"IEEE ICVRV","Date":"2016-09-24","Type":"Workshop / Short Paper","First Author":"Xingyao Yu","Other Authors":"Dongdong Weng, Li Cai","Key (e.g. for file names)":"yu2016reduce","Publisher URL (official)":"https://doi.org/10.1109/ICVRV.2016.78","url2":"","PDF URL (public)":"https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7938233","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The aim of this paper is to reduce simulator sickness caused by the low refresh rate of display in smartphone-based VR system. Without regard to the improvement of hardware, the method proposed in this paper reduces simulator sickness by adding static symbol on the screen of the smartphone. A series of user-participation experiments were done to validate the effectiveness of the method. Participants' responses to the symbol with different textures (cross or Minion logo) and in different positions (the center or near the corners) were assessed by Simulator Sickness Questionnaire (SSQ). The preliminary results demonstrate that the existence, the position and complexity of the symbols can be factors in relieving symptoms of simulator sickness.","bibtex":"@INPROCEEDINGS{7938233,\r\n  author={Yu, Xingyao and Weng, Dongdong and Cai, Li},\r\n  booktitle={2016 International Conference on Virtual Reality and Visualization (ICVRV)}, \r\n  title={Reduce Simulator Sickness by Overwritten Symbol in Smartphone-Based VR System}, \r\n  year={2016},\r\n  pages={426-429},\r\n  doi={10.1109/ICVRV.2016.78}}\r\n","notes":"","funding":""},{"Title":"Visualization as Seen Through its Research Paper Keywords","Submission Target":"TVCG","Date":"2016-08-10","Type":"","First Author":"Petra Isenberg","Other Authors":"Tobias Isenberg, Michael Sedlmair, Jian Chen, Möller Torsten","Key (e.g. for file names)":"isenberg2016tvcg","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2016.2598827","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"http://tobias.isenberg.cc/uploads/VideosAndDemos/Isenberg_2017_VST_additional.zip","Acknowledgements":"","Abstract":"We present the results of a comprehensive multi-pass analysis of visualization paper keywords supplied by authors for their papers published in the IEEE Visualization conference series (now called IEEE VIS) between 1990-2015. From this analysis we derived a set of visualization topics that we discuss in the context of the current taxonomy that is used to categorize papers and assign reviewers in the IEEE VIS reviewing process. We point out missing and overemphasized topics in the current taxonomy and start a discussion on the importance of establishing common visualization terminology. Our analysis of research topics in visualization can, thus, serve as a starting point to (a) help create a common vocabulary to improve communication among different visualization sub-groups, (b) facilitate the process of understanding differences and commonalities of the various research sub-fields in visualization, (c) provide an understanding of emerging new research trends, (d) facilitate the crucial step of finding the right reviewers for research submissions, and (e) it can eventually lead to a comprehensive taxonomy of visualization research. One additional tangible outcome of our work is an online query tool (http://keyvis.org/) that allows visualization researchers to easily browse the 3952 keywords used for IEEE VIS papers since 1990 to find related work or make informed keyword choices.","bibtex":"@ARTICLE{7539364,\r\n  author={Isenberg, Petra and Isenberg, Tobias and Sedlmair, Michael and Chen, Jian and Möller, Torsten},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Visualization as Seen through its Research Paper Keywords}, \r\n  year={2017},\r\n  volume={23},\r\n  number={1},\r\n  pages={771-780},\r\n  doi={10.1109/TVCG.2016.2598827}}","notes":"","funding":""},{"Title":"Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis","Submission Target":"TVCG","Date":"2016-08-08","Type":"","First Author":"Dominik Sacha","Other Authors":"Leishi Zhang, Michael Sedlmair, John Aldo Lee, Jaakko Peltonen, Daniel Weiskopf, Stephen North, Daniel A Keim","Key (e.g. for file names)":"sacha2016vast","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2016.2598495","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Dimensionality Reduction (DR) is a core building block in visualizing multidimensional data. For DR techniques to be useful in exploratory data analysis, they need to be adapted to human needs and domain-specific problems, ideally, interactively, and on-the-fly. Many visual analytics systems have already demonstrated the benefits of tightly integrating DR with interactive visualizations. Nevertheless, a general, structured understanding of this integration is missing. To address this, we systematically studied the visual analytics and visualization literature to investigate how analysts interact with automatic DR techniques. The results reveal seven common interaction scenarios that are amenable to interactive control such as specifying algorithmic constraints, selecting relevant features, or choosing among several DR algorithms. We investigate specific implementations of visual analysis systems integrating DR, and analyze ways that other machine learning methods have been combined with DR. Summarizing the results in a “human in the loop” process model provides a general lens for the evaluation of visual interactive DR systems. We apply the proposed model to study and classify several systems previously described in the literature, and to derive future research opportunities.","bibtex":"@ARTICLE{7536217,\r\n  author={Sacha, Dominik and Zhang, Leishi and Sedlmair, Michael and Lee, John A. and Peltonen, Jaakko and Weiskopf, Daniel and North, Stephen C. and Keim, Daniel A.},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis}, \r\n  year={2017},\r\n  volume={23},\r\n  number={1},\r\n  pages={241-250},\r\n  doi={10.1109/TVCG.2016.2598495}}\r\n","notes":"","funding":""},{"Title":"Virtual UNREALity: Exploring Alternative Visualization Techniques for Virtual Reality","Submission Target":"xCoAX","Date":"2016-07-07","Type":"Full Paper","First Author":"Natalie Hube","Other Authors":"Hannes Grusla, Mathias Müller, Ingmar S. Franke, Tobias Günther, Rainer Groh","Key (e.g. for file names)":"hube2016virtual","Publisher URL (official)":"http://2016.xcoax.org/pdf/xcoax2016-Hube.pdf","url2":"","PDF URL (public)":"http://2016.xcoax.org/pdf/xcoax2016-Hube.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Virtual Reality (VR) offers new ways to perceive and interact with virtual content. Apart from photo-realism, VR can be used to explore new ways of visualization and interaction. In this contribution, we describe two student projects, which focused on creating innovative concepts for an artistic VR experience. We provide a review of sources of inspiration ranging from standard NPR-techniques through movies, interactive artworks and games to phenomena of human perception. Based on this wide collection of material we describe the prototypes, and discuss observations during implementation and from user feedback. Finally, possible future directions to use the potential of VR as a tool for novel, artful and unconventional experiences are discussed.","bibtex":"","notes":"","funding":""},{"Title":"Visual Analysis of Governing Topological Structures in Excitable Network Dynamics","Submission Target":"CGF","Date":"2016-07-04","Type":"Full Paper","First Author":"Quynh Quang Ngo","Other Authors":"Marc-Thorsten Hütt, Lars Linsen","Key (e.g. for file names)":"ngo2016visual","Publisher URL (official)":"https://doi.org/10.1111/cgf.12906","url2":"https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.12906","PDF URL (public)":"https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12906","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"To understand how topology shapes the dynamics in excitable networks is one of the fundamental problems in network science when applied to computational systems biology and neuroscience. Recent advances in the field discovered the influential role of two macroscopic topological structures, namely hubs and modules. We propose a visual analytics approach that allows for a systematic exploration of the role of those macroscopic topological structures on the dynamics in excitable networks. Dynamical patterns are discovered using the dynamical features of excitation ratio and co-activation. Our approach is based on the interactive analysis of the correlation of topological and dynamical features using coordinated views. We designed suitable visual encodings for both the topological and the dynamical features. A degree map and an adjacency matrix visualization allow for the interaction with hubs and modules, respectively. A barycentric-coordinates layout and a multi-dimensional scaling approach allow for the analysis of excitation ratio and co-activation, respectively. We demonstrate how the interplay of the visual encodings allows us to quickly reconstruct recent findings in the field within an interactive analysis and even discovered new patterns. We apply our approach to network models of commonly investigated topologies as well as to the structural networks representing the connectomes of different species. We evaluate our approach with domain experts in terms of its intuitiveness, expressiveness, and usefulness.","bibtex":"@article{DBLP:journals/cgf/NgoHL16,\r\n  author    = {Quynh Quang Ngo and\r\n               Marc{-}Thorsten H{\\\"{u}}tt and\r\n               Lars Linsen},\r\n  title     = {Visual Analysis of Governing Topological Structures in Excitable Network\r\n               Dynamics},\r\n  journal   = {Comput. Graph. Forum},\r\n  volume    = {35},\r\n  number    = {3},\r\n  pages     = {301--310},\r\n  year      = {2016},\r\n  url       = {https://doi.org/10.1111/cgf.12906},\r\n  doi       = {10.1111/cgf.12906},\r\n  timestamp = {Fri, 26 May 2017 22:53:54 +0200},\r\n  biburl    = {https://dblp.org/rec/journals/cgf/NgoHL16.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}","notes":"","funding":""},{"Title":"SepMe: 2002 New Visual Separation Measures","Submission Target":"PacificVis","Date":"2016-05-05","Type":"","First Author":"Michael Aupetit","Other Authors":"Michael Sedlmair","Key (e.g. for file names)":"aupetit2016sepme","Publisher URL (official)":"https://doi.org/10.1109/PACIFICVIS.2016.7465244","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Our goal is to accurately model human class separation judgements in color-coded scatterplots. Towards this goal, we propose a set of 2002 visual separation measures, by systematically combining 17 neighborhood graphs and 14 class purity functions, with different parameterizations. Using a Machine Learning framework, we evaluate these measures based on how well they predict human separation judgements. We found that more than 58% of the 2002 new measures outperform the best state-of-the-art Distance Consistency (DSC) measure. Among the 2002, the best measure is the average proportion of same-class neighbors among the 0.35-Observable Neighbors of each point of the target class (short GONG 0.35 DIR CPT), with a prediction accuracy of 92.9%, which is 11.7% better than DSC. We also discuss alternative, well-performing measures and give guidelines when to use which.","bibtex":"@INPROCEEDINGS{7465244,\r\n  author={Aupetit, Michael and Sedlmair, Michael},\r\n  booktitle={2016 IEEE Pacific Visualization Symposium (PacificVis)}, \r\n  title={SepMe: 2002 New visual separation measures}, \r\n  year={2016},\r\n  pages={1-8},\r\n  doi={10.1109/PACIFICVIS.2016.7465244}}","notes":"","funding":""},{"Title":"Visual Analytics for Concept Exploration in Subspaces of Patient Groups: Making Sense of Complex Datasets with the Doctor-in-the-Loop","Submission Target":"Brain Informatics","Date":"2016-03-21","Type":"","First Author":"Michael Hund","Other Authors":"Dominic Böhm, Werner Sturm, Michael Sedlmair, Tobias Schreck, Torsten Ullrich, Daniel A. Keim, Ljiljana Majnaric, Andreas Holzinger ","Key (e.g. for file names)":"hund2016brain","Publisher URL (official)":"https://doi.org/10.1007/s40708-016-0043-5","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Medical doctors and researchers in bio-medicine are increasingly confronted with complex patient data, posing new and difficult analysis challenges. These data are often comprising high-dimensional descriptions of patient conditions and measurements on the success of certain therapies. An important analysis question in such data is to compare and correlate patient conditions and therapy results along with combinations of dimensions. As the number of dimensions is often very large, one needs to map them to a smaller number of relevant dimensions to be more amenable for expert analysis. This is because irrelevant, redundant, and conflicting dimensions can negatively affect effectiveness and efficiency of the analytic process (the so-called curse of dimensionality). However, the possible mappings from high- to low-dimensional spaces are ambiguous. For example, the similarity between patients may change by considering different combinations of relevant dimensions (subspaces). We demonstrate the potential of subspace analysis for the interpretation of high-dimensional medical data. Specifically, we present SubVIS, an interactive tool to visually explore subspace clusters from different perspectives, introduce a novel analysis workflow, and discuss future directions for high-dimensional (medical) data analysis and its visual exploration. We apply the presented workflow to a real-world dataset from the medical domain and show its usefulness with a domain expert evaluation.","bibtex":"@article{hund2016visual,\r\n  title={Visual analytics for concept exploration in subspaces of patient groups},\r\n  author={Hund, Michael and B{\\\"o}hm, Dominic and Sturm, Werner and Sedlmair, Michael and Schreck, Tobias and Ullrich, Torsten and Keim, Daniel A and Majnaric, Ljiljana and Holzinger, Andreas},\r\n  journal={Brain Informatics},\r\n  volume={3},\r\n  number={4},\r\n  pages={233--247},\r\n  year={2016},\r\n  publisher={SpringerOpen}\r\n}\r\n","notes":"","funding":""},{"Title":"TagFlip: Active Mobile Music Discovery with Social Tags","Submission Target":"IUI","Date":"2016-03-07","Type":"","First Author":"Mohsen Kamalzadeh","Other Authors":"Christoph Kralj, Torsten Möller, Michael Sedlmair","Key (e.g. for file names)":"kamalzadeh2016tagflip","Publisher URL (official)":"https://doi.org/10.1145/2856767.2856780","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We report on the design and evaluation of TagFlip, a novel interface for active music discovery based on social tags of music. The tool, which was built for phone-sized screens, couples high user control on the recommended music with minimal interaction effort. Contrary to conventional recommenders, which only allow the specification of seed attributes and the subsequent like/dislike of songs, we put the users in the centre of the recommendation process. With a library of 100,000 songs, TagFlip describes each played song to the user through its most popular tags on Last.fm and allows the user to easily specify which of the tags should be considered for the next song, or the next stream of songs. In a lab user study where we compared it to Spotify's mobile application, TagFlip came out on top in both subjective user experience (control, transparency, and trust) and our objective measure of number of interactions per liked song. Our users found TagFlip to be an important complementary experience to that of Spotify, enabling more active and directed discovery sessions as opposed to the mostly passive experience that traditional recommenders offer.","bibtex":"@inproceedings{10.1145/2856767.2856780,\r\nauthor = {Kamalzadeh, Mohsen and Kralj, Christoph and M\\\"{o}ller, Torsten and Sedlmair, Michael},\r\ntitle = {TagFlip: Active Mobile Music Discovery with Social Tags},\r\nyear = {2016},\r\nisbn = {9781450341370},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/2856767.2856780},\r\ndoi = {10.1145/2856767.2856780},\r\nabstract = {We report on the design and evaluation of TagFlip, a novel interface for active music discovery based on social tags of music. The tool, which was built for phone-sized screens, couples high user control on the recommended music with minimal interaction effort. Contrary to conventional recommenders, which only allow the specification of seed attributes and the subsequent like/dislike of songs, we put the users in the centre of the recommendation process. With a library of 100,000 songs, TagFlip describes each played song to the user through its most popular tags on Last.fm and allows the user to easily specify which of the tags should be considered for the next song, or the next stream of songs. In a lab user study where we compared it to Spotify's mobile application, TagFlip came out on top in both subjective user experience (control, transparency, and trust) and our objective measure of number of interactions per liked song. Our users found TagFlip to be an important complementary experience to that of Spotify, enabling more active and directed discovery sessions as opposed to the mostly passive experience that traditional recommenders offer.},\r\nbooktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},\r\npages = {19–30},\r\nnumpages = {12},\r\nkeywords = {transparency, folksonomies, music discovery, fine tuning, social tags, minimal effort, user controlled, user-centred design, recommendation, user interface, exploration},\r\nlocation = {Sonoma, California, USA},\r\nseries = {IUI '16}\r\n}","notes":"","funding":""},{"Title":"Visual Exploration of Media Transparency for Data Journalists: Problem Characterization and Abstraction","Submission Target":"FFH","Date":"2016-01-01","Type":"","First Author":"Christina Niederer","Other Authors":"Alexander Rind, Wolfgang Aigner, Julian Ausserhofer, Robert Gutounig, Michael Sedlmair","Key (e.g. for file names)":"niederer2016ffh","Publisher URL (official)":"http://ffhoarep.fh-ooe.at/bitstream/123456789/542/1/109_305_Niederer_FullPaper_en_Final.pdf","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Today, journalists increasingly deal with complex, large, and heterogeneous datasets and, thus, face challenges in integration, wrangling, analysis, and reporting these data. Besides, the lack of money, time, and skills influence their journalistic work. Information visualization and visual analytics offer possibilities to support data journalists. This paper contributes to an overview of a possible characterization and abstraction of certain aspects of data-driven journalism in Austria. A case study was conducted based on the dataset of media transparency in Austria. We conducted four semi- structured interviews with Austrian data journalists, as well as an exploratory data analysis of the media transparency dataset. To categorize our findings we used Munzner ́s analytical framework and the Data-User-Task Design Triangle by Miksch and Aigner.\r\n","bibtex":"@article{aigner2016visual,\r\n  title={Visual exploration of media transparency for data journalists: Problem characterization and abstraction},\r\n  author={Aigner, Wolfgang and Niederer, Christina and Sedlmair, Michael and Rind, Alexander and Gutounig, Robert and Ausserhofer, Julian and others},\r\n  year={2016}\r\n}","notes":"","funding":""},{"Title":"Decision Making in Uncertainty Visualization","Submission Target":"VDMU","Date":"2015-10-25","Type":"","First Author":"Thomas Torsney-Weir","Other Authors":"Michael Sedlmair, Torsten Möller","Key (e.g. for file names)":"torsneyweir2015decision","Publisher URL (official)":"http://eprints.cs.univie.ac.at/4598/","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this position paper we investigate the role of decision making in uncertainty visualization. We introduce common decision making strategies identified by the cognitive science community [22]. These strategies are then used to reanalyze 21 design study papers that have previously been used as a foundation for defining visual parameter space analysis [26]. We found that current strategies in these tools relied mostly on one parameter at a time and are about filtering alternatives. Based on these results, we propose three questions for further discussion and research.","bibtex":"@inproceedings{cs4598,\r\n        year = {2015},\r\n      author = {Thomas Torsney-Weir and Michael Sedlmair and Torsten M{\\\"o}ller},\r\n        month = {October},\r\n        title = {Decision making in uncertainty visualization},\r\n    booktitle = {VDMU Workshop on Visualization for Decision Making under Uncertainty 2015},\r\n    abstract = {In this position paper we investigate the role of decision making in uncertainty visualization. We introduce common decision making strategies identified by the cognitive science community [22]. These strategies are then used to reanalyze 21 design study papers that have previously been used as a foundation for defining visual parameter space analysis [26]. We found that current strategies in these tools relied mostly on one parameter at a time and are about filtering alternatives. Based on these results, we propose three questions for further discussion and research.},\r\n          url = {http://eprints.cs.univie.ac.at/4598/}\r\n}","notes":"","funding":""},{"Title":"Subspace Nearest Neighbor Search - Problem Statement, Approaches, and Discussion","Submission Target":"SISAP","Date":"2015-10-17","Type":"","First Author":"Michael Hund","Other Authors":"Michael Behrisch, Ines Färber, Michael Sedlmair, Tobias Schreck, Thomas Seidl, Daniel A Keim","Key (e.g. for file names)":"hund2015snns","Publisher URL (official)":"https://doi.org/10.1007/978-3-319-25087-8_29","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Computing the similarity between objects is a central task for many applications in the field of information retrieval and data mining. For finding k-nearest neighbors, typically a ranking is computed based on a predetermined set of data dimensions and a distance function, constant over all possible queries. However, many high-dimensional feature spaces contain a large number of dimensions, many of which may contain noise, irrelevant, redundant, or contradicting information. More specifically, the relevance of dimensions may depend on the query object itself, and in general, different dimension sets (subspaces) may be appropriate for a query. Approaches for feature selection or -weighting typically provide a global subspace selection, which may not be suitable for all possibly queries. In this position paper, we frame a new research problem, called subspace nearest neighbor search, aiming at multiple query-dependent subspaces for nearest neighbor search. We describe relevant problem characteristics, relate to existing approaches, and outline potential research directions.","bibtex":"@InProceedings{10.1007/978-3-319-25087-8_29,\r\nauthor=\"Hund, Michael\r\nand Behrisch, Michael\r\nand F{\\\"a}rber, Ines\r\nand Sedlmair, Michael\r\nand Schreck, Tobias\r\nand Seidl, Thomas\r\nand Keim, Daniel\",\r\neditor=\"Amato, Giuseppe\r\nand Connor, Richard\r\nand Falchi, Fabrizio\r\nand Gennaro, Claudio\",\r\ntitle=\"Subspace Nearest Neighbor Search - Problem Statement, Approaches, and Discussion\",\r\nbooktitle=\"Similarity Search and Applications\",\r\nyear=\"2015\",\r\npublisher=\"Springer International Publishing\",\r\naddress=\"Cham\",\r\npages=\"307--313\",\r\nabstract=\"Computing the similarity between objects is a central task for many applications in the field of information retrieval and data mining. For finding k-nearest neighbors, typically a ranking is computed based on a predetermined set of data dimensions and a distance function, constant over all possible queries. However, many high-dimensional feature spaces contain a large number of dimensions, many of which may contain noise, irrelevant, redundant, or contradicting information. More specifically, the relevance of dimensions may depend on the query object itself, and in general, different dimension sets (subspaces) may be appropriate for a query. Approaches for feature selection or -weighting typically provide a global subspace selection, which may not be suitable for all possibly queries. In this position paper, we frame a new research problem, called subspace nearest neighbor search, aiming at multiple query-dependent subspaces for nearest neighbor search. We describe relevant problem characteristics, relate to existing approaches, and outline potential research directions.\",\r\nisbn=\"978-3-319-25087-8\"\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Data-driven Evaluation of Visual Quality Measures","Submission Target":"CGF","Date":"2015-07-20","Type":"","First Author":"Michael Sedlmair","Other Authors":"Michael Aupetit","Key (e.g. for file names)":"sedlmair2015ddeval","Publisher URL (official)":"https://doi.org/10.1111/cgf.12632","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Visual quality measures seek to algorithmically imitate human judgments of patterns such as class separability, correlation, or outliers. In this paper, we propose a novel data-driven framework for evaluating such measures. The basic idea is to take a large set of visually encoded data, such as scatterplots, with reliable human “ground truth” judgements, and to use this human-labeled data to learn how well a measure would predict human judgements on previously unseen data. Measures can then be evaluated based on predictive performance—an approach that is crucial for generalizing across datasets but has gained little attention so far. To illustrate our framework, we use it to evaluate 15 state-of-the-art class separation measures, using human ground truth data from 828 class separation judgments on color-coded 2D scatterplots.","bibtex":"@article{https://doi.org/10.1111/cgf.12632,\r\nauthor = {Sedlmair, M. and Aupetit, M.},\r\ntitle = {Data-driven Evaluation of Visual Quality Measures},\r\njournal = {Computer Graphics Forum},\r\nvolume = {34},\r\nnumber = {3},\r\npages = {201-210},\r\nkeywords = {Categories and Subject Descriptors (according to ACM CCS), H.5.0 Information Interfaces and Presentation: General},\r\ndoi = {https://doi.org/10.1111/cgf.12632},\r\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12632},\r\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12632},\r\nabstract = {Abstract Visual quality measures seek to algorithmically imitate human judgments of patterns such as class separability, correlation, or outliers. In this paper, we propose a novel data-driven framework for evaluating such measures. The basic idea is to take a large set of visually encoded data, such as scatterplots, with reliable human “ground truth” judgements, and to use this human-labeled data to learn how well a measure would predict human judgements on previously unseen data. Measures can then be evaluated based on predictive performance—an approach that is crucial for generalizing across datasets but has gained little attention so far. To illustrate our framework, we use it to evaluate 15 state-of-the-art class separation measures, using human ground truth data from 828 class separation judgments on color-coded 2D scatterplots.},\r\nyear = {2015}\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Bridging the Gap of Domain and Visualization Experts with a Liaison","Submission Target":"EuroVis","Date":"2015-05-25","Type":"Short Paper","First Author":"Svenja Simon","Other Authors":"Sebastian Mittelstädt, Daniel A Keim, Michael Sedlmair","Key (e.g. for file names)":"simon2015liaison","Publisher URL (official)":"http://eprints.cs.univie.ac.at/4550/","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We introduce the role Liaison for design study projects. With considerable expertise in visualization and the application domain, a Liaison can help to foster richer and more effective interdisciplinary communication in problem characterization, design, and evaluation processes. We characterize this role, provide a list of tasks of Liaison and visualization experts, and discuss concrete benefits and potential limitations based on our experience from multiple design studies. To illustrate our contributions we use as an example a molecular biology design study.","bibtex":"@inproceedings{cs4550,\r\n          volume = {2015},\r\n       booktitle = {Eurographics Conference on Visualization (EuroVis) - Short Papers},\r\n           title = {Bridging the gap of domain and visualization experts with a Liaison},\r\n            year = {2015},\r\n          author = {Svenja Simon and Sebastian Mittelst{\\\"a}dt and Daniel A. Keim and Michael Sedlmair},\r\n       publisher = {The Eurographics Association},\r\n             doi = {10.2312/eurovisshort.20151137},\r\n             url = {http://eprints.cs.univie.ac.at/4550/},\r\n        abstract = {We introduce the role Liaison for design study projects. With considerable expertise in visualization and the\r\napplication domain, a Liaison can help to foster richer and more effective interdisciplinary communication in\r\nproblem characterization, design, and evaluation processes. We characterize this role, provide a list of tasks of\r\nLiaison and visualization experts, and discuss concrete benefits and potential limitations based on our experience\r\nfrom multiple design studies. To illustrate our contributions we use as an example a molecular biology design study.}\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Utilizing the Effects of Priming to Facilitate Text Comprehension","Submission Target":"CHI","Date":"2015-04-18","Type":"Extended Abstract","First Author":"Katrin Angerbauer","Other Authors":"Tilman Dingler, Dagmar Kern, Albrecht Schmidt","Key (e.g. for file names)":"angerbauer2015chi","Publisher URL (official)":"https://doi.org/10.1145/2702613.2732914","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Due to the ever-growing amount of textual information we face in our everyday life, the skill of scanning and absorbing the essence of a piece of text is crucial. We cannot afford to read every text in detail, hence we need to acquire strategies to quickly decide on the importance of a text and how to grasp its content. Additionally, the sheer amount of daily reading makes it hard to remember the gist of every text encountered. Research in psychology has proposed priming as an implicit memory effect where exposure to one stimulus influences the response to a subsequent stimulus. Hence, exposure to contextual information can influence comprehension and recall. In our work we investigate the feasibility of using such an effect to visually present text summaries that are quick to understand and deliver the essence of a text in order to help readers not only make informed decisions about whether to read the text or not, but also to build out more cognitive associations that help to remember the content of the text afterward. In two focus groups we discussed our approach by providing four different visualizations representing the gist and important details of the text. In this paper we introduce the visualizations as well as results of the focus groups.","bibtex":"@inproceedings{10.1145/2702613.2732914,\r\nauthor = {Angerbauer, Katrin and Dingler, Tilman and Kern, Dagmar and Schmidt, Albrecht},\r\ntitle = {Utilizing the Effects of Priming to Facilitate Text Comprehension},\r\nyear = {2015},\r\nisbn = {9781450331463},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/2702613.2732914},\r\ndoi = {10.1145/2702613.2732914},\r\nabstract = {Due to the ever-growing amount of textual information we face in our everyday life, the skill of scanning and absorbing the essence of a piece of text is crucial. We cannot afford to read every text in detail, hence we need to acquire strategies to quickly decide on the importance of a text and how to grasp its content. Additionally, the sheer amount of daily reading makes it hard to remember the gist of every text encountered. Research in psychology has proposed priming as an implicit memory effect where exposure to one stimulus influences the response to a subsequent stimulus. Hence, exposure to contextual information can influence comprehension and recall. In our work we investigate the feasibility of using such an effect to visually present text summaries that are quick to understand and deliver the essence of a text in order to help readers not only make informed decisions about whether to read the text or not, but also to build out more cognitive associations that help to remember the content of the text afterward. In two focus groups we discussed our approach by providing four different visualizations representing the gist and important details of the text. In this paper we introduce the visualizations as well as results of the focus groups.},\r\nbooktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},\r\npages = {1043–1048},\r\nnumpages = {6},\r\nkeywords = {comprehension, reading interfaces, priming},\r\nlocation = {Seoul, Republic of Korea},\r\nseries = {CHI EA '15}\r\n}","notes":"","funding":""},{"Title":"Visualizing Dimensionally-Reduced Data: Interviews with Analysts and a Characterization of Task Sequences","Submission Target":"CHI BELIV","Date":"2014-11-10","Type":"Workshop Paper","First Author":"Matthew Brehmer","Other Authors":"Michael Sedlmair, Stephen Ingram, Tamara Munzner","Key (e.g. for file names)":"brehmer2014drtasks","Publisher URL (official)":"https://doi.org/10.1145/2669557.2669559","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We characterize five task sequences related to visualizing dimensionally-reduced data, drawing from data collected from interviews with ten data analysts spanning six application domains, and from our understanding of the technique literature. Our characterization of visualization task sequences for dimensionally-reduced data fills a gap created by the abundance of proposed techniques and tools that combine high-dimensional data analysis, dimensionality reduction, and visualization, and is intended to be used in the design and evaluation of future techniques and tools. We discuss implications for the evaluation of existing work practices, for the design of controlled experiments, and for the analysis of post-deployment field observations.","bibtex":"@inproceedings{10.1145/2669557.2669559,\r\nauthor = {Brehmer, Matthew and Sedlmair, Michael and Ingram, Stephen and Munzner, Tamara},\r\ntitle = {Visualizing Dimensionally-Reduced Data: Interviews with Analysts and a Characterization of Task Sequences},\r\nyear = {2014},\r\nisbn = {9781450332095},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/2669557.2669559},\r\ndoi = {10.1145/2669557.2669559},\r\nabstract = {We characterize five task sequences related to visualizing dimensionally-reduced data, drawing from data collected from interviews with ten data analysts spanning six application domains, and from our understanding of the technique literature. Our characterization of visualization task sequences for dimensionally-reduced data fills a gap created by the abundance of proposed techniques and tools that combine high-dimensional data analysis, dimensionality reduction, and visualization, and is intended to be used in the design and evaluation of future techniques and tools. We discuss implications for the evaluation of existing work practices, for the design of controlled experiments, and for the analysis of post-deployment field observations.},\r\nbooktitle = {Proceedings of the Fifth Workshop on Beyond Time and Errors: Novel Evaluation Methods for Visualization},\r\npages = {1–8},\r\nnumpages = {8},\r\nkeywords = {tasks, dimensionally-reduced data, interview study},\r\nlocation = {Paris, France},\r\nseries = {BELIV '14}\r\n}","notes":"","funding":""},{"Title":"Opening the Black Box: Strategies for Increased User Involvement in Existing Algorithm Implementations","Submission Target":"TVCG","Date":"2014-11-06","Type":"","First Author":"Thomas Mühlbacher","Other Authors":"Harald Piringer, Samuel Gratzl, Michael Sedlmair, Marc Streit","Key (e.g. for file names)":"muhlbacher2014tui","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2014.2346578","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"An increasing number of interactive visualization tools stress the integration with computational software like MATLAB and R to access a variety of proven algorithms. In many cases, however, the algorithms are used as black boxes that run to completion in isolation which contradicts the needs of interactive data exploration. This paper structures, formalizes, and discusses possibilities to enable user involvement in ongoing computations. Based on a structured characterization of needs regarding intermediate feedback and control, the main contribution is a formalization and comparison of strategies for achieving user involvement for algorithms with different characteristics. In the context of integration, we describe considerations for implementing these strategies either as part of the visualization tool or as part of the algorithm, and we identify requirements and guidelines for the design of algorithmic APIs. To assess the practical applicability, we provide a survey of frequently used algorithm implementations within R regarding the fulfillment of these guidelines. While echoing previous calls for analysis modules which support data exploration more directly, we conclude that a range of pragmatic options for enabling user involvement in ongoing computations exists on both the visualization and algorithm side and should be used.","bibtex":"@ARTICLE{6875995,\r\n  author={Mühlbacher, Thomas and Piringer, Harald and Gratzl, Samuel and Sedlmair, Michael and Streit, Marc},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Opening the Black Box: Strategies for Increased User Involvement in Existing Algorithm Implementations}, \r\n  year={2014},\r\n  volume={20},\r\n  number={12},\r\n  pages={1643-1652},\r\n  doi={10.1109/TVCG.2014.2346578}}\r\n","notes":"","funding":""},{"Title":"Visual Parameter Space Analysis: A Conceptual Framework","Submission Target":"TVCG","Date":"2014-11-06","Type":"","First Author":"Michael Sedlmair","Other Authors":"Christoph Heinzl, Stefan Bruckner, Harald Piringer, Torsten Möller","Key (e.g. for file names)":"sedlmair2014toi","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2014.2346321","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Various case studies in different application domains have shown the great potential of visual parameter space analysis to support validating and using simulation models. In order to guide and systematize research endeavors in this area, we provide a conceptual framework for visual parameter space analysis problems. The framework is based on our own experience and a structured analysis of the visualization literature. It contains three major components: (1) a data flow model that helps to abstractly describe visual parameter space analysis problems independent of their application domain; (2) a set of four navigation strategies of how parameter space analysis can be supported by visualization tools; and (3) a characterization of six analysis tasks. Based on our framework, we analyze and classify the current body of literature, and identify three open research gaps in visual parameter space analysis. The framework and its discussion are meant to support visualization designers and researchers in characterizing parameter space analysis problems and to guide their design and evaluation processes.","bibtex":"@ARTICLE{6876043,\r\n  author={Sedlmair, Michael and Heinzl, Christoph and Bruckner, Stefan and Piringer, Harald and Möller, Torsten},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Visual Parameter Space Analysis: A Conceptual Framework}, \r\n  year={2014},\r\n  volume={20},\r\n  number={12},\r\n  pages={2161-2170},\r\n  doi={10.1109/TVCG.2014.2346321}}\r\n","notes":"","funding":""},{"Title":"Toward a Deeper Understanding of Visualization Through Keyword Analysis","Submission Target":"arXiv","Date":"2014-08-13","Type":"Technical Report","First Author":"Petra Isenberg","Other Authors":"Tobias Isenberg, Michael Sedlmair, Jian Chen, Torsten Möller","Key (e.g. for file names)":"isenberg2014keyvis-tr","Publisher URL (official)":"https://doi.org/10.48550/arXiv.1408.3297","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present the results of a comprehensive analysis of visualization paper keywords supplied for 4366 papers submitted to five main visualization conferences. We describe main keywords, topic areas, and 10-year historic trends from two datasets: (1) the standardized PCS taxonomy keywords in use for paper submissions for IEEE InfoVis, IEEE Vis-SciVis, IEEE VAST, EuroVis, and IEEE PacificVis since 2009 and (2) the author-chosen keywords for papers published in the IEEE Visualization conference series (now called IEEE VIS) since 2004. Our analysis of research topics in visualization can serve as a starting point to (a) help create a common vocabulary to improve communication among different visualization sub-groups, (b) facilitate the process of understanding differences and commonalities of the various research sub-fields in visualization, (c) provide an understanding of emerging new research trends, (d) facilitate the crucial step of finding the right reviewers for research submissions, and (e) it can eventually lead to a comprehensive taxonomy of visualization research. One additional tangible outcome of our work is an application that allows visualization researchers to easily browse the 2600+ keywords used for IEEE VIS papers during the past 10 years, aiming at more informed and, hence, more effective keyword selections for future visualization publications. ","bibtex":"@article{DBLP:journals/corr/Isenberg0SCM14,\r\n  author    = {Petra Isenberg and\r\n               Tobias Isenberg and\r\n               Michael Sedlmair and\r\n               Jian Chen and\r\n               Torsten M{\\\"{o}}ller},\r\n  title     = {Toward a deeper understanding of Visualization through keyword analysis},\r\n  journal   = {CoRR},\r\n  volume    = {abs/1408.3297},\r\n  year      = {2014},\r\n  url       = {http://arxiv.org/abs/1408.3297},\r\n  eprinttype = {arXiv},\r\n  eprint    = {1408.3297},\r\n  timestamp = {Thu, 05 Aug 2021 16:55:04 +0200},\r\n  biburl    = {https://dblp.org/rec/journals/corr/Isenberg0SCM14.bib},\r\n  bibsource = {dblp computer science bibliography, https://dblp.org}\r\n}","notes":"","funding":""},{"Title":"The Nested Blocks and Guidelines Model","Submission Target":"Information Visualization","Date":"2013-12-10","Type":"","First Author":"Miriah Meyer","Other Authors":"Michael Sedlmair, P Samuel Quinan, Tamara Munzner","Key (e.g. for file names)":"meyer2013nbgm","Publisher URL (official)":"https://doi.org/10.1177%2F1473871613510429","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"http://www.cs.ubc.ca/labs/imager/tr/2013/NBGM/","Acknowledgements":"","Abstract":"We propose the nested blocks and guidelines model for the design and validation of visualization systems. The nested blocks and guidelines model extends the previously proposed four-level nested model by adding finer grained structure within each level, providing explicit mechanisms to capture and discuss design decision rationale. Blocks are the outcomes of the design process at a specific level, and guidelines discuss relationships between these blocks. Blocks at the algorithm and technique levels describe design choices, as do data blocks at the abstraction level, whereas task abstraction blocks and domain situation blocks are identified as the outcome of the designer’s understanding of the requirements. In the nested blocks and guidelines model, there are two types of guidelines: within-level guidelines provide comparisons for blocks within the same level, while between-level guidelines provide mappings between adjacent levels of design. We analyze several recent articles using the nested blocks and guidelines model to provide concrete examples of how a researcher can use blocks and guidelines to describe and evaluate visualization research. We also discuss the nested blocks and guidelines model with respect to other design models to clarify its role in visualization design. Using the nested blocks and guidelines model, we pinpoint two implications for visualization evaluation. First, comparison of blocks at the domain level must occur implicitly downstream at the abstraction level; second, comparison between blocks must take into account both upstream assumptions and downstream requirements. Finally, we use the model to analyze two open problems: the need for mid-level task taxonomies to fill in the task blocks at the abstraction level and the need for more guidelines mapping between the algorithm and technique levels.","bibtex":"@article{meyer2015nested,\r\n  title={The nested blocks and guidelines model},\r\n  author={Meyer, Miriah and Sedlmair, Michael and Quinan, P Samuel and Munzner, Tamara},\r\n  journal={Information Visualization},\r\n  volume={14},\r\n  number={3},\r\n  pages={234--249},\r\n  year={2015},\r\n  publisher={SAGE Publications Sage UK: London, England}\r\n}\r\n","notes":"","funding":""},{"Title":"Empirical Guidance on Scatterplot and Dimension Reduction Technique Choices","Submission Target":"TVCG","Date":"2013-10-16","Type":"","First Author":"Michael Sedlmair","Other Authors":"Tamara Munzner, Melanie Tory","Key (e.g. for file names)":"sedlmair2013infovis","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2013.153","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"http://www.cs.ubc.ca/labs/imager/tr/2013/ScatterplotEval/","Acknowledgements":"","Abstract":"To verify cluster separation in high-dimensional data, analysts often reduce the data with a dimension reduction (DR) technique, and then visualize it with 2D Scatterplots, interactive 3D Scatterplots, or Scatterplot Matrices (SPLOMs). With the goal of providing guidance between these visual encoding choices, we conducted an empirical data study in which two human coders manually inspected a broad set of 816 scatterplots derived from 75 datasets, 4 DR techniques, and the 3 previously mentioned scatterplot techniques. Each coder scored all color-coded classes in each scatterplot in terms of their separability from other classes. We analyze the resulting quantitative data with a heatmap approach, and qualitatively discuss interesting scatterplot examples. Our findings reveal that 2D scatterplots are often 'good enough', that is, neither SPLOM nor interactive 3D adds notably more cluster separability with the chosen DR technique. If 2D is not good enough, the most promising approach is to use an alternative DR technique in 2D. Beyond that, SPLOM occasionally adds additional value, and interactive 3D rarely helps but often hurts in terms of poorer class separation and usability. We summarize these results as a workflow model and implications for design. Our results offer guidance to analysts during the DR exploration process.","bibtex":"@ARTICLE{6634128,\r\n  author={Sedlmair, Michael and Munzner, Tamara and Tory, Melanie},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Empirical Guidance on Scatterplot and Dimension Reduction Technique Choices}, \r\n  year={2013},\r\n  volume={19},\r\n  number={12},\r\n  pages={2634-2643},\r\n  doi={10.1109/TVCG.2013.153}}\r\n","notes":"","funding":""},{"Title":"A Systematic Review on the Practice of Evaluating Visualization","Submission Target":"TVCG","Date":"2013-10-06","Type":"","First Author":"Tobias Isenberg","Other Authors":"Petra Isenberg, Jian Chen, Michael Sedlmair, Torsten Möller","Key (e.g. for file names)":"isenberg2013srp","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2013.126","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"http://tobias.isenberg.cc/VideosAndDemos/Isenberg2013SRP","Acknowledgements":"","Abstract":"We present an assessment of the state and historic development of evaluation practices as reported in papers published at the IEEE Visualization conference. Our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations. For this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by Lam et al. [2012]. The results of our review include an overview of the most common evaluation goals in the community, how they evolved over time, and how they contrast or align to those of the IEEE Information Visualization conference. In particular, we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent (with consistently 80-90% of all papers since 1997). However, especially over the last six years there is a steady increase in evaluation methods that include participants, either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools. Up to 2010, this trend in the IEEE Visualization conference was much more pronounced than in the IEEE Information Visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing. Since 2011, however, also papers in IEEE Information Visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools. Further, we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity.","bibtex":"@ARTICLE{6634108,\r\n  author={Isenberg, Tobias and Isenberg, Petra and Chen, Jian and Sedlmair, Michael and Möller, Torsten},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={A Systematic Review on the Practice of Evaluating Visualization}, \r\n  year={2013},\r\n  volume={19},\r\n  number={12},\r\n  pages={2818-2827},\r\n  doi={10.1109/TVCG.2013.126}}\r\n","notes":"","funding":""},{"Title":"ParaGlide: Interactive Parameter Space Partitioning for Computer Simulations","Submission Target":"TVCG","Date":"2013-03-07","Type":"","First Author":"Steven Bergner","Other Authors":"Michael Sedlmair, Torsten Möller, Sareh Nabi Abdolyousefi, Ahmed Saad","Key (e.g. for file names)":"bergner2013tvcg","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2013.61","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"In this paper, we introduce ParaGlide, a visualization system designed for interactive exploration of parameter spaces of multidimensional simulation models. To get the right parameter configuration, model developers frequently have to go back and forth between setting input parameters and qualitatively judging the outcomes of their model. Current state-of-the-art tools and practices, however, fail to provide a systematic way of exploring these parameter spaces, making informed decisions about parameter configurations a tedious and workload-intensive task. ParaGlide endeavors to overcome this shortcoming by guiding data generation using a region-based user interface for parameter sampling and then dividing the model's input parameter space into partitions that represent distinct output behavior. In particular, we found that parameter space partitioning can help model developers to better understand qualitative differences among possibly high-dimensional model outputs. Further, it provides information on parameter sensitivity and facilitates comparison of models. We developed ParaGlide in close collaboration with experts from three different domains, who all were involved in developing new models for their domain. We first analyzed current practices of six domain experts and derived a set of tasks and design requirements, then engaged in a user-centered design process, and finally conducted three longitudinal in-depth case studies underlining the usefulness of our approach.","bibtex":"@ARTICLE{6472235,\r\n  author={Bergner, Steven and Sedlmair, Michael and Moller, Torsten and Abdolyousefi, Sareh Nabi and Saad, Ahmed},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={ParaGlide: Interactive Parameter Space Partitioning for Computer Simulations}, \r\n  year={2013},\r\n  volume={19},\r\n  number={9},\r\n  pages={1499-1512},\r\n  doi={10.1109/TVCG.2013.61}}\r\n","notes":"","funding":""},{"Title":"The Four-Level Nested Model Revisited: Blocks and Guidelines","Submission Target":"CHI BELIV","Date":"2012-10-14","Type":"Workshop Paper","First Author":"Miriah Meyer","Other Authors":"Michael Sedlmair, Tamara Munzner","Key (e.g. for file names)":"meyer2012beliv","Publisher URL (official)":"https://doi.org/10.1145/2442576.2442587","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"http://www.cs.ubc.ca/labs/imager/tr/2012/NestedModelExt/","Acknowledgements":"","Abstract":"We propose an extension to the four-level nested model of design and validation of visualization system that defines the term \"guidelines\" in terms of blocks at each level. Blocks are the outcomes of the design process at a specific level, and guidelines discuss relationships between these blocks. Within-level guidelines provide comparisons for blocks within the same level, while between-level guidelines provide mappings between adjacent levels of design. These guidelines help a designer choose which abstractions, techniques, and algorithms are reasonable to combine when building a visualization system. This definition of guideline allows analysis of how the validation efforts in different kinds of papers typically lead to different kinds of guidelines. Analysis through the lens of blocks and guidelines also led us to identify four major needs: a definition of the meaning of block at the problem level; mid-level task taxonomies to fill in the blocks at the abstraction level; refinement of the model itself at the abstraction level; and a more complete set of mappings up from the algorithm level to the technique level. These gaps in visualization knowledge present rich opportunities for future work.","bibtex":"@inproceedings{10.1145/2442576.2442587,\r\nauthor = {Meyer, Miriah and Sedlmair, Michael and Munzner, Tamara},\r\ntitle = {The Four-Level Nested Model Revisited: Blocks and Guidelines},\r\nyear = {2012},\r\nisbn = {9781450317917},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/2442576.2442587},\r\ndoi = {10.1145/2442576.2442587},\r\nabstract = {We propose an extension to the four-level nested model of design and validation of visualization system that defines the term \"guidelines\" in terms of blocks at each level. Blocks are the outcomes of the design process at a specific level, and guidelines discuss relationships between these blocks. Within-level guidelines provide comparisons for blocks within the same level, while between-level guidelines provide mappings between adjacent levels of design. These guidelines help a designer choose which abstractions, techniques, and algorithms are reasonable to combine when building a visualization system. This definition of guideline allows analysis of how the validation efforts in different kinds of papers typically lead to different kinds of guidelines. Analysis through the lens of blocks and guidelines also led us to identify four major needs: a definition of the meaning of block at the problem level; mid-level task taxonomies to fill in the blocks at the abstraction level; refinement of the model itself at the abstraction level; and a more complete set of mappings up from the algorithm level to the technique level. These gaps in visualization knowledge present rich opportunities for future work.},\r\nbooktitle = {Proceedings of the 2012 BELIV Workshop: Beyond Time and Errors - Novel Evaluation Methods for Visualization},\r\narticleno = {11},\r\nnumpages = {6},\r\nkeywords = {visualization, design studies, nested model, validation},\r\nlocation = {Seattle, Washington, USA},\r\nseries = {BELIV '12}\r\n}","notes":"","funding":""},{"Title":"Design Study Methodology: Reflections from the Trenches and the Stacks","Submission Target":"TVCG","Date":"2012-10-09","Type":"","First Author":"Michael Sedlmair","Other Authors":"Miriah Meyer, Tamara Munzner","Key (e.g. for file names)":"sedlmair2012infovis-1","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2012.213","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"http://www.cs.ubc.ca/labs/imager/tr/2012/dsm/","Acknowledgements":"","Abstract":"Design studies are an increasingly popular form of problem-driven visualization research, yet there is little guidance available about how to do them effectively. In this paper we reflect on our combined experience of conducting twenty-one design studies, as well as reading and reviewing many more, and on an extensive literature review of other field work methods and methodologies. Based on this foundation we provide definitions, propose a methodological framework, and provide practical guidance for conducting design studies. We define a design study as a project in which visualization researchers analyze a specific real-world problem faced by domain experts, design a visualization system that supports solving this problem, validate the design, and reflect about lessons learned in order to refine visualization design guidelines. We characterize two axes - a task clarity axis from fuzzy to crisp and an information location axis from the domain expert's head to the computer - and use these axes to reason about design study contributions, their suitability, and uniqueness from other approaches. The proposed methodological framework consists of 9 stages: learn, winnow, cast, discover, design, implement, deploy, reflect, and write. For each stage we provide practical guidance and outline potential pitfalls. We also conducted an extensive literature survey of related methodological approaches that involve a significant amount of qualitative field work, and compare design study methodology to that of ethnography, grounded theory, and action research.","bibtex":"@ARTICLE{6327248,\r\n  author={Sedlmair, Michael and Meyer, Miriah and Munzner, Tamara},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={Design Study Methodology: Reflections from the Trenches and the Stacks}, \r\n  year={2012},\r\n  volume={18},\r\n  number={12},\r\n  pages={2431-2440},\r\n  doi={10.1109/TVCG.2012.213}}\r\n","notes":"Honorable Mention for Best Paper Award","funding":""},{"Title":"RelEx: Visualization for Actively Changing Overlay Network Specifications","Submission Target":"TVCG","Date":"2012-10-09","Type":"","First Author":"Michael Sedlmair","Other Authors":"Annika Frank, Tamara Munzner, Andreas Butz","Key (e.g. for file names)":"sedlmair2012infovis-2","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2012.255","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"http://www.cs.ubc.ca/labs/imager/tr/2012/RelEx/","Acknowledgements":"","Abstract":"We present a network visualization design study focused on supporting automotive engineers who need to specify and optimize traffic patterns for in-car communication networks. The task and data abstractions that we derived support actively making changes to an overlay network, where logical communication specifications must be mapped to an underlying physical network. These abstractions are very different from the dominant use case in visual network analysis, namely identifying clusters and central nodes, that stems from the domain of social network analysis. Our visualization tool RelEx was created and iteratively refined through a full user-centered design process that included a full problem characterization phase before tool design began, paper prototyping, iterative refinement in close collaboration with expert users for formative evaluation, deployment in the field with real analysts using their own data, usability testing with non-expert users, and summative evaluation at the end of the deployment. In the summative post-deployment study, which entailed domain experts using the tool over several weeks in their daily practice, we documented many examples where the use of RelEx simplified or sped up their work compared to previous practices.","bibtex":"@ARTICLE{6327279,\r\n  author={Sedlmair, Michael and Frank, Annika and Munzner, Tamara and Butz, Andreas},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={RelEx: Visualization for Actively Changing Overlay Network Specifications}, \r\n  year={2012},\r\n  volume={18},\r\n  number={12},\r\n  pages={2729-2738},\r\n  doi={10.1109/TVCG.2012.255}}\r\n","notes":"","funding":""},{"Title":"Dimensionality Reduction in the Wild: Gaps and Guidance","Submission Target":"UBC Computer Science Technical Report TR-2012-03","Date":"2012-06-26","Type":"Technical Report","First Author":"Michael Sedlmair","Other Authors":"Matt Brehmer, Stephen Ingram, Tamara Munzner","Key (e.g. for file names)":"sedlmair2012tr","Publisher URL (official)":"https://www.cs.ubc.ca/tr/2012/tr-2012-03","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Despite an abundance of technical literature on dimension reduction (DR), our understanding of how real data analysts are using DR techniques and what problems they face remains largely incomplete. In this paper, we contribute the first systematic and broad analysis of DR usage by a sample of real data analysts, along with their needs and problems. We present the results of a two-year qualitative research endeavor, in which we iteratively collected and analyzed a rich corpus of data in the spirit of grounded theory. We interviewed 24 data analysts from different domains and surveyed papers depicting applications of DR. The result is a descriptive taxonomy of DR usage, and concrete real-world usage examples summarized in terms of this taxonomy. We also identify seven gaps where user DR needs are unfulfilled by currently available techniques, and three mismatches where the users do not need offered techniques. At the heart of our taxonomy is a task classification that differentiates between abstract tasks related to point clusters and those related to dimensions. The taxonomy and usage examples are intended to provide a better descriptive understanding of real data analysts practices and needs with regards to DR. The gaps are intended as prescriptive pointers to future research directions, with the most important gaps being a lack of support for users without expertise in the mathematics of DR, and an absence of DR techniques for comparing explicit groups of dimensions or for relating non-linear embeddings to original dimensions.","bibtex":"@article{sedlmair2012dimensionality,\r\n  title={Dimensionality reduction in the wild: Gaps and guidance},\r\n  author={Sedlmair, M and Brehmer, Matt and Ingram, S and Munzner, T},\r\n  journal={Dept. Comput. Sci., Univ. British Columbia, Vancouver, BC, Canada, Tech. Rep. TR-2012-03},\r\n  year={2012}\r\n}","notes":"","funding":""},{"Title":"A Taxonomy of Visual Cluster Separation Factors","Submission Target":"CGF","Date":"2012-06-25","Type":"","First Author":"Michael Sedlmair","Other Authors":"Andrada Tatu, Tamara Munzner, Melanie Tory","Key (e.g. for file names)":"sedlmair2012eurovis","Publisher URL (official)":"https://doi.org/10.1111/j.1467-8659.2012.03125.x","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"http://www.cs.ubc.ca/labs/imager/tr/2012/VisClusterSep/","Acknowledgements":"","Abstract":"We provide two contributions, a taxonomy of visual cluster separation factors in scatterplots, and an in-depth qualitative evaluation of two recently proposed and validated separation measures. We initially intended to use these measures to provide guidance for the use of dimension reduction (DR) techniques and visual encoding (VE) choices, but found that they failed to produce reliable results. To understand why, we conducted a systematic qualitative data study covering a broad collection of 75 real and synthetic high-dimensional datasets, four DR techniques, and three scatterplot-based visual encodings. Two authors visually inspected over 800 plots to determine whether or not the measures created plausible results. We found that they failed in over half the cases overall, and in over two-thirds of the cases involving real datasets. Using open and axial coding of failure reasons and separability characteristics, we generated a taxonomy of visual cluster separability factors. We iteratively refined its explanatory clarity and power by mapping the studied datasets and success and failure ranges of the measures onto the factor axes. Our taxonomy has four categories, ordered by their ability to influence successors: Scale, Point Distance, Shape, and Position. Each category is split into Within-Cluster factors such as density, curvature, isotropy, and clumpiness, and Between-Cluster factors that arise from the variance of these properties, culminating in the overarching factor of class separation. The resulting taxonomy can be used to guide the design and the evaluation of cluster separation measures.","bibtex":"@article{https://doi.org/10.1111/j.1467-8659.2012.03125.x,\r\nauthor = {Sedlmair, M. and Tatu, A. and Munzner, T. and Tory, M.},\r\ntitle = {A Taxonomy of Visual Cluster Separation Factors},\r\njournal = {Computer Graphics Forum},\r\nvolume = {31},\r\nnumber = {3pt4},\r\npages = {1335-1344},\r\nkeywords = {H.5.0 Information Interfaces and Presentation: General, J.0 Computer Applications: General},\r\ndoi = {https://doi.org/10.1111/j.1467-8659.2012.03125.x},\r\nurl = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2012.03125.x},\r\neprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8659.2012.03125.x},\r\nabstract = {Abstract We provide two contributions, a taxonomy of visual cluster separation factors in scatterplots, and an in-depth qualitative evaluation of two recently proposed and validated separation measures. We initially intended to use these measures to provide guidance for the use of dimension reduction (DR) techniques and visual encoding (VE) choices, but found that they failed to produce reliable results. To understand why, we conducted a systematic qualitative data study covering a broad collection of 75 real and synthetic high-dimensional datasets, four DR techniques, and three scatterplot-based visual encodings. Two authors visually inspected over 800 plots to determine whether or not the measures created plausible results. We found that they failed in over half the cases overall, and in over two-thirds of the cases involving real datasets. Using open and axial coding of failure reasons and separability characteristics, we generated a taxonomy of visual cluster separability factors. We iteratively refined its explanatory clarity and power by mapping the studied datasets and success and failure ranges of the measures onto the factor axes. Our taxonomy has four categories, ordered by their ability to influence successors: Scale, Point Distance, Shape, and Position. Each category is split into Within-Cluster factors such as density, curvature, isotropy, and clumpiness, and Between-Cluster factors that arise from the variance of these properties, culminating in the overarching factor of class separation. The resulting taxonomy can be used to guide the design and the evaluation of cluster separation measures.},\r\nyear = {2012}\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Visualisierung von Busdaten – Fehleranalyse im Forschungsprojekt AutobahnVis","Submission Target":"Journal of Automobil Elektronik","Date":"2011-10-10","Type":"","First Author":"Michael Sedlmair","Other Authors":"Michael Schraut, Wolfgang Hintermaier","Key (e.g. for file names)":"sedlmair2011auto","Publisher URL (official)":"https://www.all-electronics.de/automotive-transportation/visualisierung-von-busdaten.html","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Wie können Methoden der Informationsvisualisierung die explorative Analyse von Busdaten verbessern? Ein Forschungsprojekt mit dem Namen „AutobahnVis“ zeigt beispielhaft, wie Visualisierung neue Einsichten in komplexe Zusammenhänge ermöglicht und zur Fehleranalyse von Busaufzeichnungen beiträgt.","bibtex":"","notes":"","funding":""},{"Title":"Information Visualization Evaluation in Large Companies: Challenges, Experiences and Recommendations","Submission Target":"Information Visualization","Date":"2011-07-20","Type":"","First Author":"Michael Sedlmair","Other Authors":"Petra Isenberg, Dominikus Baur, Andreas Butz","Key (e.g. for file names)":"sedlmair2011ivs","Publisher URL (official)":"https://doi.org/10.1177%2F1473871611413099","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We examine the implications of evaluating data analysis processes and information visualization tools in a large company setting. While several researchers have addressed the difficulties of evaluating information visualizations with regards to changing data, tasks, and visual encodings, considerably less work has been published on the difficulties of evaluation within specific work contexts. We specifically focus on the challenges, which arise in the context of large companies with several thousand employees. Based on our own experience from a 3.5-year collaboration within a large automotive company, we first present a collection of nine information visualization evaluation challenges. We then discuss these challenges by means of two concrete visualization case studies from our own work. We finally derive a set of 16 recommendations for planning and conducting evaluations in large company settings. The set of challenges and recommendations and the discussion of our experience are meant to provide practical guidance to other researchers and practitioners, who plan to study information visualization in large company settings.","bibtex":"@article{sedlmair2011information,\r\n  title={Information visualization evaluation in large companies: Challenges, experiences and recommendations},\r\n  author={Sedlmair, Michael and Isenberg, Petra and Baur, Dominikus and Butz, Andreas},\r\n  journal={Information Visualization},\r\n  volume={10},\r\n  number={3},\r\n  pages={248--266},\r\n  year={2011},\r\n  publisher={Sage Publications Sage UK: London, England}\r\n}","notes":"","funding":""},{"Title":"Cardiogram: Visual Analytics for Automotive Engineers","Submission Target":"CHI","Date":"2011-05-07","Type":"","First Author":"Michael Sedlmair","Other Authors":"Petra Isenberg, Dominikus Baur, Michael Mauerer, Christian Pigorsch, Andreas Butz","Key (e.g. for file names)":"sedlmair2011chi","Publisher URL (official)":"https://doi.org/10.1145/1978942.1979194","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We present Cardiogram, a visual analytics system that supports automotive engineers in debugging masses of traces each consisting of millions of recorded messages from in-car communication networks. With their increasing complexity, ensuring these safety-critical networks to be error-free has become a major task and challenge for automotive engineers. To overcome shortcomings of current analysis tools, Cardiogram combines visualization techniques with a data preprocessing approach to automatically reduce complexity based on engineers' domain knowledge. In this paper, we provide the findings from an exploratory, three-year field study within a large automotive company, studying current practices of engineers, the challenges they meet and the characteristics for integrating novel visual analytics tools into their work practices.\r\n\r\nWe then introduce Cardiogram, discuss how our field analysis influenced our design decisions, and present a qualitative, long-term, in-depth evaluation. Results of this study showed that our participants successfully used Cardiogram to increase the amount of analyzable information, to externalize domain knowledge, and to provide new insights into trace data. Our design approach finally led to the adoption of Cardiogram into engineers' daily practices.","bibtex":"@inproceedings{10.1145/1978942.1979194,\r\nauthor = {Sedlmair, Michael and Isenberg, Petra and Baur, Dominikus and Mauerer, Michael and Pigorsch, Christian and Butz, Andreas},\r\ntitle = {Cardiogram: Visual Analytics for Automotive Engineers},\r\nyear = {2011},\r\nisbn = {9781450302289},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/1978942.1979194},\r\ndoi = {10.1145/1978942.1979194},\r\nabstract = {We present Cardiogram, a visual analytics system that supports automotive engineers in debugging masses of traces each consisting of millions of recorded messages from in-car communication networks. With their increasing complexity, ensuring these safety-critical networks to be error-free has become a major task and challenge for automotive engineers. To overcome shortcomings of current analysis tools, Cardiogram combines visualization techniques with a data preprocessing approach to automatically reduce complexity based on engineers' domain knowledge. In this paper, we provide the findings from an exploratory, three-year field study within a large automotive company, studying current practices of engineers, the challenges they meet and the characteristics for integrating novel visual analytics tools into their work practices.We then introduce Cardiogram, discuss how our field analysis influenced our design decisions, and present a qualitative, long-term, in-depth evaluation. Results of this study showed that our participants successfully used Cardiogram to increase the amount of analyzable information, to externalize domain knowledge, and to provide new insights into trace data. Our design approach finally led to the adoption of Cardiogram into engineers' daily practices.},\r\nbooktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\r\npages = {1727–1736},\r\nnumpages = {10},\r\nkeywords = {automotive, visual analytics, visualization, in-car communication, field study},\r\nlocation = {Vancouver, BC, Canada},\r\nseries = {CHI '11}\r\n}","notes":"","funding":""},{"Title":"The Streams of Our Lives: Visualizing Listening Histories in Context ","Submission Target":"TVCG","Date":"2010-10-28","Type":"","First Author":"Dominikus Baur","Other Authors":"Frederik Seiffert, Michael Sedlmair, Sebastian Boring","Key (e.g. for file names)":"baur2010infovis","Publisher URL (official)":"https://doi.org/10.1109/TVCG.2010.206","url2":"","PDF URL (public)":"","Video":"https://vimeo.com/9495577","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The choices we take when listening to music are expressions of our personal taste and character. Storing and accessing our listening histories is trivial due to services like Last.fm, but learning from them and understanding them is not. Existing solutions operate at a very abstract level and only produce statistics. By applying techniques from information visualization to this problem, we were able to provide average people with a detailed and powerful tool for accessing their own musical past. LastHistory is an interactive visualization for displaying music listening histories, along with contextual information from personal photos and calendar entries. Its two main user tasks are (1) analysis, with an emphasis on temporal patterns and hypotheses related to musical genre and sequences, and (2) reminiscing, where listening histories and context represent part of one's past. In this design study paper we give an overview of the field of music listening histories and explain their unique characteristics as a type of personal data. We then describe the design rationale, data and view transformations of LastHistory and present the results from both a laband a large-scale online study. We also put listening histories in contrast to other lifelogging data. The resonant and enthusiastic feedback that we received from average users shows a need for making their personal data accessible. We hope to stimulate such developments through this research.","bibtex":"@ARTICLE{5613450,\r\n  author={Baur, Dominikus and Seiffert, Frederik and Sedlmair, Michael and Boring, Sebastian},\r\n  journal={IEEE Transactions on Visualization and Computer Graphics}, \r\n  title={The Streams of Our Lives: Visualizing Listening Histories in Context}, \r\n  year={2010},\r\n  volume={16},\r\n  number={6},\r\n  pages={1119-1128},\r\n  doi={10.1109/TVCG.2010.206}}\r\n","notes":"","funding":""},{"Title":"Visual Analysis of In-Car Communication Networks","Submission Target":"University of Munich","Date":"2010-10-10","Type":"PhD Thesis","First Author":"Michael Sedlmair","Other Authors":"","Key (e.g. for file names)":"sedlmair2010dissertation","Publisher URL (official)":"https://doi.org/10.5282/edoc.12448","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Analyzing, understanding and working with complex systems and large datasets has become a familiar challenge in the information era. The explosion of data worldwide affects nearly every part of society, particularly the science, engineering, health, and financial domains. Looking, for instance at the automotive industry, engineers are confronted with the enormously increased complexity of vehicle electronics. Over the years, a large number of advanced functions, such as ACC (adaptive cruise control), rear seat entertainment systems or automatic start/stop engines, has been integrated into the vehicle. Thereby, the functions have been more and more distributed over the vehicle, leading to the introduction of several communication networks. Overlooking all relevant data facets, understanding dependencies, analyzing the flow of messages and tracking down problems in these networks has become a major challenge for automotive engineers.  \r\n\r\nPromising approaches to overcome information overload and to provide insight into complex data are Information Visualization (InfoVis) and Visual Analytics (VA). Over the last decades, these research communities spent much effort on developing new methods to help users obtain insight into complex data. However, few of these solutions have yet reached end users, and moving research into practice remains one of the great challenges in visual data analysis. This situation is particularly true for large company settings, where very little is known about additional challenges, obstacles and requirements in InfoVis/VA development and evaluation. Users have to be better integrated into our research processes in terms of adequate requirements analysis, understanding practices and challenges, developing well-directed, user-centered technologies and evaluating their value within a realistic context.  \r\n\r\nThis dissertation explores a novel InfoVis/VA application area, namely in-car communication networks, and demonstrates how information visualization methods and techniques can help engineers to work with and better understand these networks. Based on a three-year internship with a large automotive company and the close cooperation with domain experts, I grounded a profound understanding of specific challenges, requirements and obstacles for InfoVis/VA application in this area and learned that “designing with not for the people” is highly important for successful solutions. The three main contributions of this dissertation are: (1) An empirical analysis of current working practices of automotive engineers and the derivation of specific design requirements for InfoVis/VA tools; (2) the successful application and evaluation of nine prototypes, including the deployment of five systems; and (3) based on the three-year experience, a set of recommendations for developing and evaluating InfoVis systems in large company settings.\r\n\r\nI present ethnographic studies with more than 150 automotive engineers. These studies helped us to understand currently used tools, the underlying data, tasks as well as user groups and to categorize the field into application sub-domains. Based on these findings, we propose implications and recommendations for designing tools to support current practices of automotive network engineers with InfoVis/VA technologies. I also present nine InfoVis design studies that we built and evaluated with automotive domain experts and use them to systematically explore the design space of applying InfoVis to in-car communication networks. Each prototype was developed in a user-centered, participatory process, respectively with a focus on a specific sub-domain of target users with specific data and tasks. Experimental results from studies with real users are presented, that show that the visualization prototypes can improve the engineers’ work in terms of working efficiency, better understanding and novel insights. Based on lessons learned from repeatedly designing and evaluating our tools together with domain experts at a large automotive company, I discuss challenges and present recommendations for deploying and evaluating VA/InfoVis tools in large company settings. I hope that these recommendations can guide other InfoVis researchers and practitioners in similar projects by providing them with new insights, such as the necessity for close integration with current tools and given processes, distributed knowledge and high degree of specialization, and the importance of addressing prevailing mental models and time restrictions. In general, I think that large company settings are a promising and fruitful field for novel InfoVis applications and expect our recommendations to be useful tools for other researchers and tool designers.","bibtex":"@thesis{ediss12448,\r\n           month = {October},\r\n           title = {Visual Analysis of In-Car Communication Networks},\r\n            year = {2010},\r\n          author = {Michael Sedlmair},\r\n       publisher = {Ludwig-Maximilians-Universit{\\\"a}t M{\\\"u}nchen},\r\n             url = {http://nbn-resolving.de/urn:nbn:de:bvb:19-124488},\r\n        keywords = {Visualization, Visual Analytics, Automotive, In-Car Networks}\r\n}\r\n","notes":"","funding":""},{"Title":"Evaluating Information Visualization in Large Companies: Challenges, Experiences and Recommendations ","Submission Target":"CHI BELIV","Date":"2010-04-10","Type":"Workshop Paper","First Author":"Michael Sedlmair","Other Authors":"Petra Isenberg, Dominikus Baur, Andreas Butz","Key (e.g. for file names)":"sedlmair2010beliv","Publisher URL (official)":"https://doi.org/10.1145/2110192.2110204","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"We examine the process and some implications of evaluating information visualization in a large company setting. While several researchers have addressed the difficulties of evaluating information visualizations with regards to changing data, tasks, and visual encodings, considerably less work has been published on the difficulties of evaluation within specific work contexts. In this paper, we specifically focus on the challenges arising in the context of large companies with several thousand employees. We present a collection of evaluation challenges, discuss our own experiences conducting information visualization evaluation within the context of a large automotive company, and present a set of recommendations derived from our experiences. The set of challenges and recommendations can aid researchers and practitioners in preparing and conducting evaluations of their products within a large company setting.","bibtex":"@inproceedings{10.1145/2110192.2110204,\r\nauthor = {Sedlmair, Michael and Isenberg, Petra and Baur, Dominikus and Butz, Andreas},\r\ntitle = {Evaluating Information Visualization in Large Companies: Challenges, Experiences and Recommendations},\r\nyear = {2010},\r\nisbn = {9781450300070},\r\npublisher = {Association for Computing Machinery},\r\naddress = {New York, NY, USA},\r\nurl = {https://doi.org/10.1145/2110192.2110204},\r\ndoi = {10.1145/2110192.2110204},\r\nabstract = {We examine the process and some implications of evaluating information visualization in a large company setting. While several researchers have addressed the difficulties of evaluating information visualizations with regards to changing data, tasks, and visual encodings, considerably less work has been published on the difficulties of evaluation within specific work contexts. In this paper, we specifically focus on the challenges arising in the context of large companies with several thousand employees. We present a collection of evaluation challenges, discuss our own experiences conducting information visualization evaluation within the context of a large automotive company, and present a set of recommendations derived from our experiences. The set of challenges and recommendations can aid researchers and practitioners in preparing and conducting evaluations of their products within a large company setting.},\r\nbooktitle = {Proceedings of the 3rd BELIV'10 Workshop: BEyond Time and Errors: Novel EvaLuation Methods for Information Visualization},\r\npages = {79–86},\r\nnumpages = {8},\r\nkeywords = {information visualization, company setting, evaluation},\r\nlocation = {Atlanta, Georgia},\r\nseries = {BELIV '10}\r\n}","notes":"Best Paper Award","funding":""},{"Title":"Trends in Information Visualization","Submission Target":"LMU-MI-2010-1: Media Informatics Advanced Seminar","Date":"2010-04-01","Type":"Technical Report","First Author":"Dominikus Baur","Other Authors":"Michael Sedlmair, Raphael Wimmer, Yaxi Chen, Sara Streng, Sebastian Boring, Alexander De Luca, Andreas Butz","Key (e.g. for file names)":"baur2010infovisHS","Publisher URL (official)":"","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"This report provides an overview of current applications and research trends in the field of\r\ninformation visualization. The content ranges from classical information visualization aspects\r\nsuch as network visualization, multivariate data representation and multiple coordinated\r\nviews to topics beyond the traditional scope such as aesthetics, collaboration or casual\r\naspects in information visualization.","bibtex":"@article{baurtrends,\r\n  title={Trends in Information Visualization},\r\n  author={Baur, Dominikus and Sedlmair, Michael and Wimmer, Raphael and Chen, Ya-Xi and Streng, Sara and Boring, Sebastian and De Luca, Alexander and Butz, Andreas}\r\n}","notes":"","funding":""},{"Title":"Collaborative Visualization on Interactive Surfaces - CoVIS '09","Submission Target":"CoVIS '09, Workshop at Visweek","Date":"2010-04-01","Type":"Technical Report","First Author":"Petra Isenberg","Other Authors":"Michael Sedlmair, Dominikus Baur, Tobias Isenberg, Andreas Butz","Key (e.g. for file names)":"isenberg2010covis","Publisher URL (official)":"","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The report comprises interdisciplinary aspects form the fields of information visualization,\r\nscientific visualization, visual analytics as well as CSCW and HCI. It is meant to help other\r\nresearchers better understand the role and the growing impact of interactive surfaces as an\r\nemerging technology for supporting collaborative visualization and visual analytics settings.","bibtex":"@article{isenbergcollaborative,\r\n  title={Collaborative Visualization on Interactive Surfaces-CoVIS'09},\r\n  author={Isenberg, Petra and Sedlmair, Michael and Baur, Dominikus and Isenberg, Tobias and Butz, Andreas}\r\n}\r\n","notes":"","funding":""},{"Title":"MostVis: An Interactive Visualization Supporting Automotive Engineers in MOST Catalog Exploration","Submission Target":"IV","Date":"2009-08-04","Type":"","First Author":"Michael Sedlmair","Other Authors":"Christian Berhold, Daniel Herrscher, Sebastian Boring, Andreas But","Key (e.g. for file names)":"sedlmair2009iv","Publisher URL (official)":"https://doi.org/10.1109/IV.2009.95","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The MOST bus is a current bus technology for connecting multimedia components in cars, such as radios, navigation systems, or media players. The bus functionality is described in a large hierarchically structured catalog of some 4psila000 entries. Browsing this catalog has become infeasible on paper as well as with currently used textual database interfaces. An observation of current work practices has revealed many problems and inefficiencies. We describe the (iteratively developed) design of MostVis, a visual tool for exploring MOST function catalogs, as well as an evaluation of our implemented prototype. Our design carefully adapts existing visualization techniques and combines them in a multiple coordinated view (MCV) approach to satisfy the specific needs of our target group. With this paper, we hope to provide a living example of how existing general-purpose techniques can be successfully trimmed and tailored for a very specific audience.","bibtex":"@INPROCEEDINGS{5190770,\r\n  author={Sedlmair, Michael and Bernhold, Christian and Herrscher, Daniel and Boring, Sebastian and Butz, Andreas},\r\n  booktitle={2009 13th International Conference Information Visualisation}, \r\n  title={MostVis: An Interactive Visualization Supporting Automotive Engineers in MOST Catalog Exploration}, \r\n  year={2009},\r\n  pages={173-182},\r\n  doi={10.1109/IV.2009.95}}\r\n","notes":"","funding":""},{"Title":"LibViz: Data Visualisation of the Old Library ","Submission Target":"International Journal of Architectural Computing","Date":"2009-01-01","Type":"","First Author":"Kerstin Ruhland","Other Authors":"Michael Sedlmair, Susan Bioletti, Carol O'Sullivan","Key (e.g. for file names)":"ruhland2009ijac","Publisher URL (official)":"https://doi.org/10.1260%2F147807709788549402","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"The Old Library of Trinity College Dublin, built in 1732, is an internationally renowned research library. In recent decades it has also become a major tourist attraction in Dublin, with the display of the Book of Kells within the Old Library now drawing over half a million visitors per year. The Preservation and Conservation Department of the Library has raised concerns about the impact of the environment on the collection. The location of the building in the city centre, large visitor numbers, and the conditions within the building are putting the collection at risk. In developing a strategic plan to find solutions to these problems, the department has been assessing and documenting the current situation. This paper introduces ongoing work on a system to visualise the collected data, which includes: dust levels and dispersion, internal and external temperature and relative humidity levels, and visitor numbers in the Old Library. We are developing a user interface for which the data, originally stored in various file formats, is consolidated in a database which can be explored using a 3D virtual reconstruction of the Old Library. With this novel technique, it is also possible to compare and assess the relationships between the various datasets in context.","bibtex":"@article{ruhland2009libviz,\r\n  title={Libviz: Data visualisation of the old library},\r\n  author={Ruhland, Kerstin and O'Sullivan, C and Bioletti, SUSAN and Sedlmair, M},\r\n  journal={International Journal of Architectural Computing},\r\n  volume={7},\r\n  number={1},\r\n  pages={177--192},\r\n  year={2009},\r\n  publisher={SAGE Publications Sage UK: London, England}\r\n}\r\n","notes":"","funding":""},{"Title":"User-centered Development of a Visual Exploration System for In-Car Communication","Submission Target":"SG","Date":"2009-01-01","Type":"","First Author":"Michael Sedlmair","Other Authors":"Benjamin Kunze, Wolfgang Hintermaier, Andreas Butz","Key (e.g. for file names)":"sedlmair2009sg1","Publisher URL (official)":"https://doi.org/10.1007/978-3-642-02115-2_9","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Modern premium automobiles are equipped with an increasing number of Electronic Control Units (ECUs). These ECUs are interconnected and form a complex network to provide a wide range of advanced vehicle functionality. Analyzing the flow of messages in this network and tracking down problems has become a major challenge for automotive engineers. By observing their working practices, we found that the tools they currently use are mostly text-based and largely fail to provide correlations among the enormous amount of data. We established requirements for a more appropriate (visual) tool set. We followed a user-centered approach to design several visualizations for in-car communication processes, each with a clear purpose and application scenario. Then we used low-fidelity prototypes to evaluate our ideas and to identify the “working” designs. Based on this selection, we finally implemented a prototype and conducted an expert evaluation which revealed the emergence of a novel mental model for thinking about and discussing in-car communication processes.","bibtex":"@InProceedings{10.1007/978-3-642-02115-2_9,\r\nauthor=\"Sedlmair, Michael\r\nand Kunze, Benjamin\r\nand Hintermaier, Wolfgang\r\nand Butz, Andreas\",\r\neditor=\"Butz, Andreas\r\nand Fisher, Brian\r\nand Christie, Marc\r\nand Kr{\\\"u}ger, Antonio\r\nand Olivier, Patrick\r\nand Ther{\\'o}n, Roberto\",\r\ntitle=\"User-Centered Development of a Visual Exploration System for In-Car Communication\",\r\nbooktitle=\"Smart Graphics\",\r\nyear=\"2009\",\r\npublisher=\"Springer Berlin Heidelberg\",\r\naddress=\"Berlin, Heidelberg\",\r\npages=\"105--116\",\r\nabstract=\"Modern premium automobiles are equipped with an increasing number of Electronic Control Units (ECUs). These ECUs are interconnected and form a complex network to provide a wide range of advanced vehicle functionality. Analyzing the flow of messages in this network and tracking down problems has become a major challenge for automotive engineers. By observing their working practices, we found that the tools they currently use are mostly text-based and largely fail to provide correlations among the enormous amount of data. We established requirements for a more appropriate (visual) tool set. We followed a user-centered approach to design several visualizations for in-car communication processes, each with a clear purpose and application scenario. Then we used low-fidelity prototypes to evaluate our ideas and to identify the ``working'' designs. Based on this selection, we finally implemented a prototype and conducted an expert evaluation which revealed the emergence of a novel mental model for thinking about and discussing in-car communication processes.\",\r\nisbn=\"978-3-642-02115-2\"\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"Towards the Big Picture: Enriching 3D Models with Information Visualisation and Vice Versa","Submission Target":"SG","Date":"2009-01-01","Type":"","First Author":"Michael Sedlmair","Other Authors":"Kerstin Ruhland, Fabian Hennecke, Andreas Butz, Susan Bioletti, Carol O'Sullivan","Key (e.g. for file names)":"sedlmair2009sg2","Publisher URL (official)":"https://doi.org/10.1007/978-3-642-02115-2_3","url2":"https://link.springer.com/chapter/10.1007/978-3-642-02115-2_3","PDF URL (public)":"http://www.medien.ifi.lmu.de/pubdb/publications/pub/sedlmair2009sg2/sedlmair2009sg2.pdf","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Most information visualisation methods are based on abstract visual representations without any concrete manifestation in the “real world”. However, a variety of abstract datasets can indeed be related to, and hence enriched by, real-world aspects. In these cases an additional virtual representation of the 3D object can help to gain a better insight into the connection between abstract and real-world issues. We demonstrate this approach with two prototype systems that combine information visualisation with 3D models in multiple coordinated views. The first prototype involves the visualisation of in-car communication traces. The 3D model of the car serves as one view among several and provides the user with information about the car’s activities. LibViz, our second prototype, is based on a full screen 3D representation of a library building. Measured data is visualised in overlaid, semi-transparent windows to allow the user interpretation of the data in its spatial context of the library’s 3D model. Based on the two prototypes, we identify the benefits and drawbacks of the approach, investigate aspects of coordination between the 3D model and the abstract visualisations, and discuss principals for a general approach.","bibtex":"@InProceedings{10.1007/978-3-642-02115-2_3,\r\nauthor=\"Sedlmair, Michael\r\nand Ruhland, Kerstin\r\nand Hennecke, Fabian\r\nand Butz, Andreas\r\nand Bioletti, Susan\r\nand O'Sullivan, Carol\",\r\neditor=\"Butz, Andreas\r\nand Fisher, Brian\r\nand Christie, Marc\r\nand Kr{\\\"u}ger, Antonio\r\nand Olivier, Patrick\r\nand Ther{\\'o}n, Roberto\",\r\ntitle=\"Towards the Big Picture: Enriching 3D Models with Information Visualisation and Vice Versa\",\r\nbooktitle=\"Smart Graphics\",\r\nyear=\"2009\",\r\npublisher=\"Springer Berlin Heidelberg\",\r\naddress=\"Berlin, Heidelberg\",\r\npages=\"27--39\",\r\nabstract=\"Most information visualisation methods are based on abstract visual representations without any concrete manifestation in the ``real world''. However, a variety of abstract datasets can indeed be related to, and hence enriched by, real-world aspects. In these cases an additional virtual representation of the 3D object can help to gain a better insight into the connection between abstract and real-world issues. We demonstrate this approach with two prototype systems that combine information visualisation with 3D models in multiple coordinated views. The first prototype involves the visualisation of in-car communication traces. The 3D model of the car serves as one view among several and provides the user with information about the car's activities. LibViz, our second prototype, is based on a full screen 3D representation of a library building. Measured data is visualised in overlaid, semi-transparent windows to allow the user interpretation of the data in its spatial context of the library's 3D model. Based on the two prototypes, we identify the benefits and drawbacks of the approach, investigate aspects of coordination between the 3D model and the abstract visualisations, and discuss principals for a general approach.\",\r\nisbn=\"978-3-642-02115-2\"\r\n}\r\n\r\n","notes":"","funding":""},{"Title":"A Dual-View Visualization of In-Car Communication Processes","Submission Target":"IV","Date":"2008-07-25","Type":"","First Author":"Michael Sedlmair","Other Authors":"Wolfgang Hintermaier, Konrad Stocker, Thortsen Büring, Andreas Butz","Key (e.g. for file names)":"sedlmair2008iv","Publisher URL (official)":"https://doi.org/10.1109/IV.2008.20","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"With the increasing complexity of in-car communication architectures, their diagnostics have become essential for automotive development and maintenance. In order to help engineers to detect and analyze the potential sources and consequences of errors, it is crucial to provide both comprehensive and detailed insight into the communication processes and their contexts. Two important aspects of these are the dependencies and correlations between onboard functions. In this paper we present a dual-view visualization for exploring the functional dependency chains of in-car communication processes. One view presents the dependencies of hardware components using a space filling approach similar to a treemap, whereas the other view displays the functional correlations as an interactive sequence chart. The views are coupled via color coding and show the dependencies of an interactively selectable functional unit. In an expert evaluation, we assessed the benefits of using this visualization technique for in-car communication diagnostics with very positive results.","bibtex":"@INPROCEEDINGS{4577941,\r\n  author={Sedlmair, Michael and Hintermaier, Wolfgang and Stocker, Konrad and Büring, Thorsten and Butz, Andreas},\r\n  booktitle={2008 12th International Conference Information Visualisation}, \r\n  title={A Dual-View Visualization of In-Car Communication Processes}, \r\n  year={2008},\r\n  pages={157-162},\r\n  doi={10.1109/IV.2008.20}}\r\n","notes":"","funding":""},{"Title":"LibViz: Data Visualisation of the Old Library ","Submission Target":"VSMM","Date":"2008-01-01","Type":"","First Author":"Kerstin Ruhland","Other Authors":"Susan Bioletti, Michael Sedlmair, Carol O'Sullivan","Key (e.g. for file names)":"ruhland2008vsmm","Publisher URL (official)":"","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"","bibtex":"","notes":"Best Paper Award","funding":""},{"Title":"Requirements for a MDE System to Support Collaborative In-Car Communication Diagnostics","Submission Target":"International CSCW Workshop Beyond the Laboratory","Date":"2008-01-01","Type":"Workshop Paper","First Author":"Michael Sedlmair","Other Authors":"Dominikus Baur, Sebastian Boring, Petra Isenberg, Marko Jurmu, Andreas Butz","Key (e.g. for file names)":"sedlmair2008cscw","Publisher URL (official)":"","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"","bibtex":"","notes":"","funding":""},{"Title":"MSCar: Enhancing Message Sequence Charts with Interactivity for Analysing (Automotive) Communication Sequences","Submission Target":"Workshop on the Layout of (Software) Engineering Diagrams (LED)","Date":"2008-01-01","Type":"Workshop Paper","First Author":"Michael Sedlmair","Other Authors":"","Key (e.g. for file names)":"sedlmair2008led","Publisher URL (official)":"https://doi.org/10.14279/tuj.eceasst.13.170","url2":"","PDF URL (public)":"","Video":"","Video2":"","Supplemental":"","Acknowledgements":"","Abstract":"Message Sequence Charts (MSCs) are a standardized and widespread form to visually describe interactions in distributed systems. Our approach proposes the enrichment of large scaled MSCs with novel interaction and design techniques used in the field of information visualization. Additionally, we show a graphical solution to visualize parallel, multi-directed communication processes in MSCs. Instead of the common application to specify system behaviours our interactive MSCs are aimed at exploring and diagnosing dependencies in network communication in general and, regarding our special requirements, within in-car communication traces. We implemented a prototype called MSCar with Focus and Context techniques, Dynamic Path Highlighting, Details on Demand and Colour Coding to support the users' cognitive abilities. A qualitative user study on MSCar gave us\r\npreliminary feedback and disclosed potentials of our approach.","bibtex":"@article{sedlmair2008mscar,\r\n  title={MSCar: Enhancing Message Sequence Charts with interactivity for analysing (automotive) communication sequences},\r\n  author={Sedlmair, Michael},\r\n  journal={Electronic Communications of the EASST},\r\n  volume={13},\r\n  year={2008}\r\n}","notes":"","funding":""}]