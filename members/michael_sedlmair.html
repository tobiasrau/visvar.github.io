<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Michael Sedlmair | VISVAR Research Group, University of Stuttgart</title>
    <link rel="stylesheet" href="../style.css">
    <script src="../script.js"></script>
    <link rel="shortcut icon" href="../img/favicon.png">
    <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
</head>
<body>
    <a class="anchor" name="top"></a>
    <main>
        <div>
            
<header>
    <div>
        <a href="https://visvar.github.io/">
            <h1 class="h1desktop">
                <div>
                    VISVAR
                </div>
                <div>
                    Research
                </div>
                <div>
                    Group
                </div>
            </h1>
            <h1 class="h1mobile">
                VISVAR
            </h1>
        </a>
    </div>
    <div>
        <nav>
            <ul>
                <li>
                    <a href="https://visvar.github.io/#aboutus">about VISVAR</a>
                </li>
                <li>
                    <a href="https://visvar.github.io/#publications">publications</a>
                </li>
                <li class="memberNav">
                    <a href="https://visvar.github.io/#members">members</a>
                </li>
                <ul class="memberNav">
                    
                        <li>
                            <a href="https://visvar.github.io/members/aimee_sousa_calepso.html">
                                Aimee Sousa Calepso
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/alexander_achberger.html">
                                Alexander Achberger
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/frank_heyen.html">
                                Frank Heyen
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/jonas_haischt.html">
                                Jonas Haischt
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/katrin_angerbauer.html">
                                Katrin Angerbauer
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/melissa_reinelt.html">
                                Melissa Reinelt
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/michael_sedlmair.html">
                                Michael Sedlmair
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/natalie_hube.html">
                                Natalie Hube
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/quynh_ngo.html">
                                Quynh Ngo
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/rene_cutura.html">
                                Rene Cutura
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/ruben_bauer.html">
                                Ruben Bauer
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/sebastian_rigling.html">
                                Sebastian Rigling
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/simeon_rau.html">
                                Simeon Rau
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/xingyao_yu.html">
                                Xingyao Yu
                            </a>
                        </li>
                    
                </ul>
            </ul>
        </nav>
    </div>
</header>
        </div>
        <div>
            <article> <a class="anchor" name="aboutus"></a>
                <h1>
  Prof. Dr. Michael Sedlmair
</h1>

<div class="aboutMember">

  <div class="avatarAndBio">
    <img class="avatar" src="../img/people/michael_sedlmair.jpg" />

    <div class="bio">
      <p>
        Michael Sedlmair is a professor at the University of Stuttgart and leads the research group for
        Visualization and Virtual/Augmented Reality there.
        He received his Ph.D. degree in Computer Science from the University of Munich, Germany, in 2010.
        Further stops included the Jacobs University Bremen, University of Vienna, University of British
        Columbia in Vancouver, and the BMW Group Research and Technology, Munich.
      </p>
      <p>
        His research interests focus on visual and interactive machine learning, perceptual modeling for
        visualization, immersive analytics and situated visualization, novel interaction technologies, as well
        as the methodological and theoretical foundations underlying them.
      </p>
    </div>
  </div>

  <p>
    <a href="https://www.vis.uni-stuttgart.de/en/institute/team/Sedlmair-00002/" target="_blank">Institute website</a>
  </p>

  <h2>Research Interests</h2>
  <ul>
    <li>Visualization &amp; visual analytics</li>
    <li>VR/AR</li>
    <li>HCI</li>
  </ul>

  <h2>More</h2>
  <ul>
    <li>
      <a href="https://scholar.google.com/citations?user=objnJXoAAAAJ" target="_blank">Google Scholar</a>
    </li>
    <li>
      <a href="https://orcid.org/0000-0001-7048-9292" target="_blank">ORCID</a>
    </li>
    <li>
      <a href="https://arxiv.org/a/sedlmair_m_1.html" target="_blank">arXiv</a>
    </li>
  </ul>
</div>

            </article>
            <article> <a class="anchor" name="publications"></a>
                <h1>Publications</h1>
                
    
    <h2>
        2022
    </h2>
    
    <div
        class="paper small"
        id="papercalepso2022cardlearner"
    >
        
            <img
                id="imagecalepso2022cardlearner"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercalepso2022cardlearner', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/calepso2022cardlearner.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercalepso2022cardlearner', 'small'); toggleImageSize(imagecalepso2022cardlearner);"
                title="Click to show details"
            >
                cARdLearner: Using Expressive Virtual Agents when Learning Vocabulary in Augmented Reality
            </h3>  <a class="anchor" name="calepso2022cardlearner"></a>
            <div class="authors">
                <span class="firstAuthor">Aimee Sousa Calepso</span>,
                Natalie Hube, Noah Berenguel Senn, Vincent Brandt, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CHI 2022</span>
                <span class="publication">Late-Breaking Work</span>
                <a href="../pdf/calepso2022cardlearner.pdf" target="_blank">PDF</a>
                
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Augmented reality (AR) has a diverse range of applications, including language teaching. When studying a foreign language, one of the biggest challenges learners face is memorizing new vocabulary. While augmented holograms are a promising means of supporting this memorization process, few studies have explored their potential in the language learning context. We demonstrate the possibility of using flashcard along with an expressive holographic agent on vocabulary learning. Users scan a flashcard and play an animation that is connected with an emotion related to the word they are seeing. Our goal is to propose an alternative to the traditional use of flashcards, and also introduce another way of using AR in the association process.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2022using"
    >
        
            <img
                id="imagehube2022using"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2022using', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2022using.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2022using', 'small'); toggleImageSize(imagehube2022using);"
                title="Click to show details"
            >
                Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study
            </h3>  <a class="anchor" name="hube2022using"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                 Kresimir Vidackovic, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CHI 2022</span>
                <span class="publication">Late-Breaking Work</span>
                <a href="../pdf/hube2022using.pdf" target="_blank">PDF</a>
                
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Virtual avatars are widely used for collaborating in virtual environments. Yet, often these avatars lack expressiveness to determine a state of mind. Prior work has demonstrated efective usage of determining emotions and animated lip movement through analyzing mere audio tracks of spoken words. To provide this information on a virtual avatar, we created a natural audio data set consisting of 17 audio fles from which we then extracted the underlying emotion and lip movement. To conduct a pilot study, we developed a prototypical system that displays the extracted visual parameters and then maps them on a virtual avatar while playing the corresponding audio fle. We tested the system with 5 participants in two conditions: (i) while seeing the virtual avatar only an audio fle was played. (ii) In addition to the audio fle, the extracted facial visual parameters were displayed on the virtual avatar. Our results suggest the validity of using additional visual parameters in the avatars face as it helps to determine emotions. We conclude with a brief discussion on the outcomes and their implications on future work.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperheyen2022cellovis"
    >
        
            <img
                id="imageheyen2022cellovis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperheyen2022cellovis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/heyen2022cellovis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperheyen2022cellovis', 'small'); toggleImageSize(imageheyen2022cellovis);"
                title="Click to show details"
            >
                Immersive Visual Analysis of Cello Bow Movements
            </h3>  <a class="anchor" name="heyen2022cellovis"></a>
            <div class="authors">
                <span class="firstAuthor">Frank Heyen</span>,
                Yannik Kohler, Sebastian Triebener, Sebastian Rigling, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CHI 2022</span>
                <span class="publication">Workshop Paper</span>
                <a href="../pdf/heyen2022cellovis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.48550/arxiv.2203.13316" target="_blank">website</a>
                <a href="../video/heyen2022cellovis.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose a 3D immersive visualization environment for analyzing the right hand movements of a cello player. To achieve this, we track the position and orientation of the cello bow and record audio. As movements mostly occur in a shallow volume and the motion is therefore mostly two-dimensional, we use the third dimension to encode time. Our concept further explores various mappings from motion and audio data to spatial and other visual attributes. We work in close cooperation with a cellist and plan to evaluate our prototype through a user study with a group of cellists in the near future.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@misc{https://doi.org/10.48550/arxiv.2203.13316,
  doi = {10.48550/ARXIV.2203.13316},
  url = {https://arxiv.org/abs/2203.13316},
  author = {Heyen, Frank and Kohler, Yannik and Triebener, Sebastian and Rigling, Sebastian and Sedlmair, Michael},
  keywords = {Human-Computer Interaction (cs.HC), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Immersive Visual Analysis of Cello Bow Movements},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy - EXC 2075 - 390740016, and by Cyber Valley (InstruData project).
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperheyen2022datadriven"
    >
        
            <img
                id="imageheyen2022datadriven"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperheyen2022datadriven', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/heyen2022datadriven.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperheyen2022datadriven', 'small'); toggleImageSize(imageheyen2022datadriven);"
                title="Click to show details"
            >
                Data-Driven Visual Reflection on Music Instrument Practice
            </h3>  <a class="anchor" name="heyen2022datadriven"></a>
            <div class="authors">
                <span class="firstAuthor">Frank Heyen</span>,
                Quynh Ngo, Kuno Kurzhals, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CHI 2022</span>
                <span class="publication">Workshop Paper</span>
                <a href="../pdf/heyen2022datadriven.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.48550/arxiv.2203.13320" target="_blank">website</a>
                <a href="../video/heyen2022datadriven.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose a data-driven approach to music instrument practice that allows studying patterns and long-term trends through visualization. Inspired by life logging and fitness tracking, we imagine musicians to record their practice sessions over the span of months or years. The resulting data in the form of MIDI or audio recordings can then be analyzed sporadically to track progress and guide decisions. Toward this vision, we started exploring various visualization designs together with a group of nine guitarists, who provided us with data and feedback over the course of three months.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@misc{https://doi.org/10.48550/arxiv.2203.13320,
  doi = {10.48550/ARXIV.2203.13320},
  url = {https://arxiv.org/abs/2203.13320},
  author = {Heyen, Frank and Ngo, Quynh Quang and Kurzhals, Kuno and Sedlmair, Michael},
  keywords = {Human-Computer Interaction (cs.HC), Graphics (cs.GR), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Data-Driven Visual Reflection on Music Instrument Practice},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}
</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                Funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy - EXC 2075 - 390740016, and by Cyber Valley (InstruData project).
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperangerbauer2022accessibility"
    >
        
            <img
                id="imageangerbauer2022accessibility"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperangerbauer2022accessibility', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/angerbauer2022accessibility.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperangerbauer2022accessibility', 'small'); toggleImageSize(imageangerbauer2022accessibility);"
                title="Click to show details"
            >
                Accessibility for Color Vision Deficiencies: Challenges and Findings of a Large Scale Study on Paper Figures
            </h3>  <a class="anchor" name="angerbauer2022accessibility"></a>
            <div class="authors">
                <span class="firstAuthor">Katrin Angerbauer</span>,
                Nils Rodrigues, Rene Cutura, Seyda Öney, Nelusa Pathmanathan, Cristina Morariu, Daniel Weiskopf, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CHI 2022</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/angerbauer2022accessibility.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3491102.3502133" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present an exploratory study on the accessibility of images in publications when viewed with color vision deficiencies (CVDs). The study is based on 1,710 images sampled from a visualization dataset (VIS30K) over five years. We simulated four CVDs on each image. First, four researchers (one with a CVD) identified existing issues and helpful aspects in a subset of the images. Based on the resulting labels, 200 crowdworkers provided  30,000 ratings on present CVD issues in the simulated images. We analyzed this data for correlations, clusters, trends, and free text comments to gain a first overview of paper figure accessibility. Overall, about 60 % of the images were rated accessible. Furthermore, our study indicates that accessibility issues are subjective and hard to detect. On a meta-level, we reflect on our study experience to point out challenges and opportunities of large-scale accessibility studies for future research directions.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.1145/3491102.3502133,
author = {Angerbauer, Katrin and Rodrigues, Nils and Cutura, Rene and \"{O}ney, Seyda and Pathmanathan, Nelusa and Morariu, Cristina and Weiskopf, Daniel and Sedlmair, Michael},
title = {Accessibility for Color Vision Deficiencies: Challenges and Findings of a Large Scale Study on Paper Figures},
year = {2022},
isbn = {9781450391573},
publisher = {ACM},
url = {https://doi.org/10.1145/3491102.3502133},
doi = {10.1145/3491102.3502133},
abstract = {We present an exploratory study on the accessibility of images in publications when viewed with color vision deficiencies (CVDs). The study is based on 1,710 images sampled from a visualization dataset (VIS30K) over five years. We simulated four CVDs on each image. First, four researchers (one with a CVD) identified existing issues and helpful aspects in a subset of the images. Based on the resulting labels, 200 crowdworkers provided  30,000 ratings on present CVD issues in the simulated images. We analyzed this data for correlations, clusters, trends, and free text comments to gain a first overview of paper figure accessibility. Overall, about 60 % of the images were rated accessible. Furthermore, our study indicates that accessibility issues are subjective and hard to detect. On a meta-level, we reflect on our study experience to point out challenges and opportunities of large-scale accessibility studies for future research directions. },
booktitle = {CHI Conference on Human Factors in Computing Systems},
articleno = {134},
numpages = {23},
keywords = {color vision deficiency, crowdsourcing, visualization, accessibility},
series = {CHI '22}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (projects A08 and B01). We thank all our study participants and in particular Sajid Baloch for his valuable input.
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperachberger2022stroe"
    >
        
            <img
                id="imageachberger2022stroe"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperachberger2022stroe', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/achberger2022stroe.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperachberger2022stroe', 'small'); toggleImageSize(imageachberger2022stroe);"
                title="Click to show details"
            >
                STROE: An Ungrounded String-Based Weight Simulation Device
            </h3>  <a class="anchor" name="achberger2022stroe"></a>
            <div class="authors">
                <span class="firstAuthor">Alexander Achberger</span>,
                Pirathipan Arulrajah, Michael Sedlmair, Kresimir Vidackovic
            </div>
            <div>
                <span class="publication">VR 2022</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/achberger2022stroe.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/VR51125.2022.00029" target="_blank">website</a>
                <a href="https://www.youtube.com/watch?v=9edaBf7VqNY" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user’s hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users’ perceived realism and immersion of VR scenes.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@INPROCEEDINGS{9756818,
  author={Achberger, Alexander and Arulrajah, Pirathipan and Sedlmair, Michael and Vidackovic, Kresimir},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={{STROE}: An Ungrounded String-Based Weight Simulation Device}, 
  year={2022},
  pages={112-120},
  abstract={We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user’s hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users’ perceived realism and immersion of VR scenes.},
  doi={10.1109/VR51125.2022.00029},
  ISSN={2642-5254}
}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papergebhardt2022molecusense"
    >
        
            <img
                id="imagegebhardt2022molecusense"
                title="Click to enlarge and show details"
                onclick="toggleClass('papergebhardt2022molecusense', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/gebhardt2022molecusense.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papergebhardt2022molecusense', 'small'); toggleImageSize(imagegebhardt2022molecusense);"
                title="Click to show details"
            >
                MolecuSense: Using Force-Feedback Gloves for Creating and Interacting with Ball-and-Stick Molecules in VR
            </h3>  <a class="anchor" name="gebhardt2022molecusense"></a>
            <div class="authors">
                <span class="firstAuthor">Patrick Gebhardt</span>,
                Xingyao Yu, Andreas Köhn, Michael Sedlmair
            </div>
            <div>
                <span class="publication">arXiv 2022</span>
                <span class="publication">Short Paper</span>
                <a href="https://arxiv.org/pdf/2203.09577.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.48550/arXiv.2203.09577" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We contribute MolecuSense, a virtual version of a physical molecule construction kit, based on visualization in Virtual Reality (VR) and interaction with force-feedback gloves. Targeting at chemistry education, our goal is to make virtual molecule structures more tangible. Results of an initial user study indicate that the VR molecular construction kit was positively received. Compared to a physical construction kit, the VR molecular construction kit is on the same level in terms of natural interaction. Besides, it fosters the typical digital advantages though, such as saving, exporting, and sharing of molecules. Feedback from the study participants has also revealed potential future avenues for tangible molecule visualizations. 
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@misc{https://doi.org/10.48550/arxiv.2203.09577,
  doi = {10.48550/ARXIV.2203.09577},
  url = {https://arxiv.org/abs/2203.09577},
  author = {Gebhardt, Patrick and Yu, Xingyao and Köhn, Andreas and Sedlmair, Michael},
  keywords = {Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MolecuSense: Using Force-Feedback Gloves for Creating and Interacting with Ball-and-Stick Molecules in VR},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperskreinig2022ar"
    >
        
            <img
                id="imageskreinig2022ar"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperskreinig2022ar', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/skreinig2022ar.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperskreinig2022ar', 'small'); toggleImageSize(imageskreinig2022ar);"
                title="Click to show details"
            >
                AR Hero: Generating Interactive Augmented Reality Guitar Tutorials
            </h3>  <a class="anchor" name="skreinig2022ar"></a>
            <div class="authors">
                <span class="firstAuthor">Lucchas Ribeiro Skreinig</span>,
                Ana Stanescu, Shohei Mori, Frank Heyen, Peter Mohr, Michael Sedlmair, Dieter Schmalstieg, Denis Kalkofen
            </div>
            <div>
                <span class="publication">VRW 2022</span>
                
                <a href="../pdf/skreinig2022ar.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/VRW55335.2022.00086" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We introduce a system capable of generating interactive Augmented Reality guitar tutorials by parsing common digital guitar tablature and by capturing the performance of an expert using a multi-camera array. Instructions are presented to the user in an Augmented Reality application using either an abstract visualization, a 3D virtual hand, or a 3D video. To support individual users at different skill levels the system provides full control of the playback of a tutorial, including its speed and looping behavior, while delivering live feedback on the user’s performance.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@INPROCEEDINGS{9757565,
  author={Skreinig, Lucchas Ribeiro and Stanescu, Ana and Mori, Shohei and Heyen, Frank and Mohr, Peter and Sedlmair, Michael and Schmalstieg, Dieter and Kalkofen, Denis},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
  title={AR Hero: Generating Interactive Augmented Reality Guitar Tutorials}, 
  year={2022},
  pages={395-401},
  doi={10.1109/VRW55335.2022.00086}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperfleck2022ragrug"
    >
        
            <img
                id="imagefleck2022ragrug"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperfleck2022ragrug', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/fleck2022ragrug.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperfleck2022ragrug', 'small'); toggleImageSize(imagefleck2022ragrug);"
                title="Click to show details"
            >
                RagRug: A Toolkit for Situated Analytics
            </h3>  <a class="anchor" name="fleck2022ragrug"></a>
            <div class="authors">
                <span class="firstAuthor">Philipp Fleck</span>,
                Aimee Sousa Calepso, Sebastian Hubenschmid, Michael Sedlmair, Dieter Schmalstieg
            </div>
            <div>
                <span class="publication">TVCG 2022</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/fleck2022ragrug.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2022.3157058" target="_blank">website</a>
                <a href="https://www.youtube.com/watch?v=mFxSdvQhSVU" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present RagRug, an open-source toolkit for situated analytics. The abilities of RagRug go beyond previous immersive analytics toolkits by focusing on specific requirements emerging when using augmented reality (AR) rather than virtual reality. RagRug combines state of the art visual encoding capabilities with a comprehensive physical-virtual model, which lets application developers systematically describe the physical objects in the real world and their role in AR. We connect AR visualization with data streams from the Internet of Things using distributed dataflow. To this aim, we use reactive programming patterns so that visualizations become context-aware, i.e., they adapt to events coming in from the environment. The resulting authoring system is low-code; it emphasises describing the physical and the virtual world and the dataflow between the elements contained therein. We describe the technical design and implementation of RagRug, and report on five example applications illustrating the toolkit's abilities.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{fleck2022ragrug,
  title={{RagRug}: A Toolkit for Situated Analytics},
  author={Fleck, Philipp and Calepso, Aimee Sousa and Hubenschmid, Sebastian and Sedlmair, Michael and Schmalstieg, Dieter},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  year={2022},
  publisher={IEEE}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papereirich2022rfx"
    >
        
            <img
                id="imageeirich2022rfx"
                title="Click to enlarge and show details"
                onclick="toggleClass('papereirich2022rfx', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/eirich2022rfx.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papereirich2022rfx', 'small'); toggleImageSize(imageeirich2022rfx);"
                title="Click to show details"
            >
                RfX: A Design Study for the Interactive Exploration of a Random Forest to Enhance Testing Procedures for Electrical Engines
            </h3>  <a class="anchor" name="eirich2022rfx"></a>
            <div class="authors">
                <span class="firstAuthor">Joscha Eirich</span>,
                M. Münch, Dominik Jäckle, Michael Sedlmair, Jakob Bonart, Tobias Schreck
            </div>
            <div>
                <span class="publication">CGF 2022</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/eirich2022rfx.pdf" target="_blank">PDF</a>
                
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Random Forests (RFs) are a machine learning (ML) technique widely used across industries. The interpretation of a given RF usually relies on the analysis of statistical values and is often only possible for data analytics experts. To make RFs accessible to experts with no data analytics background, we present RfX, a Visual Analytics (VA) system for the analysis of a RF's decision-making process. RfX allows to interactively analyse the properties of a forest and to explore and compare multiple trees in a RF. Thus, its users can identify relationships within a RF's feature subspace and detect hidden patterns in the model's underlying data. We contribute a design study in collaboration with an automotive company. A formative evaluation of RFX was carried out with two domain experts and a summative evaluation in the form of a field study with five domain experts. In this context, new hidden patterns such as increased eccentricities in an engine's rotor by observing secondary excitations of its bearings were detected using analyses made with RfX. Rules derived from analyses with the system led to a change in the company's testing procedures for electrical engines, which resulted in 80% reduced testing time for over 30% of all components.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{https://doi.org/10.1111/cgf.14452,
author = {Eirich, J. and Münch, M. and Jäckle, D. and Sedlmair, M. and Bonart, J. and Schreck, T.},
title = {{RfX}: A Design Study for the Interactive Exploration of a Random Forest to Enhance Testing Procedures for Electrical Engines},
journal = {Computer Graphics Forum (CGF)},
keywords = {human–computer interfaces, interaction, visual analytics, visualization},
doi = {https://doi.org/10.1111/cgf.14452},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14452},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14452},
abstract = {Abstract Random Forests (RFs) are a machine learning (ML) technique widely used across industries. The interpretation of a given RF usually relies on the analysis of statistical values and is often only possible for data analytics experts. To make RFs accessible to experts with no data analytics background, we present RfX, a Visual Analytics (VA) system for the analysis of a RF's decision-making process. RfX allows to interactively analyse the properties of a forest and to explore and compare multiple trees in a RF. Thus, its users can identify relationships within a RF's feature subspace and detect hidden patterns in the model's underlying data. We contribute a design study in collaboration with an automotive company. A formative evaluation of RFX was carried out with two domain experts and a summative evaluation in the form of a field study with five domain experts. In this context, new hidden patterns such as increased eccentricities in an engine's rotor by observing secondary excitations of its bearings were detected using analyses made with RfX. Rules derived from analyses with the system led to a change in the company's testing procedures for electrical engines, which resulted in 80\% reduced testing time for over 30\% of all components.}
}

</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2021
    </h2>
    
    <div
        class="paper small"
        id="paperrau2021visual"
    >
        
            <img
                id="imagerau2021visual"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperrau2021visual', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/rau2021visual.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperrau2021visual', 'small'); toggleImageSize(imagerau2021visual);"
                title="Click to show details"
            >
                Visual Support for Human-AI Co-Composition
            </h3>  <a class="anchor" name="rau2021visual"></a>
            <div class="authors">
                <span class="firstAuthor">Simeon Rau</span>,
                Frank Heyen, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMIR 2021</span>
                <span class="publication">Late-Breaking Demo Poster</span>
                <a href="../pdf/rau2021visual.pdf" target="_blank">PDF</a>
                <a href="https://archives.ismir.net/ismir2021/latebreaking/000014.pdf" target="_blank">website</a>
                <a href="../video/rau2021visual.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose a visual approach for AI-assisted music composition, where the user interactively generates, selects, and adapts short melodies. Based on an entered start melody, we automatically generate multiple continuation samples. Repeating this step and in turn generating continuations for these samples results in a tree or graph of melodies. We visualize this structure with two visualizations, where nodes display the piano roll of the corresponding sample. By interacting with these visualizations, the user can quickly listen to, choose, and adapt melodies, to iteratively create a composition. A third visualization provides an overview over larger numbers of samples, allowing for insights into the AI's predictions and the sample space.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{rau2021visual,
  title={Visual Support for Human-{AI} Co-Composition},
  author={Rau, Simeon and Heyen, Frank and Sedlmair, Michael},
  year={2021},
  booktitle={Extended Abstracts for the Late-Breaking Demo Session of the 22nd Int. Society for Music Information Retrieval Conf. (ISMIR)},
  url={https://archives.ismir.net/ismir2021/latebreaking/000014.pdf}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was funded by the Cyber Valley Research Fund – Project InstruData.
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2021ismar"
    >
        
            <img
                id="imagehube2021ismar"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2021ismar', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2021ismar.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2021ismar', 'small'); toggleImageSize(imagehube2021ismar);"
                title="Click to show details"
            >
                VR Collaboration in Large Companies: An Interview Study on the Role of Avatars
            </h3>  <a class="anchor" name="hube2021ismar"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Katrin Angerbauer, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2021</span>
                <span class="publication">Short Paper</span>
                <a href="../pdf/hube2021ismar.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Collaboration is essential in companies and often physical presence is required, thus, more and more Virtual Reality (VR) systems are used to work together remotely. To support social interaction, human representations in form of avatars are used in collaborative virtual environment (CVE) tools. However, up to now, the avatar representations often are limited in their design and functionality, which may hinder effective collaboration. In our interview study, we explored the status quo of VR collaboration in a large automotive company setting with a special focus on the role of avatars. We collected inter-view data from 21 participants, from which we identified challenges of current avatar representations used in our setting. Based on these findings, we discuss design suggestions for avatars in a company setting, which aim to improve social interaction. As opposed to state-of-the-art research, we found that users within the context of a large automotive company have an altered need with respect to avatar representations.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{hube2021ismar,
	Author = {Natalie Hube and Katrin Angerbauer and Daniel Pohlandt and Kresimir Vidackovic and Michael Sedlmair},
	Title = {VR Collaboration in Large Companies: An Interview Study on the Role of Avatars},
	Booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct, Short paper)},
	pages = {139--144},
	url = {https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037},
  	doi = {10.1109/ISMAR-Adjunct54149.2021.00037},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2021visap"
    >
        
            <img
                id="imagecutura2021visap"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2021visap', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/cutura2021visap.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2021visap', 'small'); toggleImageSize(imagecutura2021visap);"
                title="Click to show details"
            >
                DaRt: Generative Art using Dimensionality Reduction Algorithms
            </h3>  <a class="anchor" name="cutura2021visap"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Kathrin Angerbauer, Frank Heyen, Natalie Hube, Michael Sedlmair
            </div>
            <div>
                <span class="publication">VIS 2021</span>
                <span class="publication">Pictorial</span>
                <a href="../pdf/cutura2021visap.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/VISAP52981.2021.00013" target="_blank">website</a>
                <a href="https://youtu.be/pOcksJOiAPw" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Dimensionality Reduction (DR) is a popular technique that is often used in Machine Learning and Visualization communities to analyze high-dimensional data. The approach is empirically proven to be powerful for uncovering previously unseen structures in the data. While observing the results of the intermediate optimization steps of DR algorithms, we coincidently discovered the artistic beauty of the DR process. With enthusiasm for the beauty, we decided to look at DR from a generative art lens rather than their technical application aspects and use DR techniques to create artwork. Particularly, we use the optimization process to generate images, by drawing each intermediate step of the optimization process with some opacity over the previous intermediate result. As another alternative input, we used a neural-network model for face-landmark detection, to apply DR to portraits, while maintaining some facial properties, resulting in abstracted facial avatars. In this work, we provide such a collection of such artwork.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2021dart,
  title={{DaRt}: Generative Art using Dimensionality Reduction Algorithms},
  author={Cutura, Rene and Angerbauer, Katrin and Heyen, Frank and Hube, Natalie and Sedlmair, Michael},
  booktitle={2021 IEEE VIS Arts Program (VISAP)},
  pages={59--72},
  year={2021},
  organization={IEEE},
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 251654672 - TRR 161
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papergrossmann2021vis"
    >
        
            <img
                id="imagegrossmann2021vis"
                title="Click to enlarge and show details"
                onclick="toggleClass('papergrossmann2021vis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/grossmann2021vis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papergrossmann2021vis', 'small'); toggleImageSize(imagegrossmann2021vis);"
                title="Click to show details"
            >
                Does the Layout Really Matter? A Study on Visual Model Accuracy Estimation
            </h3>  <a class="anchor" name="grossmann2021vis"></a>
            <div class="authors">
                <span class="firstAuthor">Nicolas Grossmann</span>,
                Jürgen Bernard, Michael Sedlmair, Manuela Waldner
            </div>
            <div>
                <span class="publication">VIS 2021</span>
                <span class="publication">Short Paper</span>
                <a href="https://arxiv.org/abs/2110.07188" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/VIS49827.2021.9623326" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In visual interactive labeling, users iteratively assign labels to data items until the machine model reaches an acceptable accuracy. A crucial step of this process is to inspect the model’s accuracy and decide whether it is necessary to label additional elements. In scenarios with no or very little labeled data, visual inspection of the predictions is required. Similarity-preserving scatterplots created through a dimensionality reduction algorithm are a common visualization that is used in these cases. Previous studies investigated the effects of layout and image complexity on tasks like labeling. However, model evaluation has not been studied systematically. We present the results of an experiment studying the influence of image complexity and visual grouping of images on model accuracy estimation. We found that users outperform traditional automated approaches when estimating a model’s accuracy. Furthermore, while the complexity of images impacts the overall performance, the layout of the items in the plot has little to no effect on estimations.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{grossmann2021vis,
	Author = {Nicolas Grossmann and J{\"u}rgen Bernard and Michael Sedlmair and Manuela Waldner},
	Title = {Does the Layout Really Matter? A Study on Visual Model Accuracy Estimation},
	Booktitle = {IEEE Visualization Conference  (VIS, Short Paper)},
	pages = {61--65},
	url = {https://arxiv.org/abs/2110.07188},
  	doi = {10.1109/VIS49827.2021.9623326},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperachberger2021uist"
    >
        
            <img
                id="imageachberger2021uist"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperachberger2021uist', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/achberger2021uist.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperachberger2021uist', 'small'); toggleImageSize(imageachberger2021uist);"
                title="Click to show details"
            >
                Strive: String-Based Force Feedback for Automotive Engineering
            </h3>  <a class="anchor" name="achberger2021uist"></a>
            <div class="authors">
                <span class="firstAuthor">Alexander Achberger</span>,
                Fabian Aust, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair
            </div>
            <div>
                <span class="publication">UIST 2021</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/achberger2021uist.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3472749.3474790" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The large potential of force feedback devices for interacting in Virtual Reality (VR) has been illustrated in a plethora of research prototypes. Yet, these devices are still rarely used in practice and it remains an open challenge how to move this research into practice. To that end, we contribute a participatory design study on the use of haptic feedback devices in the automotive industry. Based on a 10-month observing process with 13 engineers, we developed STRIVE, a string-based haptic feedback device. In addition to the design of STRIVE, this process led to a set of requirements for introducing haptic devices into industrial settings, which center around a need for flexibility regarding forces, comfort, and mobility. We evaluated STRIVE with 16 engineers in five different day-to-day automotive VR use cases. The main results show an increased level of trust and perceived safety as well as further challenges towards moving haptics research into practice. 
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{achberger2021uist,
	Author = {Alexander Achberger and Fabian Aust and Daniel Pohlandt and Kresimir Vidackovic and Michael Sedlmair},
	Title = {{STRIVE}: String-Based Force Feedback for Automotive Engineering},
	Booktitle = {ACM Symposium on User Interface Software and Technology (UIST)},
	pages = {841--853},
	url = {https://doi.org/10.1145/3472749.3474790},
  	doi = {10.1145/3472749.3474790},
	Year = {2021}
} </textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papereirich2021vis"
    >
        
            <img
                id="imageeirich2021vis"
                title="Click to enlarge and show details"
                onclick="toggleClass('papereirich2021vis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/eirich2021vis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papereirich2021vis', 'small'); toggleImageSize(imageeirich2021vis);"
                title="Click to show details"
            >
                IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines
            </h3>  <a class="anchor" name="eirich2021vis"></a>
            <div class="authors">
                <span class="firstAuthor">Joscha Eirich</span>,
                Jakob Bonart, Dominik Jäckle, Michael Sedlmair, Ute Schmid, Kai Fischbach, Tobias Schreck, Jürgen Bernard
            </div>
            <div>
                <span class="publication">TVCG 2021</span>
                <span class="publication">Full paper</span>
                <a href="../pdf/eirich2021vis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2021.3114797" target="_blank">website</a>
                <a href="../video/eirich2021vis.mp4" target="_blank">video</a>
                <a href="../suppl/eirich2021vis.zip" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this design study, we present IRVINE, a Visual Analytics (VA) system, which facilitates the analysis of acoustic data to detect and understand previously unknown errors in the manufacturing of electrical engines. In serial manufacturing processes, signatures from acoustic data provide valuable information on how the relationship between multiple produced engines serves to detect and understand previously unknown errors. To analyze such signatures, IRVINE leverages interactive clustering and data labeling techniques, allowing users to analyze clusters of engines with similar signatures, drill down to groups of engines, and select an engine of interest. Furthermore, IRVINE allows to assign labels to engines and clusters and annotate the cause of an error in the acoustic raw measurement of an engine. Since labels and annotations represent valuable knowledge, they are conserved in a knowledge database to be available for other stakeholders. We contribute a design study, where we developed IRVINE in four main iterations with engineers from a company in the automotive sector. To validate IRVINE, we conducted a field study with six domain experts. Our results suggest a high usability and usefulness of IRVINE as part of the improvement of a real-world manufacturing process. Specifically, with IRVINE domain experts were able to label and annotate produced electrical engines more than 30% faster.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{eirich2021vis,
	author = {Joscha Eirich and Jakob Bonart and Dominik J{\"a}ckle and Michael Sedlmair and Ute Schmid and Kai Fischbach and Tobias Schreck and J{\"u}rgen Bernard},
	title = {{IRVINE}: A Design Study on Analyzing Correlation Patterns of Electrical Engines},
	journal = {IEEE Trans. Visualization and Computer Graphics (TVCG, Proc. VIS 2021)},
	note = {To appear. Best paper award},
	url = {https://doi.org/10.1109/TVCG.2021.3114797},
  	doi = {10.1109/TVCG.2021.3114797},
	year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2021vinci"
    >
        
            <img
                id="imagecutura2021vinci"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2021vinci', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/cutura2021vinci.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2021vinci', 'small'); toggleImageSize(imagecutura2021vinci);"
                title="Click to show details"
            >
                Hagrid — Gridify Scatterplots with Hilbert and Gosper Curves
            </h3>  <a class="anchor" name="cutura2021vinci"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                
Cristina Morariu, Zhanglin Cheng, Yunhai Wang, Daniel Weiskopf, Michael Sedlmair
            </div>
            <div>
                <span class="publication">VINCI 2021</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/cutura2021vinci.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3481549.3481569" target="_blank">website</a>
                <a href="https://youtu.be/E_XP31_JzGY" target="_blank">video</a>
                <a href="https://renecutura.eu/pdfs/hagrid_supplemental.pdf" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                A common enhancement of scatterplots represents points as small multiples, glyphs, or thumbnail images. As this encoding often results in overlaps, a general strategy is to alter the position of the data points, for instance, to a grid-like structure. Previous approaches rely on solving expensive optimization problems or on dividing the space that alter the global structure of the scatterplot. To find a good balance between efficiency and neighborhood and layout preservation, we propose Hagrid, a technique that uses space-filling curves (SFCs) to “gridify” a scatterplot without employing expensive collision detection and handling mechanisms. Using SFCs ensures that the points are plotted close to their original position, retaining approximately the same global structure. The resulting scatterplot is mapped onto a rectangular or hexagonal grid, using Hilbert and Gosper curves. We discuss and evaluate the theoretic runtime of our approach and quantitatively compare our approach to three state-of-the-art gridifying approaches, DGrid, Small multiples with gaps SMWG, and CorrelatedMultiples CMDS, in an evaluation comprising 339 scatterplots. Here, we compute several quality measures for neighborhood preservation together with an analysis of the actual runtimes. The main results show that, compared to the best other technique, Hagrid is faster by a factor of four, while achieving similar or even better quality of the gridified layout. Due to its computational efficiency, our approach also allows novel applications of gridifying approaches in interactive settings, such as removing local overlap upon hovering over a scatterplot.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2021hagrid,
	author = {Cutura, Rene and Morariu, Cristina and Cheng, Zhanglin and Wang, Yunhai and Weiskopf, Daniel and Sedlmair, Michael},
	title = {{Hagrid -- Gridify Scatterplots with Hilbert and Gosper Curves}},
	year = {2021},
	isbn = {9781450386470},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3481549.3481569},
	doi = {10.1145/3481549.3481569},
	booktitle = {The 14th International Symposium on Visual Information Communication and Interaction},
	articleno = {1},
	numpages = {8},
	keywords = {Grid layout, Neighborhood-preserving., Space-filling curve},
	location = {Potsdam, Germany},
	series = {VINCI 2021}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was supported by the BMK FFG ICT of the Future program via the ViSciPub project (no. 867378), and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161.
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperachberger2021vinci"
    >
        
            <img
                id="imageachberger2021vinci"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperachberger2021vinci', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/achberger2021vinci.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperachberger2021vinci', 'small'); toggleImageSize(imageachberger2021vinci);"
                title="Click to show details"
            >
                PropellerHand: Hand-Mounted, Propeller-Based Force Feedback Device
            </h3>  <a class="anchor" name="achberger2021vinci"></a>
            <div class="authors">
                <span class="firstAuthor">Alexander Achberger</span>,
                Frank Heyen, Kresimir Vidackovic, Michael Sedlmair
            </div>
            <div>
                <span class="publication">VINCI 2021</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/achberger2021vinci.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3481549.3481563" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Immersive analytics is a fast growing field that is often applied in virtual reality (VR). VR environments often lack immersion due to missing sensory feedback when interacting with data. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our final version through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{achberger2021vinci,
  author = {Alexander Achberger and Frank Heyen and Kresimir Vidackovic and Michael Sedlmair},
  title = {PropellerHand: {A} Hand-Mounted, Propeller-Based Force Feedback Device},
  booktitle = {International Symposium on Visual Information Communication and Interaction (VINCI)},
  pages     = {4:1--4:8},
  publisher = {ACM},
  year      = {2021},
  url       = {https://doi.org/10.1145/3481549.3481563},
  doi       = {10.1145/3481549.3481563}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkrauter2021muc"
    >
        
            <img
                id="imagekrauter2021muc"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperkrauter2021muc', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/krauter2021muc.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperkrauter2021muc', 'small'); toggleImageSize(imagekrauter2021muc);"
                title="Click to show details"
            >
                Don’t Catch It: An Interactive Virtual-Reality Environment to Learn About COVID-19 Measures Using Gamification Elements
            </h3>  <a class="anchor" name="krauter2021muc"></a>
            <div class="authors">
                <span class="firstAuthor">Christian Krauter</span>,
                Jonas Vogelsang, Aimee Sousa Calepso, Katrin Angerbauer, Michael Sedlmair
            </div>
            <div>
                <span class="publication">MuC 2021</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/krauter2021muc.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3473856.3474031" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The world is still under the influence of the COVID-19 pandemic. Even though vaccines are deployed as rapidly as possible, it is still necessary to use other measures to reduce the spread of the virus. Measures such as social distancing or wearing a mask receive a lot of criticism. Therefore, we want to demonstrate a serious game to help the players understand these measures better and show them why they are still necessary. The player of the game has to avoid other agents to keep their risk of a COVID-19 infection low. The game uses Virtual Reality through a Head-Mounted-Display to deliver an immersive and enjoyable experience. Gamification elements are used to engage the user with the game while they explore various environments. We also implemented visualizations that help the user with social distancing.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{krauter2021muc,
	Author = {Christian Krauter and Jonas Vogelsang and Aimee Sousa Calepso and Katrin Angerbauer and Michael Sedlmair},
	Title = {Don’t Catch It: An Interactive Virtual-Reality Environment to Learn About {COVID-19} Measures Using Gamification Elements},
	Booktitle = {Mensch und Computer},
	publisher = {ACM},
	pages = {593--596},
	url = {https://doi.org/10.1145/3473856.3474031},
  	doi = {10.1145/3473856.3474031},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperrijken2021illegible"
    >
        
            <img
                id="imagerijken2021illegible"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperrijken2021illegible', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/rijken2021illegible.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperrijken2021illegible', 'small'); toggleImageSize(imagerijken2021illegible);"
                title="Click to show details"
            >
                Illegible Semantics: Exploring the Design Space of Metal Logos
            </h3>  <a class="anchor" name="rijken2021illegible"></a>
            <div class="authors">
                <span class="firstAuthor">Gerrit J. Rijken</span>,
                Rene Cutura, Frank Heyen, Michael Sedlmair, Michael Correll, Jason Dykes, Noeska Smit
            </div>
            <div>
                <span class="publication">alt.VIS 2021</span>
                <span class="publication">Workshop Paper</span>
                <a href="https://arxiv.org/ftp/arxiv/papers/2109/2109.01688.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.48550/arXiv.2109.01688" target="_blank">website</a>
                <a href="https://www.youtube.com/watch?v=BZOdIhU-mrA" target="_blank">video</a>
                <a href="http://illegiblesemantics.com" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The logos of metal bands can be by turns gaudy, uncouth, or nearly illegible. Yet, these logos work: they communicate sophisticated notions of genre and emotional affect. In this paper we use the design considerations of metal logos to explore the space of “illegible semantics”: the ways that text can communicate information at the cost of readability, which is not always the most important objective. In this work, drawing on formative visualization theory, professional design expertise, and empirical assessments of a corpus ofmetal band logos, we describe a design space of metal logos and present a tool through which logo characteristics can be explored through visualization. We investigate ways in which logo designers imbue their text with meaning and consider opportunities and implications for visualization more widely.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{rijken2021altvis,
	Author = {Gerrit J Rijken and Rene Cutura and Frank Heyen and Michael Sedlmair and Michael Correll and Jason Dykes and Noeska Smit},
	Title = {Illegible Semantics: Exploring the Design Space of Metal Logos},
	Booktitle = {{IEEE VIS} alt.VIS Workshop},
	url = {https://arxiv.org/abs/2109.01688},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2021tiis"
    >
        
            <img
                id="imagebernard2021tiis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2021tiis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/bernard2021tiis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2021tiis', 'small'); toggleImageSize(imagebernard2021tiis);"
                title="Click to show details"
            >
                A Taxonomy of Property Measures to Unify Active Learning and Human-centered Approaches to Data Labeling
            </h3>  <a class="anchor" name="bernard2021tiis"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                 Marco Hutter, Michael Sedlmair, Matthias Zeppelzauer, Tamara Munzner
            </div>
            <div>
                <span class="publication">TiiS 2021</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/bernard2021tiis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3439333" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Strategies for selecting the next data instance to label, in service of generating labeled data for machine learning, have been considered separately in the machine learning literature on active learning and in the visual analytics literature on human-centered approaches. We propose a unified design space for instance selection strategies to support detailed and fine-grained analysis covering both of these perspectives. We identify a concise set of 15 properties, namely measureable characteristics of datasets or of machine learning models applied to them, that cover most of the strategies in these literatures. To quantify these properties, we introduce Property Measures (PM) as fine-grained building blocks that can be used to formalize instance selection strategies. In addition, we present a taxonomy of PMs to support the description, evaluation, and generation of PMs across four dimensions: machine learning (ML) Model Output, Instance Relations, Measure Functionality, and Measure Valence. We also create computational infrastructure to support qualitative visual data analysis: a visual analytics explainer for PMs built around an implementation of PMs using cascades of eight atomic functions. It supports eight analysis tasks, covering the analysis of datasets and ML models using visual comparison within and between PMs and groups of PMs, and over time during the interactive labeling process. We iteratively refined the PM taxonomy, the explainer, and the task abstraction in parallel with each other during a two-year formative process, and show evidence of their utility through a summative evaluation with the same infrastructure. This research builds a formal baseline for the better understanding of the commonalities and differences of instance selection strategies, which can serve as the stepping stone for the synthesis of novel strategies in future work.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{bernard2021tiis,
	author = {J{\"u}rgen Bernard and Marco Hutter and Michael Sedlmair and Matthias Zeppelzauer and Tamara Munzner},
	title = {A Taxonomy of Property Measures to Unify Active Learning and Human-centered Approaches to Data Labeling},
	journal = {ACM Transactions on Interactive Intelligent Systems (TiiS)},
	volume={11},
    number={3-4},
    pages={1--42},
	url = {https://doi.org/10.1145/3439333},
  	doi = {10.1145/3439333},
	year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkraus2021cga"
    >
        
            <img
                id="imagekraus2021cga"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperkraus2021cga', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/kraus2021cga.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperkraus2021cga', 'small'); toggleImageSize(imagekraus2021cga);"
                title="Click to show details"
            >
                The Value of Immersive Visualization
            </h3>  <a class="anchor" name="kraus2021cga"></a>
            <div class="authors">
                <span class="firstAuthor">Matthias Kraus</span>,
                Karsten Klein, Johannes Fuchs, Daniel A Keim, Falk Schreiber, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CG&A 2021</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/kraus2021cga.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/MCG.2021.3075258" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In recent years, research on immersive environments has experienced a new wave of interest, and immersive analytics has been established as a new research field. Every year, a vast amount of different techniques, applications, and user studies are published that focus on employing immersive environments for visualizing and analyzing data. Nevertheless, immersive analytics is still a relatively unexplored field that needs more basic research in many aspects and is still viewed with skepticism. Rightly so, because in our opinion, many researchers do not fully exploit the possibilities offered by immersive environments and, on the contrary, sometimes even overestimate the power of immersive visualizations. Although a growing body of papers has demonstrated individual advantages of immersive analytics for specific tasks and problems, the general benefit of using immersive environments for effective analytic tasks remains controversial. In this article, we reflect on when and how immersion may be appropriate for the analysis and present four guiding scenarios. We report on our experiences, discuss the landscape of assessment strategies, and point out the directions where we believe immersive visualizations have the greatest potential.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{kraus2021cga,
	author = {Matthias Kraus and Karsten Klein and Johannes Fuchs and Daniel A Keim and Falk Schreiber and Michael Sedlmair},
	title = {The Value of Immersive Visualization},
    journal={IEEE Computer Graphics and Applications (CG\&A)},
	volume={41},
    number={4},
    pages={125-132},
	url = {https://doi.org/10.1109/MCG.2021.3075258},
  	doi = {10.1109/MCG.2021.3075258},
	year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperabbas2021arxiv"
    >
        
            <img
                id="imageabbas2021arxiv"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperabbas2021arxiv', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/abbas2021arxiv.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperabbas2021arxiv', 'small'); toggleImageSize(imageabbas2021arxiv);"
                title="Click to show details"
            >
                ClustRank: A Visual Quality Measure Trained on Perceptual Data for Sorting Scatterplots by Cluster Patterns
            </h3>  <a class="anchor" name="abbas2021arxiv"></a>
            <div class="authors">
                <span class="firstAuthor">Mostafa Abbas</span>,
                Ehsan Ullah, Abdelkader Baggag, Halima Bensmail, Michael Sedlmair, Michael Aupetit
            </div>
            <div>
                <span class="publication"> 2021</span>
                <span class="publication">Technical Report</span>
                <a href="https://arxiv.org/pdf/2106.00599.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.48550/arXiv.2106.00599" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Visual quality measures (VQMs) are designed to support analysts by automatically detecting and quantifying patterns in visualizations. We propose a new data-driven technique called ClustRank that allows to rank scatterplots according to visible grouping patterns. Our model first encodes scatterplots in the parametric space of a Gaussian Mixture Model, and then uses a classifier trained on human judgment data to estimate the perceptual complexity of grouping patterns. The numbers of initial mixture components and final combined groups determine the rank of the scatterplot. ClustRank improves on existing VQM techniques by mimicking human judgments on two-Gaussian cluster patterns and gives more accuracy when ranking general cluster patterns in scatterplots. We demonstrate its benefit by analyzing kinship data for genome-wide association studies, a domain in which experts rely on the visual analysis of large sets of scatterplots. We make the three benchmark datasets and the ClustRank VQM available for practical use and further improvements. 
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@techreport{abbas2021arxiv,
	title = {{ClustRank}: A Visual Quality Measure Trained on Perceptual Data for Sorting Scatterplots by Cluster Patterns},
	author = {Mostafa Abbas and Ehsan Ullah and Abdelkader Baggag and Halima Bensmail and Michael Sedlmair and Michael Aupetit},
	Institution = {{arXiv} preprint},
	Number = {arXiv:2106.00599},
	Type = {Technical Report},
	url = {https://arxiv.org/pdf/2106.00599.pdf},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperling2021icdar"
    >
        
            <img
                id="imageling2021icdar"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperling2021icdar', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/ling2021icdar.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperling2021icdar', 'small'); toggleImageSize(imageling2021icdar);"
                title="Click to show details"
            >
                Document Domain Randomization for Deep Learning Document Layout Extraction
            </h3>  <a class="anchor" name="ling2021icdar"></a>
            <div class="authors">
                <span class="firstAuthor">Meng Ling</span>,
                Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert S Laramee, Han-Wei Shen, Jian Wu, C Lee Giles
            </div>
            <div>
                <span class="publication">ICDAR 2021</span>
                <span class="publication">Full Paper</span>
                <a href="https://arxiv.org/pdf/2105.14931" target="_blank">PDF</a>
                <a href="https://doi.org/10.1007/978-3-030-86549-8_32" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present document domain randomization (DDR), the first successful transfer of convolutional neural networks (CNNs) trained only on graphically rendered pseudo-paper pages to real-world document segmentation. DDR renders pseudo-document pages by modeling randomized textual and non-textual contents of interest, with user-defined layout and font styles to support joint learning of fine-grained classes. We demonstrate competitive results using our DDR approach to extract nine document classes from the benchmark CS-150 and papers published in two domains, namely annual meetings of Association for Computational Linguistics (ACL) and IEEE Visualization (VIS). We compare DDR to conditions of style mismatch, fewer or more noisy samples that are more easily obtained in the real world. We show that high-fidelity semantic information is not necessary to label semantic classes but style mismatch between train and test can lower model accuracy. Using smaller training samples had a slightly detrimental effect. Finally, network models still achieved high test accuracy when correct labels are diluted towards confusing labels; this behavior hold across several classes. 
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{ling2021icdar,
	Author = {Meng Ling and Jian Chen and Torsten M{\"o}ller and Petra Isenberg and Tobias Isenberg and Michael Sedlmair and Robert S Laramee and Han-Wei Shen and Jian Wu and C Lee Giles},
	Title = {Document Domain Randomization for Deep Learning Document Layout Extraction},
	Booktitle = {Document Analysis and Recognition (ICDAR)},
	publisher= {Springer International Publishing},
	pages = {497--513},
	url = {https://arxiv.org/abs/2105.14931},
  	doi = {10.1007/978-3-030-86549-8_32},
  	isbn = {978-3-030-86549-8},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperling2021visimages"
    >
        
            <img
                id="imageling2021visimages"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperling2021visimages', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/ling2021visimages.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperling2021visimages', 'small'); toggleImageSize(imageling2021visimages);"
                title="Click to show details"
            >
                Three Benchmark Datasets for Scholarly Article Layout Analysis
            </h3>  <a class="anchor" name="ling2021visimages"></a>
            <div class="authors">
                <span class="firstAuthor">Meng Ling</span>,
                Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert Laramee, Han-Wei Shen, Jian Wu, Clyde Lee Giles
            </div>
            <div>
                <span class="publication"> 2021</span>
                <span class="publication">Dataset</span>
                
                <a href="https://doi.org/10.21227/326q-bf39" target="_blank">website</a>
                
                <a href="https://ieee-dataport.org/open-access/three-benchmark-datasets-scholarly-article-layout-analysis" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                This dataset contains three benchmark datasets as part of the scholarly output of an ICDAR 2021 paper: 

Meng Ling, Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert S. Laramee, Han-Wei Shen, Jian Wu, and C. Lee Giles, Document Domain Randomization for Deep Learning Document Layout Extraction, 16th International Conference on Document Analysis and Recognition (ICDAR) 2021. September 5-10, Lausanne, Switzerland. 

This dataset contains nine class lables: abstract, algorithm, author, body text, caption, equation, figure, table, and title.

* Dataset 1: CS-150x, an extension of the classical benchmark dataset CS-150 from three classes (figure, table, and caption) to nine classes, 1176 pages, Clark, C., Divvala, S.: Looking beyond text: Extracting figures, tables and captions from com- puter science papers. In: Workshops at the 29th AAAI Conference on Artificial Intelligence (2015), https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10092.

* Dataset 2: ACL300, 300 randomly sampled articles (or 2508 pages) from the 55,759 papers scraped from the ACL anthology website; https://www.aclweb.org/anthology/.

* Dataset 3: VIS300, about 10% (or 2619 pages) of the document pages in randomly partitioned articles from 26,350 VIS paper pages published in  Chen, J., Ling, M., Li, R., Isenberg, P., Isenberg, T., Sedlmair, M., Möller, T., Laramee, R.S., Shen, H.W., Wünsche, K., Wang, Q.: VIS30K: A collection of figures and tables from IEEE visualization conference publications. IEEE Trans. Vis. Comput. Graph. 27 (2021), to appear doi: 10.1109/TVCG.2021.3054916.

This dataset is also available online at https://web.cse.ohio-state.edu/~chen.8028/ICDAR2021Benchmark/.

            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@data{ling2021visimages,
	author = {Meng Ling and Jian Chen and Torsten M{\"o}ller and Petra Isenberg and Tobias Isenberg and Michael Sedlmair and Robert Laramee and Han-Wei Shen and Jian Wu and Clyde Lee Giles},
	title = {Three Benchmark Datasets for Scholarly Article Layout Analysis},
	publisher = {IEEE Dataport},
	url = {https://ieee-dataport.org/open-access/three-benchmark-datasets-scholarly-article-layout-analysis},
	doi = {10.21227/326q-bf39},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papermorariu2021arxiv"
    >
        
            <img
                id="imagemorariu2021arxiv"
                title="Click to enlarge and show details"
                onclick="toggleClass('papermorariu2021arxiv', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/morariu2021arxiv.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papermorariu2021arxiv', 'small'); toggleImageSize(imagemorariu2021arxiv);"
                title="Click to show details"
            >
                DumbleDR: Predicting User Preferences of Dimensionality Reduction Projection Quality
            </h3>  <a class="anchor" name="morariu2021arxiv"></a>
            <div class="authors">
                <span class="firstAuthor">Cristina Morariu</span>,
                Adrien Bibal, Rene Cutura, Benoît Frénay, Michael Sedlmair
            </div>
            <div>
                <span class="publication">arXiv 2021</span>
                
                <a href="https://arxiv.org/pdf/2105.09275.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.48550/arXiv.2105.09275" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional parametrization (e.g. t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D projections is usually qualitatively decided, by setting projections side-by-side and letting human judgment decide which projection is the best. In this work, we propose a quantitative way of evaluating projections, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select 'good' and 'misleading' views between scatterplots of low-level projections of image datasets, simulating the way people usually select projections. We use the study data as labels for a set of quality metrics whose purpose is to discover and quantify what exactly people are looking for when deciding between projections. With this proxy for human judgments, we use it to rank projections on new datasets, explain why they are relevant, and quantify the degree of subjectivity in projections selected.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@techreport{morariu2021arxiv,
	title = {{DumbleDR}: Predicting User Preferences of Dimensionality Reduction Projection Quality},
	author = {Cristina Morariu and Adrien Bibal and Rene Cutura and Benoit Frenay and Michael Sedlmair},
	Institution = {{arXiv} preprint},
	Number = {arXiv:2105.09275},
	Type = {Technical Report},
	url = {https://arxiv.org/abs/2105.09275},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2021proseco"
    >
        
            <img
                id="imagebernard2021proseco"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2021proseco', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/bernard2021proseco.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2021proseco', 'small'); toggleImageSize(imagebernard2021proseco);"
                title="Click to show details"
            >
                ProSeCo: Visual analysis of class separation measures and dataset characteristics
            </h3>  <a class="anchor" name="bernard2021proseco"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                Marco Hutter, Matthias Zeppelzauer, Michael Sedlmair, Tamara Munzner
            </div>
            <div>
                <span class="publication">C&G 2021</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://doi.org/10.1016/j.cag.2021.03.004" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Class separation is an important concept in machine learning and visual analytics. We address the visual analysis of class separation measures for both high-dimensional data and its corresponding projections into 2D through dimensionality reduction (DR) methods. Although a plethora of separation measures have been proposed, it is difficult to compare class separation between multiple datasets with different characteristics, multiple separation measures, and multiple DR methods. We present ProSeCo, an interactive visualization approach to support comparison between up to 20 class separation measures and up to 4 DR methods, with respect to any of 7 dataset characteristics: dataset size, dataset dimensions, class counts, class size variability, class size skewness, outlieriness, and real-world vs. synthetically generated data. ProSeCo supports (1) comparing across measures, (2) comparing high-dimensional to dimensionally-reduced 2D data across measures, (3) comparing between different DR methods across measures, (4) partitioning with respect to a dataset characteristic, (5) comparing partitions for a selected characteristic across measures, and (6) inspecting individual datasets in detail. We demonstrate the utility of ProSeCo in two usage scenarios, using datasets [1] posted at https://osf.io/epcf9/.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{bernard2021proseco,
	author = {J{\"u}rgen Bernard and Marco Hutter and Matthias Zeppelzauer and Michael Sedlmair and Tamara Munzner},
	title = {{ProSeCo}: Visual analysis of class separation measures and dataset characteristics},
	journal = {Computers \& Graphics},
	publisher = {Elsevier},
  	volume = {96},
  	pages = {48--60},
  	year = {2021},
	url = {https://doi.org/10.1016/j.cag.2021.03.004},
  	doi = {10.1016/j.cag.2021.03.004}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperchen2021tvcg"
    >
        
            <img
                id="imagechen2021tvcg"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperchen2021tvcg', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/chen2021tvcg.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperchen2021tvcg', 'small'); toggleImageSize(imagechen2021tvcg);"
                title="Click to show details"
            >
                VIS30K: A collection of figures and tables from IEEE visualization conference publications
            </h3>  <a class="anchor" name="chen2021tvcg"></a>
            <div class="authors">
                <span class="firstAuthor">Jian Chen</span>,
                Meng Ling, Rui Li, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Torsten Möller, Robert S Laramee, Han-Wei Shen, Katharina Wünsche, Qiru Wang
            </div>
            <div>
                <span class="publication">TVCG 2021</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/chen2021tvcg.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2021.3054916" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present the VIS30K dataset, a collection of 29,689 images that represents 30 years of figures and tables from each track of the IEEE Visualization conference series (Vis, SciVis, InfoVis, VAST). VIS30K's comprehensive coverage of the scientific literature in visualization not only reflects the progress of the field but also enables researchers to study the evolution of the state-of-the-art and to find relevant work based on graphical content. We describe the dataset and our semi-automatic collection process, which couples convolutional neural networks (CNN) with curation. Extracting figures and tables semi-automatically allows us to verify that no images are overlooked or extracted erroneously. To improve quality further, we engaged in a peer-search process for high-quality figures from early IEEE Visualization papers. With the resulting data, we also contribute VISImageNavigator (VIN, visimagenavigator.github.io ), a web-based tool that facilitates searching and exploring VIS30K by author names, paper keywords, title and abstract, and years.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{chen2021tvcg,
	author = {Jian Chen and Meng Ling and Rui Li and Petra Isenberg and Tobias Isenberg and Michael Sedlmair and Torsten M{\"o}ller and Robert S Laramee and Han-Wei Shen and Katharina W{\"u}nsche and Qiru Wang},
	title = {{VIS30K}: A collection of figures and tables from {IEEE} visualization conference publications},
	journal = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},
  	year = {2021},
  	volume = {27},
  	number = {9},
  	pages = {3826--3833},
  	url = {https://doi.org/10.1109/TVCG.2021.3054916},
  	doi = {10.1109/TVCG.2021.3054916}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperwaldner2021iv"
    >
        
            <img
                id="imagewaldner2021iv"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperwaldner2021iv', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/waldner2021iv.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperwaldner2021iv', 'small'); toggleImageSize(imagewaldner2021iv);"
                title="Click to show details"
            >
                Linking unstructured evidence to structured observations
            </h3>  <a class="anchor" name="waldner2021iv"></a>
            <div class="authors">
                <span class="firstAuthor">Manuela Waldner</span>,
                Thomas Geymayer, Dieter Schmalstieg, Michael Sedlmair
            </div>
            <div>
                <span class="publication">IV 2021</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/waldner2021iv.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1177/1473871620986249" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Many professionals, like journalists, writers, or consultants, need to acquire information from various sources, make sense of this unstructured evidence, structure their observations, and finally create and deliver their product, such as a report or a presentation. In formative interviews, we found that tools allowing structuring of observations are often disconnected from the corresponding evidence. Therefore, we designed a sensemaking environment with a flexible observation graph that visually ties together evidence in unstructured documents with the user’s structured knowledge. This is achieved through bi-directional deep links between highlighted document portions and nodes in the observation graph. In a controlled study, we compared users’ sensemaking strategies using either the observation graph or a simple text editor on a large display. Results show that the observation graph represents a holistic, compact representation of users’ observations, which can be linked to unstructured evidence on demand. In contrast, users taking textual notes required much more display space to spatially organize source documents containing unstructured evidence. This implies that spatial organization is a powerful strategy to structure observations even if the available space is limited.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{waldner2021iv,
	author = {Manuela Waldner and Thomas Geymayer and Dieter Schmalstieg and Michael Sedlmair},
	title = {Linking unstructured evidence to structured observations},
	journal = {Information Visualization},
	publisher = {{SAGE} Publications},
	volume = {20},
    number = {1},
    pages = {47--65},
    url = {https://doi.org/10.1177/1473871620986249},
    doi = {10.1177/1473871620986249},
	year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2020
    </h2>
    
    <div
        class="paper small"
        id="papermerino2020ismar"
    >
        
            <img
                id="imagemerino2020ismar"
                title="Click to enlarge and show details"
                onclick="toggleClass('papermerino2020ismar', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/merino2020ismar.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papermerino2020ismar', 'small'); toggleImageSize(imagemerino2020ismar);"
                title="Click to show details"
            >
                Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009-2019)
            </h3>  <a class="anchor" name="merino2020ismar"></a>
            <div class="authors">
                <span class="firstAuthor">Leonel Merino</span>,
                Magdalena Schwarzl, Matthias Kraus, Michael Sedlmair, Dieter Schmalstieg, Daniel Weiskopf
            </div>
            <div>
                <span class="publication">ISMAR 2020</span>
                
                <a href="../pdf/merino2020ismar.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/ISMAR50242.2020.00069" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present a systematic review of 45S papers that report on evaluations in mixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST over a span of 11 years (2009-2019). Our goal is to provide guidance for future evaluations of MR/AR approaches. To this end, we characterize publications by paper type (e.g., technique, design study), research topic (e.g., tracking, rendering), evaluation scenario (e.g., algorithm performance, user performance), cognitive aspects (e.g., perception, emotion), and the context in which evaluations were conducted (e.g., lab vs. in-thewild). We found a strong coupling of types, topics, and scenarios. We observe two groups: (a) technology-centric performance evaluations of algorithms that focus on improving tracking, displays, reconstruction, rendering, and calibration, and (b) human-centric studies that analyze implications of applications and design, human factors on perception, usability, decision making, emotion, and attention. Amongst the 458 papers, we identified 248 user studies that involved 5,761 participants in total, of whom only 1,619 were identified as female. We identified 43 data collection methods used to analyze 10 cognitive aspects. We found nine objective methods, and eight methods that support qualitative analysis. A majority (216/248) of user studies are conducted in a laboratory setting. Often (138/248), such studies involve participants in a static way. However, we also found a fair number (30/248) of in-the-wild studies that involve participants in a mobile fashion. We consider this paper to be relevant to academia and industry alike in presenting the state-of-the-art and guiding the steps to designing, conducting, and analyzing results of evaluations in MR/AR.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@INPROCEEDINGS{9284762,
  author={Merino, Leonel and Schwarzl, Magdalena and Kraus, Matthias and Sedlmair, Michael and Schmalstieg, Dieter and Weiskopf, Daniel},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Evaluating Mixed and Augmented Reality: A Systematic Literature Review (2009-2019)}, 
  year={2020},
  volume={},
  number={},
  pages={438-451},
  doi={10.1109/ISMAR50242.2020.00069},
  ISSN={1554-7868},
  month={Nov},}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkraus2020ismar"
    >
        
            <img
                id="imagekraus2020ismar"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperkraus2020ismar', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/kraus2020ismar.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperkraus2020ismar', 'small'); toggleImageSize(imagekraus2020ismar);"
                title="Click to show details"
            >
                A Comparative Study of Orientation Support Tools in Virtual Reality Environments with Virtual Teleportation
            </h3>  <a class="anchor" name="kraus2020ismar"></a>
            <div class="authors">
                <span class="firstAuthor">Matthias Kraus</span>,
                Hanna Schaefer, Philipp Meschenmoser, Daniel Schweitzer, Daniel Keim, Michael Sedlmair, Johannes Fuchs
            </div>
            <div>
                <span class="publication">ISMAR 2020</span>
                
                <a href="../pdf/kraus2020ismar.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/ISMAR50242.2020.00046" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Movement-compensating interactions like teleportation are commonly deployed techniques in virtual reality environments. Although practical, they tend to cause disorientation while navigating. Previous studies show the effectiveness of orientation-supporting tools, such as trails, in reducing such disorientation and reveal different strengths and weaknesses of individual tools. However, to date, there is a lack of a systematic comparison of those tools when teleportation is used as a movement-compensating technique, in particular under consideration of different tasks. In this paper, we compare the effects of three orientation-supporting tools, namely minimap, trail, and heatmap. We conducted a quantitative user study with 48 participants to investigate the accuracy and efficiency when executing four exploration and search tasks. As dependent variables, task performance, completion time, space coverage, amount of revisiting, retracing time, and memorability were measured. Overall, our results indicate that orientation-supporting tools improve task completion times and revisiting behavior. The trail and heatmap tools were particularly useful for speed-focused tasks, minimal revisiting, and space coverage. The minimap increased memorability and especially supported retracing tasks. These results suggest that virtual reality systems should provide orientation aid tailored to the specific tasks of the users.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@INPROCEEDINGS{9284697,
  author={Kraus, Matthias and Schäfer, Hanna and Meschenmoser, Philipp and Schweitzer, Daniel and Keim, Daniel A. and Sedlmair, Michael and Fuchs, Johannes},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={A Comparative Study of Orientation Support Tools in Virtual Reality Environments with Virtual Teleportation}, 
  year={2020},
  volume={},
  number={},
  pages={227-238},
  doi={10.1109/ISMAR50242.2020.00046}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperboukhelifa2020eviva"
    >
        
            <img
                id="imageboukhelifa2020eviva"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperboukhelifa2020eviva', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/boukhelifa2020eviva.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperboukhelifa2020eviva', 'small'); toggleImageSize(imageboukhelifa2020eviva);"
                title="Click to show details"
            >
                Challenges in Evaluating Interactive Visual Machine Learning Systems
            </h3>  <a class="anchor" name="boukhelifa2020eviva"></a>
            <div class="authors">
                <span class="firstAuthor">Nadia Boukhelifa</span>,
                Anastasia Bezerianos, Remco Chang, Chris Collins, Steven Drucker, Alex Endert, Jessica Hullman, Chris North, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CG&A 2020</span>
                
                <a href="../pdf/boukhelifa2020eviva.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/MCG.2020.3017064" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In interactive visual machine learning (IVML), humans and machine learning algorithms collaborate to achieve tasks mediated by interactive visual interfaces. This human-in-the-loop approach to machine learning brings forth not only numerous intelligibility, trust, and usability issues, but also many open questions with respect to the evaluation of the IVML system, both as separate components, and as a holistic entity that includes both human and machine intelligence. This article describes the challenges and research gaps identified in an IEEE VIS workshop on the evaluation of IVML systems.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{9238590,
  author={Boukhelifa, N. and Bezerianos, A. and Chang, R. and Collins, C. and Drucker, S. and Endert, A. and Hullman, J. and North, C. and Sedlmair, M.},
  journal={IEEE Computer Graphics and Applications}, 
  title={Challenges in Evaluating Interactive Visual Machine Learning Systems}, 
  year={2020},
  volume={40},
  number={6},
  pages={88-96},
  doi={10.1109/MCG.2020.3017064}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperyu2020perspective"
    >
        
            <img
                id="imageyu2020perspective"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperyu2020perspective', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/yu2020perspective.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperyu2020perspective', 'small'); toggleImageSize(imageyu2020perspective);"
                title="Click to show details"
            >
                Perspective Matters: Design Implications for Motion Guidance in Mixed Reality
            </h3>  <a class="anchor" name="yu2020perspective"></a>
            <div class="authors">
                <span class="firstAuthor">Xingyao Yu</span>,
                Katrin Angerbauer, Peter Mohr, Denis Kalkofen, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2020</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/yu2020perspective.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/ISMAR50242.2020.00085" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We investigate how Mixed Reality (MR) can be used to guide human body motions, such as in physiotherapy, dancing, or workout applications. While first MR prototypes have shown promising results, many dimensions of the design space behind such applications remain largely unexplored. To better understand this design space, we approach the topic from different angles by contributing three user studies. In particular, we take a closer look at the influence of the perspective, the characteristics of motions, and visual guidance on different user performance measures. Our results indicate that a first-person perspective performs best for all visible motions, whereas the type of visual instruction plays a minor role. From our results we compile a set of considerations that can guide future work on the design of instructions, evaluations, and the technical setup of MR motion guidance systems.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2020comparing"
    >
        
            <img
                id="imagehube2020comparing"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2020comparing', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2020comparing.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2020comparing', 'small'); toggleImageSize(imagehube2020comparing);"
                title="Click to show details"
            >
                Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion
            </h3>  <a class="anchor" name="hube2020comparing"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Oliver Lenz, Lars Engeln, Rainer Groh, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2020</span>
                <span class="publication">Poster / Short Paper</span>
                
                <a href="https://doi.org/10.1109/ISMAR-Adjunct51615.2020.00023" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@INPROCEEDINGS{9288476,
  author={Hube, Natalie and Lenz, Oliver and Engeln, Lars and Groh, Rainer and Sedlmair, Michael},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion}, 
  year={2020},
  pages={30-35},
  doi={10.1109/ISMAR-Adjunct51615.2020.00023}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperheyen2020supporting"
    >
        
            <img
                id="imageheyen2020supporting"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperheyen2020supporting', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/heyen2020supporting.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperheyen2020supporting', 'small'); toggleImageSize(imageheyen2020supporting);"
                title="Click to show details"
            >
                Supporting Music Education through Visualizations of MIDI Recordings
            </h3>  <a class="anchor" name="heyen2020supporting"></a>
            <div class="authors">
                <span class="firstAuthor">Frank Heyen</span>,
                Michael Sedlmair
            </div>
            <div>
                <span class="publication">VIS 2020</span>
                <span class="publication">Poster</span>
                <a href="../pdf/heyen2020supporting.pdf" target="_blank">PDF</a>
                <a href="https://vis2020-ieee.ipostersessions.com/default.aspx?s=82-F0-FF-F9-29-B9-B4-7F-FE-F3-A9-1D-4A-B7-4F-32" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Musicians mostly have to rely on their ears when they want to analyze what they play, for example to detect errors. Since hearing is sequential, it is not possible to quickly grasp an overview over one or multiple recordings of a whole piece of music at once. We therefore propose various visualizations that allow analyzing errors and stylistic variance. Our current approach focuses on rhythm and uses MIDI data for simplicity.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbalestrucci2020pipelines"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperbalestrucci2020pipelines', 'small'); toggleImageSize(imagebalestrucci2020pipelines);"
                title="Click to show details"
            >
                Pipelines Bent, Pipelines Broken: Interdisciplinary Self-Reflection on the Impact of COVID-19 on Current and Future Research (Position Paper)
            </h3>  <a class="anchor" name="balestrucci2020pipelines"></a>
            <div class="authors">
                <span class="firstAuthor">Priscilla Balestrucci</span>,
                Katrin Angerbauer, Cristina Morariu, Robin Welsch, Lewis L Chuang, Daniel Weiskopf, Marc O Ernst, Michael Sedlmair
            </div>
            <div>
                <span class="publication">BELIV 2020</span>
                <span class="publication">Workshop</span>
                
                <a href="https://doi.org/10.1109/BELIV51497.2020.00009" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Among the many changes brought about by the COVID-19 pandemic, one of the most pressing for scientific research concerns user testing. For the researchers who conduct studies with human participants, the requirements for social distancing have created a need for reflecting on methodologies that previously seemed relatively straightforward. It has become clear from the emerging literature on the topic and from first-hand experiences of researchers that the restrictions due to the pandemic affect every aspect of the research pipeline. The current paper offers an initial reflection on user-based research, drawing on the authors' own experiences and on the results of a survey that was conducted among researchers in different disciplines, primarily psychology, human-computer interaction (HCI), and visualization communities. While this sampling of researchers is by no means comprehensive, the multi-disciplinary approach and the consideration of different aspects of the research pipeline allow us to examine current and future challenges for user-based research. Through an exploration of these issues, this paper also invites others in the VIS-as well as in the wider-research community, to reflect on and discuss the ways in which the current crisis might also present new and previously unexplored opportunities.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2020druidjs"
    >
        
            <img
                id="imagecutura2020druidjs"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2020druidjs', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/cutura2020druidjs.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2020druidjs', 'small'); toggleImageSize(imagecutura2020druidjs);"
                title="Click to show details"
            >
                DRUIDJS — A JavaScript Library for Dimensionality Reduction
            </h3>  <a class="anchor" name="cutura2020druidjs"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Christoph Kralj, Michael Sedlmair
            </div>
            <div>
                <span class="publication">VIS 2020</span>
                <span class="publication">Short Paper</span>
                <a href="../pdf/cutura2020druidjs.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/VIS47514.2020.00029" target="_blank">website</a>
                <a href="https://youtu.be/LyiqHl4rq34" target="_blank">video</a>
                <a href="https://renecutura.eu/pdfs/Druid_Supp.pdf" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Dimensionality reduction (DR) is a widely used technique for visualization. Nowadays, many of these visualizations are developed for the web, most commonly using JavaScript as the underlying programming language. So far, only few DR methods have a JavaScript implementation though, necessitating developers to write wrappers around implementations in other languages. In addition, those DR methods that exist in JavaScript libraries, such as PCA, t-SNE, and UMAP, do not offer consistent programming interfaces, hampering the quick integration of different methods. Toward a coherent and comprehensive DR programming framework, we developed an open source JavaScript library named DruidJS. Our library contains implementations of ten different DR algorithms, as well as the required linear algebra techniques, tools, and utilities.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2020druid,
  title={{DRUIDJS — A JavaScript Library for Dimensionality Reduction}},
  author={Cutura, Rene and Kralj, Christoph and Sedlmair, Michael},
  booktitle={2020 IEEE Visualization Conference (VIS)},
  pages={111--115},
  year={2020},
  organization={IEEE}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was supported by the BMVIT ICT of the Future program via the ViSciPub project (no. 867378) and handled by the FFG.
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperweiß2020revisited"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperweiß2020revisited', 'small'); toggleImageSize(imageweiß2020revisited);"
                title="Click to show details"
            >
                Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes
            </h3>  <a class="anchor" name="weiß2020revisited"></a>
            <div class="authors">
                <span class="firstAuthor">Maximilian Weiß</span>,
                Katrin Angerbauer, Alexandra Voit, Magdalena Schwarzl, Michael Sedlmair, Sven Mayer
            </div>
            <div>
                <span class="publication">VIS 2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://doi.org/10.1109/TVCG.2020.3030400" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkecheng2021palettaoilor"
    >
        
            <img
                id="imagekecheng2021palettaoilor"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperkecheng2021palettaoilor', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/kecheng2021palettaoilor.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperkecheng2021palettaoilor', 'small'); toggleImageSize(imagekecheng2021palettaoilor);"
                title="Click to show details"
            >
                Palettailor: Discriminable Colorization for Categorical Data
            </h3>  <a class="anchor" name="kecheng2021palettaoilor"></a>
            <div class="authors">
                <span class="firstAuthor">Kecheng Lu</span>,
                Mi Feng, Xin Chen, Michael Sedlmair, Oliver Deussen, Dani Lischinski, Zhanglin Cheng, Yunhai Wang
            </div>
            <div>
                <span class="publication">TVCG 2020</span>
                
                <a href="../pdf/kecheng2021palettaoilor.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2020.3030406" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present an integrated approach for creating and assigning color palettes to different visualizations such as multi-class scatterplots, line, and bar charts. While other methods separate the creation of colors from their assignment, our approach takes data characteristics into account to produce color palettes, which are then assigned in a way that fosters better visual discrimination of classes. To do so, we use a customized optimization based on simulated annealing to maximize the combination of three carefully designed color scoring functions: point distinctness, name difference, and color discrimination. We compare our approach to state-of-the-art palettes with a controlled user study for scatterplots and line charts, furthermore we performed a case study. Our results show that Palettailor, as a fully-automated approach, generates color palettes with a higher discrimination quality than existing approaches. The efficiency of our optimization allows us also to incorporate user modifications into the color selection process.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{9222351,
  author={Lu, Kecheng and Feng, Mi and Chen, Xin and Sedlmair, Michael and Deussen, Oliver and Lischinski, Dani and Cheng, Zhanglin and Wang, Yunhai},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Palettailor: Discriminable Colorization for Categorical Data}, 
  year={2021},
  volume={27},
  number={2},
  pages={475-484},
  doi={10.1109/TVCG.2020.3030406}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbu2021sinestream"
    >
        
            <img
                id="imagebu2021sinestream"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbu2021sinestream', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/bu2021sinestream.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbu2021sinestream', 'small'); toggleImageSize(imagebu2021sinestream);"
                title="Click to show details"
            >
                SineStream: Improving the Readability of Streamgraphs by Minimizing Sine Illusion Effects
            </h3>  <a class="anchor" name="bu2021sinestream"></a>
            <div class="authors">
                <span class="firstAuthor">Chuan Bu</span>,
                Quanjie Zhang, Qianwen Wang, Jian Zhang, Oliver Deussen, Michael Sedlmair, Yunhai Wang
            </div>
            <div>
                <span class="publication">TVCG 2020</span>
                
                <a href="../pdf/bu2021sinestream.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2020.3030404" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we propose SineStream, a new variant of streamgraphs that improves their readability by minimizing sine illusion effects. Such effects reflect the tendency of humans to take the orthogonal rather than the vertical distance between two curves as their distance. In SineStream, we connect the readability of streamgraphs with minimizing sine illusions and by doing so provide a perceptual foundation for their design. As the geometry of a streamgraph is controlled by its baseline (the bottom-most curve) and the ordering of the layers, we re-interpret baseline computation and layer ordering algorithms in terms of reducing sine illusion effects. For baseline computation, we improve previous methods by introducing a Gaussian weight to penalize layers with large thickness changes. For layer ordering, three design requirements are proposed and implemented through a hierarchical clustering algorithm. Quantitative experiments and user studies demonstrate that SineStream improves the readability and aesthetics of streamgraphs compared to state-of-the-art methods.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{9222035,
  author={Bu, Chuan and Zhang, Quanjie and Wang, Qianwen and Zhang, Jian and Sedlmair, Michael and Deussen, Oliver and Wang, Yunhai},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={SineStream: Improving the Readability of Streamgraphs by Minimizing Sine Illusion Effects}, 
  year={2021},
  volume={27},
  number={2},
  pages={1634-1643},
  doi={10.1109/TVCG.2020.3030404}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperheyen2020clavis"
    >
        
            <img
                id="imageheyen2020clavis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperheyen2020clavis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/heyen2020clavis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperheyen2020clavis', 'small'); toggleImageSize(imageheyen2020clavis);"
                title="Click to show details"
            >
                ClaVis: An Interactive Visual Comparison System for Classifiers
            </h3>  <a class="anchor" name="heyen2020clavis"></a>
            <div class="authors">
                <span class="firstAuthor">Frank Heyen</span>,
                Tanja Munz, Michael Neumann, Daniel Ortega, Ngoc Thang Vu, Daniel Weiskopf, Michael Sedlmair
            </div>
            <div>
                <span class="publication">AVI 2020</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/heyen2020clavis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3399715.3399814" target="_blank">website</a>
                
                <a href="https://github.com/fheyen/clavis" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose ClaVis, a visual analytics system for comparative analysis of classification models. ClaVis allows users to visually compare the performance and behavior of tens to hundreds of classifiers trained with different hyperparameter configurations. Our approach is plugin-based and classifier-agnostic and allows users to add their own datasets and classifier implementations. It provides multiple visualizations, including a multivariate ranking, a similarity map, a scatterplot that reveals correlations between parameters and scores, and a training history chart. We demonstrate the effectivity of our approach in multiple case studies for training classification models in the domain of natural language processing.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.1145/3399715.3399814,
author = {Heyen, Frank and Munz, Tanja and Neumann, Michael and Ortega, Daniel and Vu, Ngoc Thang and Weiskopf, Daniel and Sedlmair, Michael},
title = {ClaVis: An Interactive Visual Comparison System for Classifiers},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399814},
doi = {10.1145/3399715.3399814},
abstract = {We propose ClaVis, a visual analytics system for comparative analysis of classification
models. ClaVis allows users to visually compare the performance and behavior of tens
to hundreds of classifiers trained with different hyperparameter configurations. Our
approach is plugin-based and classifier-agnostic and allows users to add their own
datasets and classifier implementations. It provides multiple visualizations, including
a multivariate ranking, a similarity map, a scatterplot that reveals correlations
between parameters and scores, and a training history chart. We demonstrate the effectivity
of our approach in multiple case studies for training classification models in the
domain of natural language processing.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {9},
numpages = {9},
keywords = {visual analytics, Visualization, machine learning, classifier comparison},
location = {Salerno, Italy},
series = {AVI '20}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (A08) and under Germany’s Excellence Strategy – EXC-2075 – 39074001
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2020comparing"
    >
        
            <img
                id="imagecutura2020comparing"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2020comparing', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/cutura2020comparing.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2020comparing', 'small'); toggleImageSize(imagecutura2020comparing);"
                title="Click to show details"
            >
                Comparing and Exploring High-Dimensional Data with Dimensionality Reduction Algorithms and Matrix Visualizations
            </h3>  <a class="anchor" name="cutura2020comparing"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Michaël Aupetit, Jean-Daniel Fekete, Michael Sedlmair
            </div>
            <div>
                <span class="publication">AVI  2020</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/cutura2020comparing.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3399715.3399875" target="_blank">website</a>
                <a href="https://youtu.be/UPkH7rc0ulU" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose Compadre, a tool for visual analysis for comparing distances of high-dimensional (HD) data and their low-dimensional projections. At the heart is a matrix visualization to represent the discrepancy between distance matrices, linked side-by-side with 2D scatterplot projections of the data. Using different examples and datasets, we illustrate how this approach fosters (1) evaluating dimensionality reduction techniques w.r.t. how well they project the HD data, (2) comparing them to each other side-by-side, and (3) evaluate important data features through subspace comparison. We also present a case study, in which we analyze IEEE VIS authors from 1990 to 2018, and gain new insights on the relationships between coauthors, citations, and keywords. The coauthors are projected as accurately with UMAP as with t-SNE but the projections show different insights. The structure of the citation subspace is very different from the coauthor subspace. The keyword subspace is noisy yet consistent among the three IEEE VIS sub-conferences.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2020comparing,
  title={Comparing and exploring high-dimensional data with dimensionality reduction algorithms and matrix visualizations},
  author={Cutura, Rene and Aupetit, Micha{\"e}l and Fekete, Jean-Daniel and Sedlmair, Michael},
  booktitle={Proc. Intl. Conf. on Advanced Visual Interfaces (AVI)},
  pages={1--9},
  year={2020},
  doi={10.1145/3399715.3399875}}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was supported by the BMVIT ICT of the Future program via the ViSciPub project (no. 867378) and handled by the FFG.
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperachberger2020caarvida"
    >
        
            <img
                id="imageachberger2020caarvida"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperachberger2020caarvida', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/achberger2020caarvida.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperachberger2020caarvida', 'small'); toggleImageSize(imageachberger2020caarvida);"
                title="Click to show details"
            >
                Caarvida: Visual Analytics for Test Drive Videos
            </h3>  <a class="anchor" name="achberger2020caarvida"></a>
            <div class="authors">
                <span class="firstAuthor">Alexander Achberger</span>,
                Rene Cutura, Oguzhan Türksoy, Michael Sedlmair
            </div>
            <div>
                <span class="publication">AVI  2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://doi.org/10.1145/3399715.3399862" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.1145/3399715.3399862,
author = {Achberger, Alexander and Cutura, Ren\'{e} and T\"{u}rksoy, Oguzhan and Sedlmair, Michael},
title = {Caarvida: Visual Analytics for Test Drive Videos},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399862},
doi = {10.1145/3399715.3399862},
abstract = {We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {6},
numpages = {9},
keywords = {visual analytics, object detection, automotive, information visualization, human computer interaction},
location = {Salerno, Italy},
series = {AVI '20}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperschatz2020scivis"
    >
        
            <img
                id="imageschatz2020scivis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperschatz2020scivis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/schatz2020scivis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperschatz2020scivis', 'small'); toggleImageSize(imageschatz2020scivis);"
                title="Click to show details"
            >
                2019 IEEE Scientific Visualization Contest Winner: Visual Analysis of Structure Formation in Cosmic Evolution
            </h3>  <a class="anchor" name="schatz2020scivis"></a>
            <div class="authors">
                <span class="firstAuthor">Karsten Schatz</span>,
                Christoph Müller, Patrick Gralka, Moritz Heinemann, Alexander Straub, Christoph Schulz, Matthias Braun, Tobias Rau, Michael Becher, Steffen Frey,  Guido Reina, Michael Sedlmair, others
            </div>
            <div>
                <span class="publication">CG&A 2020</span>
                
                <a href="../pdf/schatz2020scivis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/MCG.2020.3004613" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Simulations of cosmic evolution are a means to explain the formation of the universe as we see it today. The resulting data of such simulations comprise numerous physical quantities, which turns their analysis into a complex task. Here, we analyze such high-dimensional and time-varying particle data using various visualization techniques from the fields of particle visualization, flow visualization, volume visualization, and information visualization. Our approach employs specialized filters to extract and highlight the development of so-called active galactic nuclei and filament structures formed by the particles. Additionally, we calculate X-ray emission of the evolving structures in a preprocessing step to complement visual analysis. Our approach is integrated into a single visual analytics framework to allow for analysis of star formation at interactive frame rates. Finally, we lay out the methodological aspects of our work that led to success at the 2019 IEEE SciVis Contest.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{schatz2020scivis,
   title={2019 {IEEE} {S}cientific {V}isualization Contest Winner: Visual Analysis of Structure Formation in Cosmic Evolution},
   author={Schatz, Karsten and M{\"u}ller, Christoph and Gralka, Patrick and Heinemann, Moritz and Straub, Alexander and Schulz, Christoph and Braun, Matthias and Rau, Tobias and Becher, Michael and Frey, Steffen and Reina, Guido and Sedlmair, Michael and others},
   journal={IEEE Computer Graphics and Applications (CG\&A)},
   volume={41},
    number={6},
    pages={101--110},
doi={10.1109/MCG.2020.3004613},
year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperstreichert2020comparing"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperstreichert2020comparing', 'small'); toggleImageSize(imagestreichert2020comparing);"
                title="Click to show details"
            >
                Comparing Input Modalities for Shape Drawing Tasks
            </h3>  <a class="anchor" name="streichert2020comparing"></a>
            <div class="authors">
                <span class="firstAuthor">Annalena Streichert</span>,
                Katrin Angerbauer, Magdalena Schwarzl, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ETVIS 2020</span>
                <span class="publication">Workshop / Short Paper</span>
                
                <a href="https://doi.org/10.1145/3379156.3391830" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N= 20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperpathmanathan2020etra"
    >
        
            <img
                id="imagepathmanathan2020etra"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperpathmanathan2020etra', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/pathmanathan2020etra.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperpathmanathan2020etra', 'small'); toggleImageSize(imagepathmanathan2020etra);"
                title="Click to show details"
            >
                Eye vs. Head: Comparing Gaze Methods for Interaction in AR
            </h3>  <a class="anchor" name="pathmanathan2020etra"></a>
            <div class="authors">
                <span class="firstAuthor">Nelusa Pathmanathan</span>,
                Michael Becher, Nils Rodrigues, Guido Reina, Thomas Ertl, Daniel Weiskopf, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ETRA 2020</span>
                
                <a href="../pdf/pathmanathan2020etra.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3379156.3391829" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Visualization in virtual 3D environments can provide a natural way for users to explore data. Often, arm and short head movements are required for interaction in augmented reality, which can be tiring and strenuous though. In an effort toward more user-friendly interaction, we developed a prototype that allows users to manipulate virtual objects using a combination of eye gaze and an external clicker device. Using this prototype, we performed a user study comparing four different input methods of which head gaze plus clicker was preferred by most participants.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.1145/3379156.3391829,
author = {Pathmanathan, Nelusa and Becher, Michael and Rodrigues, Nils and Reina, Guido and Ertl, Thomas and Weiskopf, Daniel and Sedlmair, Michael},
title = {Eye vs.&nbsp;Head: Comparing Gaze Methods for Interaction in Augmented Reality},
year = {2020},
isbn = {9781450371346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379156.3391829},
doi = {10.1145/3379156.3391829},
abstract = {Visualization in virtual 3D environments can provide a natural way for users to explore data. Often, arm and short head movements are required for interaction in augmented reality, which can be tiring and strenuous though. In an effort toward more user-friendly interaction, we developed a prototype that allows users to manipulate virtual objects using a combination of eye gaze and an external clicker device. Using this prototype, we performed a user study comparing four different input methods of which head gaze plus clicker was preferred by most participants.},
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {50},
numpages = {5},
keywords = {Immersive analytics, interaction, visualization, eye tracking, augmented reality},
location = {Stuttgart, Germany},
series = {ETRA '20 Short Papers}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperoeney2020etra"
    >
        
            <img
                id="imageoeney2020etra"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperoeney2020etra', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/oeney2020etra.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperoeney2020etra', 'small'); toggleImageSize(imageoeney2020etra);"
                title="Click to show details"
            >
                Evaluation of Gaze Depth Estimation from Eye Tracking in Augmented Reality
            </h3>  <a class="anchor" name="oeney2020etra"></a>
            <div class="authors">
                <span class="firstAuthor">Seyda Öney</span>,
                Nils Rodrigues, Michael Becher, Guido Reina, Thomas Ertl, Michael Sedlmair, Daniel Weiskopf
            </div>
            <div>
                <span class="publication">ETRA 2020</span>
                
                <a href="../pdf/oeney2020etra.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3379156.3391835" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Gaze tracking in 3D has the potential to improve interaction with objects and visualizations in augmented reality. However, previous research showed that subjective perception of distance varies between real and virtual surroundings. We wanted to determine whether objectively measured 3D gaze depth through eye tracking also exhibits differences between entirely real and augmented environments. To this end, we conducted an experiment (N = 25) in which we used Microsoft HoloLens with a binocular eye tracking add-on from Pupil Labs. Participants performed a task that required them to look at stationary real and virtual objects while wearing a HoloLens device. We were not able to find significant differences in the gaze depth measured by eye tracking. Finally, we discuss our findings and their implications for gaze interaction in immersive analytics, and the quality of the collected gaze data. 
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.1145/3379156.3391835,
author = {Oney, Seyda and Rodrigues, Nils and Becher, Michael and Ertl, Thomas and Reina, Guido and Sedlmair, Michael and Weiskopf, Daniel},
title = {Evaluation of Gaze Depth Estimation from Eye Tracking in Augmented Reality},
year = {2020},
isbn = {9781450371346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379156.3391835},
doi = {10.1145/3379156.3391835},
abstract = {Gaze tracking in 3D has the potential to improve interaction with objects and visualizations in augmented reality. However, previous research showed that subjective perception of distance varies between real and virtual surroundings. We wanted to determine whether objectively measured 3D gaze depth through eye tracking also exhibits differences between entirely real and augmented environments. To this end, we conducted an experiment (N = 25) in which we used Microsoft HoloLens with a binocular eye tracking add-on from Pupil Labs. Participants performed a task that required them to look at stationary real and virtual objects while wearing a HoloLens device. We were not able to find significant differences in the gaze depth measured by eye tracking. Finally, we discuss our findings and their implications for gaze interaction in immersive analytics, and the quality of the collected gaze data. },
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {49},
numpages = {5},
keywords = {eye tracking, visualization, Augmented reality, depth perception, immersive analytics, user study},
location = {Stuttgart, Germany},
series = {ETRA '20 Short Papers}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papermerino2020toward"
    >
        
            <img
                id="imagemerino2020toward"
                title="Click to enlarge and show details"
                onclick="toggleClass('papermerino2020toward', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/merino2020toward.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papermerino2020toward', 'small'); toggleImageSize(imagemerino2020toward);"
                title="Click to show details"
            >
                Toward Agile Situated Visualization: An Exploratory User Study
            </h3>  <a class="anchor" name="merino2020toward"></a>
            <div class="authors">
                <span class="firstAuthor">Leonel Merino</span>,
                Boris Sotomayor-Gómez, Xingyao Yu, Ronie Salgado, Alexandre Bergel, Michael Sedlmair, Daniel Weiskopf
            </div>
            <div>
                <span class="publication">CHI 2020</span>
                <span class="publication">Extended Abstract</span>
                <a href="../pdf/merino2020toward.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3334480.3383017" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median ≥ 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkurzhals2020chi"
    >
        
            <img
                id="imagekurzhals2020chi"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperkurzhals2020chi', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/kurzhals2020chi.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperkurzhals2020chi', 'small'); toggleImageSize(imagekurzhals2020chi);"
                title="Click to show details"
            >
                A View on the Viewer: Gaze-Adaptive Captions for Videos
            </h3>  <a class="anchor" name="kurzhals2020chi"></a>
            <div class="authors">
                <span class="firstAuthor">Kuno Kurzhals</span>,
                Fabian Göbel, Katrin Angerbauer, Michael Sedlmair, Martin Raubal
            </div>
            <div>
                <span class="publication">CHI 2020</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/kurzhals2020chi.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3313831.3376266" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkraus2020chi"
    >
        
            <img
                id="imagekraus2020chi"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperkraus2020chi', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/kraus2020chi.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperkraus2020chi', 'small'); toggleImageSize(imagekraus2020chi);"
                title="Click to show details"
            >
                Assessing 2D and 3D Heatmaps for Comparative Analysis: An Empirical Study
            </h3>  <a class="anchor" name="kraus2020chi"></a>
            <div class="authors">
                <span class="firstAuthor">Matthias Kraus</span>,
                Katrin Angerbauer, Juri Buchmüller, Daniel Schweitzer, Daniel A Keim, Michael Sedlmair, Johannes Fuchs
            </div>
            <div>
                <span class="publication">CHI 2020</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/kraus2020chi.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3313831.3376675" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Heatmaps are a popular visualization technique that encode 2D density distributions using color or brightness. Experimental studies have shown though that both of these visual variables are inaccurate when reading and comparing numeric data values. A potential remedy might be to use 3D heatmaps by introducing height as a third dimension to encode the data. Encoding abstract data in 3D, however, poses many problems, too. To better understand this tradeoff, we conducted an empirical study (N=48) to evaluate the user performance of 2D and 3D heatmaps for comparative analysis tasks. We test our conditions on a conventional 2D screen, but also in a virtual reality environment to allow for real stereoscopic vision. Our main results show that 3D heatmaps are superior in terms of error rate when reading and comparing single data items. However, for overview tasks, the well-established 2D heatmap performs better.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperwang2020scag"
    >
        
            <img
                id="imagewang2020scag"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperwang2020scag', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/wang2020scag.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperwang2020scag', 'small'); toggleImageSize(imagewang2020scag);"
                title="Click to show details"
            >
                Improving the Robustness of Scagnostics
            </h3>  <a class="anchor" name="wang2020scag"></a>
            <div class="authors">
                <span class="firstAuthor">Yunhai Wang</span>,
                Zeyu Wang, Tingting Liu, Michael Correll, Zhanglin Cheng, Oliver Deussen, Michael Sedlmair
            </div>
            <div>
                <span class="publication">TVCG 2020</span>
                
                <a href="../pdf/wang2020scag.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2019.2934796" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{8807247,
  author={Wang, Yunhai and Wang, Zeyu and Liu, Tingting and Correll, Michael and Cheng, Zhanglin and Deussen, Oliver and Sedlmair, Michael},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Improving the Robustness of Scagnostics}, 
  year={2020},
  volume={26},
  number={1},
  pages={759-769},
  doi={10.1109/TVCG.2019.2934796}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2020eurova"
    >
        
            <img
                id="imagebernard2020eurova"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2020eurova', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/bernard2020eurova.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2020eurova', 'small'); toggleImageSize(imagebernard2020eurova);"
                title="Click to show details"
            >
                SepEx: Visual Analysis of Class Separation Measures
            </h3>  <a class="anchor" name="bernard2020eurova"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                Marco Hutter, Matthias Zeppelzauer, Michael Sedlmair, Tamara Munzner
            </div>
            <div>
                <span class="publication">EuroVA 2020</span>
                
                <a href="../pdf/bernard2020eurova.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.2312/eurova.20201079" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Class separation is an important concept in machine learning and visual analytics. However, the comparison of class separation for datasets with varying dimensionality is non-trivial, given a) the various possible structural characteristics of datasets and b) the plethora of separation measures that exist. Building upon recent findings in visualization research about the qualitative and quantitative evaluation of class separation for 2D dimensionally reduced data using scatterplots, this research addresses the visual analysis of class separation measures for high-dimensional data. We present SepEx, an interactive visualization approach for the assessment and comparison of class separation measures for multiple datasets. SepEx supports analysts with the comparison of multiple separation measures over many high-dimensional datasets, the effect of dimensionality reduction on measure outputs by supporting nD to 2D comparison, and the comparison of the effect of different dimensionality reduction methods on measure outputs. We demonstrate SepEx in a scenario on 100 two-class 5D datasets with a linearly increasing amount of separation between the classes, illustrating both similarities and nonlinearities across 11 measures.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings {10.2312:eurova.20201079,
booktitle = {EuroVis Workshop on Visual Analytics (EuroVA)},
editor = {Turkay, Cagatay and Vrotsou, Katerina},
title = {{SepEx: Visual Analysis of Class Separation Measures}},
author = {Bernard, Jürgen and Hutter, Marco and Zeppelzauer, Matthias and Sedlmair, Michael and Munzner, Tamara},
year = {2020},
publisher = {The Eurographics Association},
ISSN = {2664-4487},
ISBN = {978-3-03868-116-8},
DOI = {10.2312/eurova.20201079}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2019
    </h2>
    
    <div
        class="paper small"
        id="paperaupetit2019vis"
    >
        
            <img
                id="imageaupetit2019vis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperaupetit2019vis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/aupetit2019vis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperaupetit2019vis', 'small'); toggleImageSize(imageaupetit2019vis);"
                title="Click to show details"
            >
                Toward Perception-based Evaluation of Clustering Techniques for Visual Analytics
            </h3>  <a class="anchor" name="aupetit2019vis"></a>
            <div class="authors">
                <span class="firstAuthor">Michael Aupetit</span>,
                Michael Sedlmair, Mostafa Abbas, Abdelkader Baggag, Halima Bensmail
            </div>
            <div>
                <span class="publication">VIS 2019</span>
                <span class="publication">Short Paper</span>
                <a href="../pdf/aupetit2019vis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/VISUAL.2019.8933620" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Automatic clustering techniques play a central role in Visual Analytics by helping analysts to discover interesting patterns in high-dimensional data. Evaluating these clustering techniques, however, is difficult due to the lack of universal ground truth. Instead, clustering approaches are usually evaluated based on a subjective visual judgment of low-dimensional scatterplots of different datasets. As clustering is an inherent human-in-the-loop task, we propose a more systematic way of evaluating clustering algorithms based on quantification of human perception of clusters in 2D scatterplots. The core question we are asking is in how far existing clustering techniques align with clusters perceived by humans. To do so, we build on a dataset from a previous study [1], in which 34 human subjects la-beled 1000 synthetic scatterplots in terms of whether they could see one or more than one cluster. Here, we use this dataset to benchmark state-of-the-art clustering techniques in terms of how far they agree with these human judgments. More specifically, we assess 1437 variants of K-means, Gaussian Mixture Models, CLIQUE, DBSCAN, and Agglomerative Clustering techniques on these benchmarks data. We get unexpected results. For instance, CLIQUE and DBSCAN are at best in slight agreement on this basic cluster counting task, while model-agnostic Agglomerative clustering can be up to a substantial agreement with human subjects depending on the variants. We discuss how to extend this perception-based clustering benchmark approach, and how it could lead to the design of perception-based clustering techniques that would better support more trustworthy and explainable models of cluster patterns.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@INPROCEEDINGS{8933620,
  author={Aupetit, Michaël and Sedlmair, Michael and Abbas, Mostafa M. and Baggag, Abdelkader and Bensmail, Halima},
  booktitle={2019 IEEE Visualization Conference (VIS)}, 
  title={Toward Perception-Based Evaluation of Clustering Techniques for Visual Analytics}, 
  year={2019},
  pages={141-145},
  doi={10.1109/VISUAL.2019.8933620}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbrandes2019netvis"
    >
        
            <img
                id="imagebrandes2019netvis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbrandes2019netvis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/brandes2019netvis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbrandes2019netvis', 'small'); toggleImageSize(imagebrandes2019netvis);"
                title="Click to show details"
            >
                Network Visualization
            </h3>  <a class="anchor" name="brandes2019netvis"></a>
            <div class="authors">
                <span class="firstAuthor">Ulrik Brandes</span>,
                Michael Sedlmair
            </div>
            <div>
                <span class="publication">In Network Science - An Aerial View, Springer 2019</span>
                <span class="publication">Book Chapter</span>
                <a href="../pdf/brandes2019netvis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1007/978-3-030-26814-5_2" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Data visualization is the art and science of mapping data to graphical variables. In this context, networks give rise to unique difficulties because of inherent dependencies among their elements. We provide a high-level overview of the main challenges and common techniques to address them. They are illustrated with examples from two application domains, social networks and automotive engineering. The chapter concludes with opportunities for future work in network visualization.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@Inbook{Brandes2019,
author="Brandes, Ulrik
and Sedlmair, Michael",
editor="Biagini, Francesca
and Kauermann, G{\"o}ran
and Meyer-Brandis, Thilo",
title="Network Visualization",
bookTitle="Network Science: An Aerial View",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="5--21",
abstract="Data visualization is the art and science of mapping data to graphical variables. In this context, networks give rise to unique difficulties because of inherent dependencies among their elements. We provide a high-level overview of the main challenges and common techniques to address them. They are illustrated with examples from two application domains, social networks and automotive engineering. The chapter concludes with opportunities for future work in network visualization.",
isbn="978-3-030-26814-5",
doi="10.1007/978-3-030-26814-5_2",
url="https://doi.org/10.1007/978-3-030-26814-5_2"
}

</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papersippl2019tr"
    >
        
            <img
                id="imagesippl2019tr"
                title="Click to enlarge and show details"
                onclick="toggleClass('papersippl2019tr', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/sippl2019tr.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papersippl2019tr', 'small'); toggleImageSize(imagesippl2019tr);"
                title="Click to show details"
            >
                Collecting and Structuring Information in the Information Collage
            </h3>  <a class="anchor" name="sippl2019tr"></a>
            <div class="authors">
                <span class="firstAuthor">Sebastian Sippl</span>,
                Michael Sedlmair, Manuela Waldner
            </div>
            <div>
                <span class="publication">arXiv 2019</span>
                <span class="publication">Technical Report</span>
                <a href="../pdf/sippl2019tr.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.48550/arXiv.1909.00608" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Knowledge workers, such as scientists, journalists, or consultants, adaptively seek, gather, and consume information. These processes are often inefficient as existing user interfaces provide limited possibilities to combine information from various sources and different formats into a common knowledge representation. In this paper, we present the concept of an information collage (IC) -- a web browser extension combining manual spatial organization of gathered information fragments and automatic text analysis for interactive content exploration and expressive visual summaries. We used IC for case studies with knowledge workers from different domains and longer-term field studies over a period of one month. We identified three different ways how users collect and structure information and provide design recommendations how to support these observed usage strategies. 
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{DBLP:journals/corr/abs-1909-00608,
  author    = {Sebastian Sippl and
               Michael Sedlmair and
               Manuela Waldner},
  title     = {Collecting and Structuring Information in the Information Collage},
  journal   = {CoRR},
  volume    = {abs/1909.00608},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.00608},
  eprinttype = {arXiv},
  eprint    = {1909.00608},
  timestamp = {Mon, 16 Sep 2019 17:27:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-00608.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papersperrle2018speculative"
    >
        
            <img
                id="imagesperrle2018speculative"
                title="Click to enlarge and show details"
                onclick="toggleClass('papersperrle2018speculative', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/sperrle2018speculative.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papersperrle2018speculative', 'small'); toggleImageSize(imagesperrle2018speculative);"
                title="Click to show details"
            >
                Speculative Execution for Guided Visual Analytics
            </h3>  <a class="anchor" name="sperrle2018speculative"></a>
            <div class="authors">
                <span class="firstAuthor">Fabian Sperrle</span>,
                Jürgen Bernard, Michael Sedlmair, Daniel Keim, Mennatallah El-Assady
            </div>
            <div>
                <span class="publication">VIS 2019</span>
                <span class="publication">Workshop Paper</span>
                <a href="../pdf/sperrle2018speculative.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.48550/arXiv.1908.02627" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose the concept of Speculative Execution for Visual Analytics and discuss its effectiveness for model exploration and optimization. Speculative Execution enables the automatic generation of alternative, competing model configurations that do not alter the current model state unless explicitly confirmed by the user. These alternatives are computed based on either user interactions or model quality measures and can be explored using delta-visualizations. By automatically proposing modeling alternatives, systems employing Speculative Execution can shorten the gap between users and models, reduce the confirmation bias and speed up optimization processes. In this paper, we have assembled five application scenarios showcasing the potential of Speculative Execution, as well as a potential for further research. 
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{DBLP:journals/corr/abs-1908-02627,
  author    = {Fabian Sperrle and
               J{\"{u}}rgen Bernard and
               Michael Sedlmair and
               Daniel A. Keim and
               Mennatallah El{-}Assady},
  title     = {Speculative Execution for Guided Visual Analytics},
  journal   = {CoRR},
  volume    = {abs/1908.02627},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.02627},
  eprinttype = {arXiv},
  eprint    = {1908.02627},
  timestamp = {Fri, 09 Aug 2019 12:15:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-02627.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperstoiber2019eurovis"
    >
        
            <img
                id="imagestoiber2019eurovis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperstoiber2019eurovis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/stoiber2019eurovis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperstoiber2019eurovis', 'small'); toggleImageSize(imagestoiber2019eurovis);"
                title="Click to show details"
            >
                netflower: Dynamic Network Visualization for Data Journalists
            </h3>  <a class="anchor" name="stoiber2019eurovis"></a>
            <div class="authors">
                <span class="firstAuthor">Christina Stoiber</span>,
                Alexander Rind, Florian Grassinger, Robert Gutounig, Eva Goldgruber, Michael Sedlmair, Stefan Emrich, Wolfgang Aigner
            </div>
            <div>
                <span class="publication">CGF 2019</span>
                
                <a href="../pdf/stoiber2019eurovis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1111/cgf.13721" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Journalists need visual interfaces that cater to the exploratory nature of their investigative activities. In this paper, we report on a four-year design study with data journalists. The main result is netflower, a visual exploration tool that supports journalists in investigating quantitative flows in dynamic network data for story-finding. The visual metaphor is based on Sankey diagrams and has been extended to make it capable of processing large amounts of input data as well as network change over time. We followed a structured, iterative design process including requirement analysis and multiple design and prototyping iterations in close cooperation with journalists. To validate our concept and prototype, a workshop series and two diary studies were conducted with journalists. Our findings indicate that the prototype can be picked up quickly by journalists and valuable insights can be achieved in a few hours. The prototype can be accessed at: http://netflower.fhstp.ac.at/
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperabbas2019eurovis"
    >
        
            <img
                id="imageabbas2019eurovis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperabbas2019eurovis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/abbas2019eurovis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperabbas2019eurovis', 'small'); toggleImageSize(imageabbas2019eurovis);"
                title="Click to show details"
            >
                ClustMe: A Visual Quality Measure for Ranking Monochrome Scatterplots based on Cluster Patterns
            </h3>  <a class="anchor" name="abbas2019eurovis"></a>
            <div class="authors">
                <span class="firstAuthor">Mostafa Abbas</span>,
                Michael Aupetit, Michael Sedlmair, Halima Bensmail
            </div>
            <div>
                <span class="publication">CGF 2019</span>
                
                <a href="../pdf/abbas2019eurovis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1111/cgf.13684" target="_blank">website</a>
                
                <a href="../suppl/abbas2019eurovis.zip" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose ClustMe, a new visual quality measure to rank monochrome scatterplots based on cluster patterns. ClustMe is based on data collected from a human-subjects study, in which 34 participants judged synthetically generated cluster patterns in 1000 scatterplots. We generated these patterns by carefully varying the free parameters of a simple Gaussian Mixture Model with two components, and asked the participants to count the number of clusters they could see (1 or more than 1). Based on the results, we form ClustMe by selecting the model that best predicts these human judgments among 7 different state-of-the-art merging techniques (Demp). To quantitatively evaluate ClustMe, we conducted a second study, in which 31 human subjects ranked 435 pairs of scatterplots of real and synthetic data in terms of cluster patterns complexity. We use this data to compare ClustMe's performance to 4 other state-of-the-art clustering measures, including the well-known Clumpiness scagnostics. We found that of all measures, ClustMe is in strongest agreement with the human rankings.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperzhao2019barperception"
    >
        
            <img
                id="imagezhao2019barperception"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperzhao2019barperception', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/zhao2019barperception.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperzhao2019barperception', 'small'); toggleImageSize(imagezhao2019barperception);"
                title="Click to show details"
            >
                Neighborhood Perception in Bar Charts
            </h3>  <a class="anchor" name="zhao2019barperception"></a>
            <div class="authors">
                <span class="firstAuthor">Mingqian Zhao</span>,
                Huamin Qu, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CHI 2019</span>
                
                <a href="../pdf/zhao2019barperception.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3290605.3300462" target="_blank">website</a>
                
                <a href="../suppl/zhao2019barperception.zip" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we report three user experiments that investigate in how far the perception of a bar in a bar chart changes based on the height of its neighboring bars. We hypothesized that the perception of the very same bar, for instance, might differ when it is surrounded by the top highest vs. the top lowest bars. Our results show that such neighborhood effects exist: a target bar surrounded by high neighbor bars, is perceived to be lower as the same bar surrounded with low neighbors. Yet, the effect size of this neighborhood effect is small compared to other data-inherent effects: the judgment accuracy largely depends on the target bar rank, number of data items, and other data characteristics of the dataset. Based on the findings, we discuss design implications for perceptually optimizing bar charts.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.1145/3290605.3300462,
author = {Zhao, Mingqian and Qu, Huamin and Sedlmair, Michael},
title = {Neighborhood Perception in Bar Charts},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300462},
doi = {10.1145/3290605.3300462},
abstract = {In this paper, we report three user experiments that investigate in how far the perception of a bar in a bar chart changes based on the height of its neighboring bars. We hypothesized that the perception of the very same bar, for instance, might differ when it is surrounded by the top highest vs. the top lowest bars. Our results show that such neighborhood effects exist: a target bar surrounded by high neighbor bars, is perceived to be lower as the same bar surrounded with low neighbors. Yet, the effect size of this neighborhood effect is small compared to other data-inherent effects: the judgment accuracy largely depends on the target bar rank, number of data items, and other data characteristics of the dataset. Based on the findings, we discuss design implications for perceptually optimizing bar charts.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {visualization, empirical study that tells us about people},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperschulz2019deficiencySim"
    >
        
            <img
                id="imageschulz2019deficiencySim"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperschulz2019deficiencySim', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/schulz2019deficiencySim.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperschulz2019deficiencySim', 'small'); toggleImageSize(imageschulz2019deficiencySim);"
                title="Click to show details"
            >
                A Framework for Pervasive Visual Deficiency Simulation
            </h3>  <a class="anchor" name="schulz2019deficiencySim"></a>
            <div class="authors">
                <span class="firstAuthor">Christoph Schulz</span>,
                Nils Rodrigues, Marco Amann, Daniel Baumgartner, Arman Mielke, Christian Baumann, Michael Sedlmair, Daniel Weiskopf
            </div>
            <div>
                <span class="publication">VR 2019</span>
                
                <a href="../pdf/schulz2019deficiencySim.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/VR44988.2019.9044164" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present a framework for rapid prototyping of pervasive visual deficiency simulation in the context of graphical interfaces, virtual reality, and augmented reality. Our framework facilitates the emulation of various visual deficiencies for a wide range of applications, which allows users with normal vision to experience combinations of conditions such as myopia, hyperopia, presbyopia, cataract, nyctalopia, protanopia, deuteranopia, tritanopia, and achromatopsia. Our framework provides an infrastructure to encourage researchers to evaluate visualization and other display techniques regarding visual deficiencies, and opens up the field of visual disease simulation to a broader audience. The benefits of our framework are easy integration, configuration, fast prototyping, and portability to new emerging hardware. To demonstrate the applicability of our framework, we showcase a desktop application and an Android application that transform commodity hardware into glasses for visual deficiency simulation. We expect that this work promotes a greater understanding of visual impairments, leads to better product design for the visually impaired, and forms a basis for research to compensate for these impairments as everyday help.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@INPROCEEDINGS{9044164,
  author={Schulz, Christoph and Rodrigues, Nils and Amann, Marco and Baumgartner, Daniel and Mielke, Arman and Baumann, Christian and Sedlmair, Michael and Weiskopf, Daniel},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={A Framework for Pervasive Visual Deficiency Simulation}, 
  year={2019},
  pages={1-6},
  doi={10.1109/VR44988.2019.9044164}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2019eurova"
    >
        
            <img
                id="imagebernard2019eurova"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2019eurova', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/bernard2019eurova.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2019eurova', 'small'); toggleImageSize(imagebernard2019eurova);"
                title="Click to show details"
            >
                Visual Analysis of Degree-of-Interest Functions to Support Selection Strategies for Instance Labeling
            </h3>  <a class="anchor" name="bernard2019eurova"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                Marco Hutter, Christian Ritter, Markus Lehmann, Michael Sedlmair, Matthias Zeppelzauer
            </div>
            <div>
                <span class="publication">EuroVA 2019</span>
                
                <a href="../pdf/bernard2019eurova.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.2312/eurova.20191116" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Manually labeling data sets is a time-consuming and expensive task that can be accelerated by interactive machine learning and visual analytics approaches. At the core of these approaches are strategies for the selection of candidate instances to label. We introduce degree-of-interest (DOI) functions as atomic building blocks to formalize candidate selection strategies. We introduce a taxonomy of DOI functions and an approach for the visual analysis of DOI functions, which provide novel complementary views on labeling strategies and DOIs, support their in-depth analysis and facilitate their interpretation. Our method shall support the generation of novel and better explanation of existing labeling strategies in future.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings {10.2312:eurova.20191116,
booktitle = {EuroVis Workshop on Visual Analytics (EuroVA)},
editor = {Landesberger, Tatiana von and Turkay, Cagatay},
title = {{Visual Analysis of Degree-of-Interest Functions to Support Selection Strategies for Instance Labeling}},
author = {Bernard, Jürgen and Hutter, Marco and Ritter, Christian and Lehmann, Markus and Sedlmair, Michael and Zeppelzauer, Matthias},
year = {2019},
publisher = {The Eurographics Association},
ISBN = {978-3-03868-087-1},
DOI = {10.2312/eurova.20191116}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2018
    </h2>
    
    <div
        class="paper small"
        id="paperelassady2018ltma"
    >
        
            <img
                id="imageelassady2018ltma"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperelassady2018ltma', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/elassady2018ltma.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperelassady2018ltma', 'small'); toggleImageSize(imageelassady2018ltma);"
                title="Click to show details"
            >
                LTMA: Layered Topic Matching for the Comparative Exploration, Evaluation, and Refinement of Topic Modeling Results
            </h3>  <a class="anchor" name="elassady2018ltma"></a>
            <div class="authors">
                <span class="firstAuthor">Mennatallah El-Assady</span>,
                Fabian Sperrle, Rita Sevastjanova, Michael Sedlmair, Daniel Keim
            </div>
            <div>
                <span class="publication">BDVA 2018</span>
                
                <a href="../pdf/elassady2018ltma.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/BDVA.2018.8534018" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present LTMA, a Layered Topic Matching approach for the unsupervised comparative analysis of topic modeling results. Due to the vast number of available modeling algorithms, an efficient and effective comparison of their results is detrimental to a data- and task-driven selection of a model. LTMA automates this comparative analysis by providing topic matching based on two layers (document-overlap and keyword-similarity), creating a novel topic-match data structure. This data structure builds a basis for model exploration and optimization, thus, allowing for an efficient evaluation of their performance in the context of a given type of text data and task. This is especially important for text types where an annotated gold standard dataset is not readily available and, therefore, quantitative evaluation methods are not applicable. We confirm the usefulness of our technique based on three use cases, namely: (1) the automatic comparative evaluation of topic models, (2) the visual exploration of topic modeling differences, and (3) the optimization of topic modeling results through combining matches.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@INPROCEEDINGS{8534018,
  author={El-Assady, Mennatallah and Sperrle, Fabian and Sevastjanova, Rita and Sedlmair, Michael and Keim, Daniel},
  booktitle={2018 International Symposium on Big Data Visual and Immersive Analytics (BDVA)}, 
  title={LTMA: Layered Topic Matching for the Comparative Exploration, Evaluation, and Refinement of Topic Modeling Results}, 
  year={2018},
  pages={1-10},
  doi={10.1109/BDVA.2018.8534018}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercalero2018bias"
    >
        
            <img
                id="imagecalero2018bias"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercalero2018bias', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/calero2018bias.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercalero2018bias', 'small'); toggleImageSize(imagecalero2018bias);"
                title="Click to show details"
            >
                Studying Biases in Visualization Research: Framework and Methods
            </h3>  <a class="anchor" name="calero2018bias"></a>
            <div class="authors">
                <span class="firstAuthor">André Calero Valdez</span>,
                Martina Ziefle, Michael Sedlmair
            </div>
            <div>
                <span class="publication">Cognitive Biases in Visualizations, Springer 2018</span>
                <span class="publication">Book Chapter</span>
                <a href="../pdf/calero2018bias.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1007/978-3-319-95831-6_2" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this chapter, we propose and discuss a lightweight framework to help organize research questions that arise around biases in visualization and visual analysis. We contrast our framework against the cognitive bias codex by Buster Benson. The framework is inspired by Norman’s Human Action Cycle and classifies biases into three levels: perceptual biases, action biases, and social biases. For each of the levels of cognitive processing, we discuss examples of biases from the cognitive science literature and speculate how they might also be important to the area of visualization. In addition, we put forward a methodological discussion on how biases might be studied on all three levels, and which pitfalls and threats to validity exist. We hope that the framework will help spark new ideas and guide researchers that study the important topic of biases in visualization.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@Inbook{CaleroValdez2018,
author="Calero Valdez, Andr{\'e}
and Ziefle, Martina
and Sedlmair, Michael",
editor="Ellis, Geoffrey",
title="Studying Biases in Visualization Research: Framework and Methods",
bookTitle="Cognitive Biases in Visualizations",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="13--27",
abstract="In this chapter, we propose and discuss a lightweight framework to help organize research questions that arise around biases in visualization and visual analysis. We contrast our framework against the cognitive bias codex by Buster Benson. The framework is inspired by Norman's Human Action Cycle and classifies biases into three levels: perceptual biases, action biases, and social biases. For each of the levels of cognitive processing, we discuss examples of biases from the cognitive science literature and speculate how they might also be important to the area of visualization. In addition, we put forward a methodological discussion on how biases might be studied on all three levels, and which pitfalls and threats to validity exist. We hope that the framework will help spark new ideas and guide researchers that study the important topic of biases in visualization.",
isbn="978-3-319-95831-6",
doi="10.1007/978-3-319-95831-6_2",
url="https://doi.org/10.1007/978-3-319-95831-6_2"
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperwang2019fisheye"
    >
        
            <img
                id="imagewang2019fisheye"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperwang2019fisheye', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/wang2019fisheye.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperwang2019fisheye', 'small'); toggleImageSize(imagewang2019fisheye);"
                title="Click to show details"
            >
                Structure-aware Fisheye Views for Efficient Large Graph Exploration
            </h3>  <a class="anchor" name="wang2019fisheye"></a>
            <div class="authors">
                <span class="firstAuthor">Yunhai Wang</span>,
                Yanyan Wang, Haifeng Zhang, Yinqi Sun, Chi-Wing Fu, Michael Sedlmair, Baoquan Chen, Oliver Deussen
            </div>
            <div>
                <span class="publication">TVCG 2018</span>
                
                <a href="../pdf/wang2019fisheye.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2018.2864911" target="_blank">website</a>
                <a href="../video/wang2019fisheye.mp4" target="_blank">video</a>
                <a href="../suppl/wang2019fisheye.zip" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Traditional fisheye views for exploring large graphs introduce substantial distortions that often lead to a decreased readability of paths and other interesting structures. To overcome these problems, we propose a framework for structure-aware fisheye views. Using edge orientations as constraints for graph layout optimization allows us not only to reduce spatial and temporal distortions during fisheye zooms, but also to improve the readability of the graph structure. Furthermore, the framework enables us to optimize fisheye lenses towards specific tasks and design a family of new lenses: polyfocal, cluster, and path lenses. A GPU implementation lets us process large graphs with up to 15,000 nodes at interactive rates. A comprehensive evaluation, a user study, and two case studies demonstrate that our structure-aware fisheye views improve layout readability and user performance.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{8440835,
  author={Wang, Yunhai and Wang, Yanyan and Zhang, Haifeng and Sun, Yinqi and Fu, Chi-Wing and Sedlmair, Michael and Chen, Baoquan and Deussen, Oliver},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Structure-aware Fisheye Views for Efficient Large Graph Exploration}, 
  year={2019},
  volume={25},
  number={1},
  pages={566-575},
  doi={10.1109/TVCG.2018.2864911}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperwang2019color"
    >
        
            <img
                id="imagewang2019color"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperwang2019color', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/wang2019color.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperwang2019color', 'small'); toggleImageSize(imagewang2019color);"
                title="Click to show details"
            >
                Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots
            </h3>  <a class="anchor" name="wang2019color"></a>
            <div class="authors">
                <span class="firstAuthor">Yunhai Wang</span>,
                Xin Chen, Tong Ge, Chen Bao, Michael Sedlmair, Chi-Wing Fu, Oliver Deussen, Baoquan Chen
            </div>
            <div>
                <span class="publication">TVCG 2018</span>
                
                <a href="../pdf/wang2019color.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2018.2864912" target="_blank">website</a>
                
                <a href="../suppl/wang2019color.zip" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Appropriate choice of colors significantly aids viewers in understanding the structures in multiclass scatterplots and becomes more important with a growing number of data points and groups. An appropriate color mapping is also an important parameter for the creation of an aesthetically pleasing scatterplot. Currently, users of visualization software routinely rely on color mappings that have been pre-defined by the software. A default color mapping, however, cannot ensure an optimal perceptual separability between groups, and sometimes may even lead to a misinterpretation of the data. In this paper, we present an effective approach for color assignment based on a set of given colors that is designed to optimize the perception of scatterplots. Our approach takes into account the spatial relationships, density, degree of overlap between point clusters, and also the background color. For this purpose, we use a genetic algorithm that is able to efficiently find good color assignments. We implemented an interactive color assignment system with three extensions of the basic method that incorporates top K suggestions, user-defined color subsets, and classes of interest for the optimization. To demonstrate the effectiveness of our assignment technique, we conducted a numerical study and a controlled user study to compare our approach with default color assignments; our findings were verified by two expert studies. The results show that our approach is able to support users in distinguishing cluster numbers faster and more precisely than default assignment methods.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{8440853,
  author={Wang, Yunhai and Chen, Xin and Ge, Tong and Bao, Chen and Sedlmair, Michael and Fu, Chi-Wing and Deussen, Oliver and Chen, Baoquan},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots}, 
  year={2019},
  volume={25},
  number={1},
  pages={820-829},
  doi={10.1109/TVCG.2018.2864912}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2018eurovis"
    >
        
            <img
                id="imagebernard2018eurovis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2018eurovis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/bernard2018eurovis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2018eurovis', 'small'); toggleImageSize(imagebernard2018eurovis);"
                title="Click to show details"
            >
                Towards User-Centered Active Learning Algorithms
            </h3>  <a class="anchor" name="bernard2018eurovis"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                Matthias Zeppelzauer, Markus Lehmann, Martin Müller, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CGF 2018</span>
                
                <a href="../pdf/bernard2018eurovis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1111/cgf.13406" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The labeling of data sets is a time-consuming task, which is, however, an important prerequisite for machine learning and visual analytics. Visual-interactive labeling (VIAL) provides users an active role in the process of labeling, with the goal to combine the potentials of humans and machines to make labeling more efficient. Recent experiments showed that users apply different strategies when selecting instances for labeling with visual-interactive interfaces. In this paper, we contribute a systematic quantitative analysis of such user strategies. We identify computational building blocks of user strategies, formalize them, and investigate their potentials for different machine learning tasks in systematic experiments. The core insights of our experiments are as follows. First, we identified that particular user strategies can be used to considerably mitigate the bootstrap (cold start) problem in early labeling phases. Second, we observed that they have the potential to outperform existing active learning strategies in later phases. Third, we analyzed the identified core building blocks, which can serve as the basis for novel selection strategies. Overall, we observed that data-based user strategies (clusters, dense areas) work considerably well in early phases, while model-based user strategies (e.g., class separation) perform better during later phases. The insights gained from this work can be applied to develop novel active learning approaches as well as to better guide users in visual interactive labeling.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{https://doi.org/10.1111/cgf.13406,
author = {Bernard, Jürgen and Zeppelzauer, Matthias and Lehmann, Markus and Müller, Martin and Sedlmair, Michael},
title = {Towards User-Centered Active Learning Algorithms},
journal = {Computer Graphics Forum},
volume = {37},
number = {3},
pages = {121-132},
keywords = {Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation},
doi = {https://doi.org/10.1111/cgf.13406},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13406},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13406},
abstract = {Abstract The labeling of data sets is a time-consuming task, which is, however, an important prerequisite for machine learning and visual analytics. Visual-interactive labeling (VIAL) provides users an active role in the process of labeling, with the goal to combine the potentials of humans and machines to make labeling more efficient. Recent experiments showed that users apply different strategies when selecting instances for labeling with visual-interactive interfaces. In this paper, we contribute a systematic quantitative analysis of such user strategies. We identify computational building blocks of user strategies, formalize them, and investigate their potentials for different machine learning tasks in systematic experiments. The core insights of our experiments are as follows. First, we identified that particular user strategies can be used to considerably mitigate the bootstrap (cold start) problem in early labeling phases. Second, we observed that they have the potential to outperform existing active learning strategies in later phases. Third, we analyzed the identified core building blocks, which can serve as the basis for novel selection strategies. Overall, we observed that data-based user strategies (clusters, dense areas) work considerably well in early phases, while model-based user strategies (e.g., class separation) perform better during later phases. The insights gained from this work can be applied to develop novel active learning approaches as well as to better guide users in visual interactive labeling.},
year = {2018}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papertorsneyweir2018eurovis"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('papertorsneyweir2018eurovis', 'small'); toggleImageSize(imagetorsneyweir2018eurovis);"
                title="Click to show details"
            >
                Hypersliceplorer: Interactive Visualization of Shapes in Multiple Dimensions
            </h3>  <a class="anchor" name="torsneyweir2018eurovis"></a>
            <div class="authors">
                <span class="firstAuthor">Thomas Torsney-Weir</span>,
                Torsten Möller, Michael Sedlmair, Mike Kirby
            </div>
            <div>
                <span class="publication">CGF 2018</span>
                
                <a href="../pdf/torsneyweir2018eurovis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1111/cgf.13415" target="_blank">website</a>
                <a href="../video/torsneyweir2018eurovis.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper we present Hypersliceplorer, an algorithm for generating 2D slices of multi-dimensional shapes defined by a simplical mesh. Often, slices are generated by using a parametric form and then constraining parameters to view the slice. In our case, we developed an algorithm to slice a simplical mesh of any number of dimensions with a two-dimensional slice. In order to get a global appreciation of the multi-dimensional object, we show multiple slices by sampling a number of different slicing points and projecting the slices into a single view per dimension pair. These slices are shown in an interactive viewer which can switch between a global view (all slices) and a local view (single slice). We show how this method can be used to study regular polytopes, differences between spaces of polynomials, and multi-objective optimization surfaces.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{https://doi.org/10.1111/cgf.13415,
author = {Torsney-Weir, T. and Möller, T. and Sedlmair, M. and Kirby, R. M.},
title = {Hypersliceplorer: Interactive visualization of shapes in multiple dimensions},
journal = {Computer Graphics Forum},
volume = {37},
number = {3},
pages = {229-240},
keywords = {Categories and Subject Descriptors (according to ACM CCS), I.3.3 Computer Graphics: Picture/Image Generation—Line and curve generation},
doi = {https://doi.org/10.1111/cgf.13415},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13415},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.13415},
abstract = {Abstract In this paper we present Hypersliceplorer, an algorithm for generating 2D slices of multi-dimensional shapes defined by a simplical mesh. Often, slices are generated by using a parametric form and then constraining parameters to view the slice. In our case, we developed an algorithm to slice a simplical mesh of any number of dimensions with a two-dimensional slice. In order to get a global appreciation of the multi-dimensional object, we show multiple slices by sampling a number of different slicing points and projecting the slices into a single view per dimension pair. These slices are shown in an interactive viewer which can switch between a global view (all slices) and a local view (single slice). We show how this method can be used to study regular polytopes, differences between spaces of polynomials, and multi-objective optimization surfaces.},
year = {2018}
}

</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperballte2018hilda"
    >
        
            <img
                id="imageballte2018hilda"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperballte2018hilda', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/ballte2018hilda.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperballte2018hilda', 'small'); toggleImageSize(imageballte2018hilda);"
                title="Click to show details"
            >
                Evaluating Visual Data Analysis Systems: A Discussion Report
            </h3>  <a class="anchor" name="ballte2018hilda"></a>
            <div class="authors">
                <span class="firstAuthor">Leilani Battle</span>,
                Marco Angelini, Carsten Binnig, Tiziana Catarci, Philipp Eichmann, Jean-Daniel Fekete, Giuseppe Santucci, Michael Sedlmair, Wesley Willett
            </div>
            <div>
                <span class="publication">HILDA 2018</span>
                <span class="publication">Workshop Paper</span>
                <a href="../pdf/ballte2018hilda.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3209900.3209901" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                

Visual data analysis is a key tool for helping people to make sense of and interact with massive data sets. However, existing evaluation methods (e.g., database benchmarks, individual user studies) fail to capture the key points that make systems for visual data analysis (or visual data systems) challenging to design. In November 2017, members of both the Database and Visualization communities came together in a Dagstuhl seminar to discuss the grand challenges in the intersection of data analysis and interactive visualization.

In this paper, we report on the discussions of the working group on the evaluation of visual data systems, which addressed questions centered around developing better evaluation methods, such as "How do the different communities evaluate visual data systems?" and "What we could learn from each other to develop evaluation techniques that cut across areas?". In their discussions, the group brainstormed initial steps towards new joint evaluation methods and developed a first concrete initiative --- a trace repository of various real-world workloads and visual data systems --- that enables researchers to derive evaluation setups (e.g., performance benchmarks, user studies) under more realistic assumptions, and enables new evaluation perspectives (e.g., broader meta analysis across analysis contexts, reproducibility and comparability across systems).

            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.1145/3209900.3209901,
author = {Battle, Leilani and Angelini, Marco and Binnig, Carsten and Catarci, Tiziana and Eichmann, Philipp and Fekete, Jean-Daniel and Santucci, Giuseppe and Sedlmair, Michael and Willett, Wesley},
title = {Evaluating Visual Data Analysis Systems: A Discussion Report},
year = {2018},
isbn = {9781450358279},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3209900.3209901},
doi = {10.1145/3209900.3209901},
abstract = {Visual data analysis is a key tool for helping people to make sense of and interact with massive data sets. However, existing evaluation methods (e.g., database benchmarks, individual user studies) fail to capture the key points that make systems for visual data analysis (or visual data systems) challenging to design. In November 2017, members of both the Database and Visualization communities came together in a Dagstuhl seminar to discuss the grand challenges in the intersection of data analysis and interactive visualization.In this paper, we report on the discussions of the working group on the evaluation of visual data systems, which addressed questions centered around developing better evaluation methods, such as "How do the different communities evaluate visual data systems?" and "What we could learn from each other to develop evaluation techniques that cut across areas?". In their discussions, the group brainstormed initial steps towards new joint evaluation methods and developed a first concrete initiative --- a trace repository of various real-world workloads and visual data systems --- that enables researchers to derive evaluation setups (e.g., performance benchmarks, user studies) under more realistic assumptions, and enables new evaluation perspectives (e.g., broader meta analysis across analysis contexts, reproducibility and comparability across systems).},
booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
articleno = {4},
numpages = {6},
location = {Houston, TX, USA},
series = {HILDA'18}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papertorsneyweir2018eurovis_short"
    >
        
            <img
                id="imagetorsneyweir2018eurovis_short"
                title="Click to enlarge and show details"
                onclick="toggleClass('papertorsneyweir2018eurovis_short', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/torsneyweir2018eurovis_short.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papertorsneyweir2018eurovis_short', 'small'); toggleImageSize(imagetorsneyweir2018eurovis_short);"
                title="Click to show details"
            >
                Risk Fixers and Sweet Spotters: A Study of the Different Approaches to Using Visual Sensitivity Analysis in an Investment Scenario
            </h3>  <a class="anchor" name="torsneyweir2018eurovis_short"></a>
            <div class="authors">
                <span class="firstAuthor">Thomas Torsney-Weir</span>,
                Shahrzad Afroozeh, Michael Sedlmair, and Torsten Möller
            </div>
            <div>
                <span class="publication">EuroVis 2018</span>
                <span class="publication">Short Paper</span>
                <a href="../pdf/torsneyweir2018eurovis_short.pdf" target="_blank">PDF</a>
                <a href="http://eprints.cs.univie.ac.at/5649/" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present an empirical study that illustrates how individual users' decision making preferences and biases influence visualization design choices. Twenty-three participants, in a lab study, were shown two interactive financial portfolio optimization interfaces which allowed them to adjust the return for the portfolio and view how the risk changes. One interface showed the sensitivity of the risk to changes in the return and one did not have this feature. Our study highlights two classes of users. One which preferred the interface with the sensitivity feature and one group that does not prefer the sensitivity feature. We named these two groups the ``risk fixers'' and the ``sweet spotters'' due to the analysis method they used. The ``risk fixers'' selected a level of risk which they were comfortable with while the ``sweet spotters'' tried to find a point right before the risk increased greatly. Our study shows that exposing the sensitivity of investment parameters will impact the investment decision process and increase confidence for these ``sweet spotters.'' We also discuss the implications for design.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cs5649,
       booktitle = {EuroVis 2018 - Short Papers},
           title = {Risk fixers and sweet spotters: A study of the different approaches to using visual sensitivity analysis in an investment scenario},
          author = {Thomas Torsney-Weir and Shahrzad Afroozeh and Michael Sedlmair and Torsten M{\"o}ller},
            year = {2018},
       publisher = {The Eurographics Association},
        abstract = {We present an empirical study that illustrates how individual users' decision
making preferences and biases influence visualization design choices.
Twenty-three participants, in a lab study,
were shown two interactive financial portfolio optimization
interfaces which allowed them to adjust the return for the portfolio and
view how the risk changes. One interface showed the sensitivity of the
risk to changes in the return and one did not have this feature. Our
study highlights two classes of users. One which preferred the
interface with the sensitivity feature and one group that does not
prefer the sensitivity feature. We named these two groups the ``risk
fixers'' and the ``sweet spotters'' due to the analysis method they
used. The ``risk fixers'' selected a level of risk which they were
comfortable with while the ``sweet spotters'' tried to find a point
right before the risk increased greatly. Our study shows that exposing
the sensitivity of investment parameters will impact the investment
decision process and increase confidence for these ``sweet spotters.''
We also discuss the implications for design.},
             url = {http://eprints.cs.univie.ac.at/5649/}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2018viscoder"
    >
        
            <img
                id="imagecutura2018viscoder"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2018viscoder', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/cutura2018viscoder.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2018viscoder', 'small'); toggleImageSize(imagecutura2018viscoder);"
                title="Click to show details"
            >
                VisCoDeR: A Tool for Visually Comparing Dimensionality Reduction Algorithms
            </h3>  <a class="anchor" name="cutura2018viscoder"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Stefan Holzer, Michaël Aupetit, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ESANN 2018</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/cutura2018viscoder.pdf" target="_blank">PDF</a>
                
                <a href="../video/cutura2018viscoder.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose VisCoDeR, a tool that leverages comparative visualization to support learning and analyzing different dimensionality reduction (DR) methods. VisCoDeR fosters two modes. The Discover mode allows qualitatively comparing several DR results by juxtaposing and linking the resulting scatterplots. The Explore mode allows for analyzing hundreds of differently parameterized DR results in a quantitative way. We present use cases that show that our approach helps to understand similarities and differences between DR algorithms.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2018viscoder,
  title={{VisCoDeR: A Tool for Visually Comparing Dimensionality Reduction Algorithms}},
  author={Cutura, Rene and Holzer, Stefan and Aupetit, Micha{\"e}l and Sedlmair, Michael},
  booktitle={Euro. Symp. on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
  pages={641--646}
  year={2018}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperrudkowsky2018cmm"
    >
        
            <img
                id="imagerudkowsky2018cmm"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperrudkowsky2018cmm', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/rudkowsky2018cmm.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperrudkowsky2018cmm', 'small'); toggleImageSize(imagerudkowsky2018cmm);"
                title="Click to show details"
            >
                More than Bags of Words: Sentiment Analysis with Word Embeddings
            </h3>  <a class="anchor" name="rudkowsky2018cmm"></a>
            <div class="authors">
                <span class="firstAuthor">Elena Rudkowsky</span>,
                Martin Haselmayer, Matthias Wastian, Marcelo Jenny, Stefan Emrich, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CMM 2018</span>
                
                <a href="../pdf/rudkowsky2018cmm.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1080/19312458.2018.1455817" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Moving beyond the dominant bag-of-words approach to sentiment analysis we introduce an alternative procedure based on distributed word embeddings. The strength of word embeddings is the ability to capture similarities in word meaning. We use word embeddings as part of a supervised machine learning procedure which estimates levels of negativity in parliamentary speeches. The procedure’s accuracy is evaluated with crowdcoded training sentences; its external validity through a study of patterns of negativity in Austrian parliamentary speeches. The results show the potential of the word embeddings approach for sentiment analysis in the social sciences.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{doi:10.1080/19312458.2018.1455817,
author = {Elena Rudkowsky and Martin Haselmayer and Matthias Wastian and Marcelo Jenny and Štefan Emrich and Michael Sedlmair},
title = {More than Bags of Words: Sentiment Analysis with Word Embeddings},
journal = {Communication Methods and Measures},
volume = {12},
number = {2-3},
pages = {140-157},
year  = {2018},
publisher = {Routledge},
doi = {10.1080/19312458.2018.1455817},
URL = { 
        https://doi.org/10.1080/19312458.2018.1455817
},
eprint = { 
        https://doi.org/10.1080/19312458.2018.1455817
    }
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2018vial"
    >
        
            <img
                id="imagebernard2018vial"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2018vial', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/bernard2018vial.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2018vial', 'small'); toggleImageSize(imagebernard2018vial);"
                title="Click to show details"
            >
                VIAL – A Unified Process for Visual-Interactive Labeling
            </h3>  <a class="anchor" name="bernard2018vial"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                Matthias Zeppelzauer, Michael Sedlmair, Wolfgang Aigner
            </div>
            <div>
                <span class="publication">EuroVA 2018</span>
                
                <a href="../pdf/bernard2018vial.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1007/s00371-018-1500-3" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The assignment of labels to data instances is a fundamental prerequisite for many machine learning tasks. Moreover, labeling is a frequently applied process in visual interactive analysis approaches and visual analytics. However, the strategies for creating labels usually differ between these two fields. This raises the question whether synergies between the different approaches can be attained. In this paper, we study the process of labeling data instances with the user in the loop, from both the machine learning and visual interactive perspective. Based on a review of differences and commonalities, we propose the “visual interactive labeling” (VIAL) process that unifies both approaches. We describe the six major steps of the process and discuss their specific challenges. Additionally, we present two heterogeneous usage scenarios from the novel VIAL perspective, one on metric distance learning and one on object detection in videos. Finally, we discuss general challenges to VIAL and point out necessary work for the realization of future VIAL approaches.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{bernard2018vial,
  title={VIAL: a unified process for visual interactive labeling},
  author={Bernard, J{\"u}rgen and Zeppelzauer, Matthias and Sedlmair, Michael and Aigner, Wolfgang},
  journal={The Visual Computer},
  volume={34},
  number={9},
  pages={1189--1207},
  year={2018},
  publisher={Springer}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperoppermann2017bikesharingatlas"
    >
        
            <img
                id="imageoppermann2017bikesharingatlas"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperoppermann2017bikesharingatlas', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/oppermann2017bikesharingatlas.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperoppermann2017bikesharingatlas', 'small'); toggleImageSize(imageoppermann2017bikesharingatlas);"
                title="Click to show details"
            >
                Bike Sharing Atlas: Visual Analysis of Bike-Sharing Networks
            </h3>  <a class="anchor" name="oppermann2017bikesharingatlas"></a>
            <div class="authors">
                <span class="firstAuthor">Michael Oppermann</span>,
                Torsten Möller, Michael Sedlmair
            </div>
            <div>
                <span class="publication">IJT 2018</span>
                
                <a href="../pdf/oppermann2017bikesharingatlas.pdf" target="_blank">PDF</a>
                <a href="http://eprints.cs.univie.ac.at/5855/" target="_blank">website</a>
                <a href="../video/oppermann2017bikesharingatlas.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we introduce an interactive visualization system, bikesharingatlas.org, that supports the explorative data analysis of more than 468 bike-sharing networks worldwide. The system leverages a multi-coordinated view approach and innovative interaction techniques can help, for instance, to expose capacity bottlenecks, commuting patterns, and other network characteristics. Our broader goal is to illustrate how visual analysis can be used for exploring distributed, heterogeneous data from smart cities. Based on our collaboration with different target users, we present usage scenarios that show the potential of our approach to understanding bike-sharing and urban commuting behaviors.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{cs5855,
            year = {2018},
           month = {January},
            issn = {2207-6433},
          author = {Michael Oppermann and Torsten M{\"o}ller and Michael Sedlmair},
           title = {BikeSharingAtlas: Visual Analysis of Bike-Sharing Networks},
         journal = {International Journal of Transportation},
             url = {http://eprints.cs.univie.ac.at/5855/}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperaigner2018valid"
    >
        
            <img
                id="imageaigner2018valid"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperaigner2018valid', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/aigner2018valid.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperaigner2018valid', 'small'); toggleImageSize(imageaigner2018valid);"
                title="Click to show details"
            >
                Data Journalism - Guidelines and Best Practices for Getting Started
            </h3>  <a class="anchor" name="aigner2018valid"></a>
            <div class="authors">
                <span class="firstAuthor">Wolfgang Aigner</span>,
                Eva Goldgruber, Florian Grassinger, Robert Gutounig, Alexander Rind, Michael Sedlmair, Christina Stoiber
            </div>
            <div>
                <span class="publication">Booklet, FFG VALiD (project no. 845598) 2018</span>
                <span class="publication">Booklet</span>
                <a href="../pdf/aigner2018valid.pdf" target="_blank">PDF</a>
                <a href="www.validproject.at" target="_blank">website</a>
                
                <a href="https://github.com/VALIDproject/ddj-booklet" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                
            </div>
            
            
        </div>
    </div>
    
    
    <h2>
        2017
    </h2>
    
    <div
        class="paper small"
        id="papersacha2017b"
    >
        
            <img
                id="imagesacha2017b"
                title="Click to enlarge and show details"
                onclick="toggleClass('papersacha2017b', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/sacha2017b.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papersacha2017b', 'small'); toggleImageSize(imagesacha2017b);"
                title="Click to show details"
            >
                What You See Is What You Can Change: Human-Centered Machine Learning By Interactive Visualization
            </h3>  <a class="anchor" name="sacha2017b"></a>
            <div class="authors">
                <span class="firstAuthor">Dominik Sacha</span>,
                Michael Sedlmair, Leishi Zhang, John A Lee, Jaakko Peltonen, Daniel Weiskopf, Stephen C North, Daniel A Keim
            </div>
            <div>
                <span class="publication">Neurocomputing 2017</span>
                
                
                <a href="https://doi.org/10.1016/j.neucom.2017.01.105" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Visual analytics (VA) systems help data analysts solve complex problems interactively, by integrating automated data analysis and mining, such as machine learning (ML) based methods, with interactive visualizations. We propose a conceptual framework that models human interactions with ML components in the VA process, and that puts the central relationship between automated algorithms and interactive visualizations into sharp focus. The framework is illustrated with several examples and we further elaborate on the interactive ML process by identifying key scenarios where ML methods are combined with human feedback through interactive visualization. We derive five open research challenges at the intersection of ML and visualization research, whose solution should lead to more effective data analysis.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{SACHA2017164,
title = {What you see is what you can change: Human-centered machine learning by interactive visualization},
journal = {Neurocomputing},
volume = {268},
pages = {164-175},
year = {2017},
note = {Advances in artificial neural networks, machine learning and computational intelligence},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.01.105},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217307609},
author = {Dominik Sacha and Michael Sedlmair and Leishi Zhang and John A. Lee and Jaakko Peltonen and Daniel Weiskopf and Stephen C. North and Daniel A. Keim},
keywords = {Machine learning, Information visualization, Interaction, Visual analytics},
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercalero-valdez2017framework"
    >
        
            <img
                id="imagecalero-valdez2017framework"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercalero-valdez2017framework', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/calero-valdez2017framework.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercalero-valdez2017framework', 'small'); toggleImageSize(imagecalero-valdez2017framework);"
                title="Click to show details"
            >
                A Framework for Studying Biases in Visualization Research
            </h3>  <a class="anchor" name="calero-valdez2017framework"></a>
            <div class="authors">
                <span class="firstAuthor">André Calero Valdez</span>,
                Martina Ziefle, Michael Sedlmair
            </div>
            <div>
                <span class="publication">DECISIVe 2017</span>
                
                <a href="../pdf/calero-valdez2017framework.pdf" target="_blank">PDF</a>
                <a href="http://eprints.cs.univie.ac.at/5258/" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this position paper, we propose and discuss a lightweight framework to help organize research questions that arise around biases in visualization and visual analysis. We contrast our framework against cognitive bias codex by Buster Benson. The framework is inspired by Norman’s Human Action Cycle [23] and classifies biases into three levels: perceptual biases, action biases, and social biases. For each of the levels of cognitive processing, we discuss examples of biases from the cognitive science literature, and speculate how they might also be important to the area of visualization. In addition, we put forward a methodological discussion on how biases might be studied on all three levels, and which pitfalls and threats to validity exist. We hope that the framework will help spark new ideas and discussions on how to proceed studying the important topic of biases in visualization.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cs5258,
          author = {Andr{\'e} Caldero Valdez and Martina Ziefle and Michael Sedlmair},
            year = {2017},
       booktitle = {DECISIVe 2017},
           title = {A Framework for Studying Biases in Visualization Research},
             url = {http://eprints.cs.univie.ac.at/5258/}
}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercalero-valdez2017priming"
    >
        
            <img
                id="imagecalero-valdez2017priming"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercalero-valdez2017priming', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/calero-valdez2017priming.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercalero-valdez2017priming', 'small'); toggleImageSize(imagecalero-valdez2017priming);"
                title="Click to show details"
            >
                Priming and Anchoring Effects in Visualization
            </h3>  <a class="anchor" name="calero-valdez2017priming"></a>
            <div class="authors">
                <span class="firstAuthor">André Calero Valdez</span>,
                Martina Ziefle, Michael Sedlmair
            </div>
            <div>
                <span class="publication">TVCG 2017</span>
                
                <a href="../pdf/calero-valdez2017priming.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2017.2744138" target="_blank">website</a>
                <a href="../video/calero-valdez2017priming.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We investigate priming and anchoring effects on perceptual tasks in visualization. Priming or anchoring effects depict the phenomena that a stimulus might influence subsequent human judgments on a perceptual level, or on a cognitive level by providing a frame of reference. Using visual class separability in scatterplots as an example task, we performed a set of five studies to investigate the potential existence of priming and anchoring effects. Our findings show that - under certain circumstances - such effects indeed exist. In other words, humans judge class separability of the same scatterplot differently depending on the scatterplot(s) they have seen before. These findings inform future work on better understanding and more accurately modeling human perception of visual patterns.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{8022891,
  author={Valdez, André Calero and Ziefle, Martina and Sedlmair, Michael},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Priming and Anchoring Effects in Visualization}, 
  year={2018},
  volume={24},
  number={1},
  pages={584-594},
  doi={10.1109/TVCG.2017.2744138}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperwang2017edwordle"
    >
        
            <img
                id="imagewang2017edwordle"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperwang2017edwordle', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/wang2017edwordle.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperwang2017edwordle', 'small'); toggleImageSize(imagewang2017edwordle);"
                title="Click to show details"
            >
                EdWordle: Consistency-preserving Word Cloud Editing
            </h3>  <a class="anchor" name="wang2017edwordle"></a>
            <div class="authors">
                <span class="firstAuthor">Yunhai Wang</span>,
                Xiaowei Chu, Chen Bao, Lifeng Zhu, Oliver Deussen, Baoquan Chen, Michael Sedlmair
            </div>
            <div>
                <span class="publication">TVCG 2017</span>
                
                <a href="../pdf/wang2017edwordle.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2017.2745859" target="_blank">website</a>
                <a href="../video/wang2017edwordle.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present EdWordle, a method for consistently editing word clouds. At its heart, EdWordle allows users to move and edit words while preserving the neighborhoods of other words. To do so, we combine a constrained rigid body simulation with a neighborhood-aware local Wordle algorithm to update the cloud and to create very compact layouts. The consistent and stable behavior of EdWordle enables users to create new forms of word clouds such as storytelling clouds in which the position of words is carefully edited. We compare our approach with state-of-the-art methods and show that we can improve user performance, user satisfaction, as well as the layout itself.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{8017586,
  author={Wang, Yunhai and Chu, Xiaowei and Bao, Chen and Zhu, Lifeng and Deussen, Oliver and Chen, Baoquan and Sedlmair, Michael},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={EdWordle: Consistency-Preserving Word Cloud Editing}, 
  year={2018},
  volume={24},
  number={1},
  pages={647-656},
  doi={10.1109/TVCG.2017.2745859}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperwang2017graph"
    >
        
            <img
                id="imagewang2017graph"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperwang2017graph', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/wang2017graph.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperwang2017graph', 'small'); toggleImageSize(imagewang2017graph);"
                title="Click to show details"
            >
                Revisiting Stress Majorization as a Unified Framework for Interactive Constrained Graph Visualization
            </h3>  <a class="anchor" name="wang2017graph"></a>
            <div class="authors">
                <span class="firstAuthor">Yunhai Wang</span>,
                Yanyan Wang,  Yinqi Sun, Lifeng Zhu, Kecheng Lu, Chi-Wing Fu, Michael Sedlmair, Oliver Deussen, Baoquan Chen
            </div>
            <div>
                <span class="publication">TVCG 2017</span>
                
                <a href="../pdf/wang2017graph.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2017.2745919" target="_blank">website</a>
                <a href="../video/wang2017graph.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present an improved stress majorization method that incorporates various constraints, including directional constraints without the necessity of solving a constraint optimization problem. This is achieved by reformulating the stress function to impose constraints on both the edge vectors and lengths instead of just on the edge lengths (node distances). This is a unified framework for both constrained and unconstrained graph visualizations, where we can model most existing layout constraints, as well as develop new ones such as the star shapes and cluster separation constraints within stress majorization. This improvement also allows us to parallelize computation with an efficient GPU conjugant gradient solver, which yields fast and stable solutions, even for large graphs. As a result, we allow the constraint-based exploration of large graphs with 10K nodes - an approach which previous methods cannot support.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{8017634,
  author={Wang, Yunhai and Wang, Yanyan and Sun, Yinqi and Zhu, Lifeng and Lu, Kecheng and Fu, Chi-Wing and Sedlmair, Michael and Deussen, Oliver and Chen, Baoquan},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Revisiting Stress Majorization as a Unified Framework for Interactive Constrained Graph Visualization}, 
  year={2018},
  volume={24},
  number={1},
  pages={489-499},
  doi={10.1109/TVCG.2017.2745919}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2017labeling"
    >
        
            <img
                id="imagebernard2017labeling"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2017labeling', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/bernard2017labeling.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2017labeling', 'small'); toggleImageSize(imagebernard2017labeling);"
                title="Click to show details"
            >
                Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study
            </h3>  <a class="anchor" name="bernard2017labeling"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                Marco Hutter, Matthias Zeppelzauer, Dieter Fellner, Michael Sedlmair
            </div>
            <div>
                <span class="publication">TVCG 2017</span>
                
                <a href="../pdf/bernard2017labeling.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2017.2744818" target="_blank">website</a>
                <a href="../video/bernard2017labeling.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Labeling data instances is an important task in machine learning and visual analytics. Both fields provide a broad set of labeling strategies, whereby machine learning (and in particular active learning) follows a rather model-centered approach and visual analytics employs rather user-centered approaches (visual-interactive labeling). Both approaches have individual strengths and weaknesses. In this work, we conduct an experiment with three parts to assess and compare the performance of these different labeling strategies. In our study, we (1) identify different visual labeling strategies for user-centered labeling, (2) investigate strengths and weaknesses of labeling strategies for different labeling tasks and task complexities, and (3) shed light on the effect of using different visual encodings to guide the visual-interactive labeling process. We further compare labeling of single versus multiple instances at a time, and quantify the impact on efficiency. We systematically compare the performance of visual interactive labeling with that of active learning. Our main findings are that visual-interactive labeling can outperform active learning, given the condition that dimension reduction separates well the class distributions. Moreover, using dimension reduction in combination with additional visual encodings that expose the internal state of the learning model turns out to improve the performance of visual-interactive labeling.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{8019851,
  author={Bernard, Jürgen and Hutter, Marco and Zeppelzauer, Matthias and Fellner, Dieter and Sedlmair, Michael},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study}, 
  year={2018},
  volume={24},
  number={1},
  pages={298-308},
  doi={10.1109/TVCG.2017.2744818}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2017a"
    >
        
            <img
                id="imagebernard2017a"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2017a', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/bernard2017a.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2017a', 'small'); toggleImageSize(imagebernard2017a);"
                title="Click to show details"
            >
                Combining Cluster and Outlier Analysis with Visual Analytics
            </h3>  <a class="anchor" name="bernard2017a"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                Eduard Dobermann, Michael Sedlmair, Dieter Fellner
            </div>
            <div>
                <span class="publication">EuroVA 2017</span>
                
                <a href="../pdf/bernard2017a.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.2312/eurova.20171114" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Cluster and outlier analysis are two important tasks. Due to their nature these tasks seem to be opposed to each other, i.e., data objects either belong to a cluster structure or a sparsely populated outlier region. In this work, we present a visual analytics tool that allows the combined analysis of clusters and outliers. Users can add multiple clustering and outlier analysis algorithms, compare results visually, and combine the algorithms’ results. The usefulness of the combined analysis is demonstrated using the example of labeling unknown data sets. The usage scenario also shows that identified clusters and outliers can share joint areas of the data space.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.2312/eurova.20171114,
author = {Bernard, J\"{u}rgen and Dobermann, Eduard and Sedlmair, Michael and Fellner, D. W.},
title = {Combining Cluster and Outlier Analysis with Visual Analytics},
year = {2017},
publisher = {Eurographics Association},
address = {Goslar, DEU},
url = {https://doi.org/10.2312/eurova.20171114},
doi = {10.2312/eurova.20171114},
abstract = {Cluster and outlier analysis are two important tasks. Due to their nature these tasks seem to be opposed to each other, i.e., data objects either belong to a cluster structure or a sparsely populated outlier region. In this work, we present a visual analytics tool that allows the combined analysis of clusters and outliers. Users can add multiple clustering and outlier analysis algorithms, compare results visually, and combine the algorithms' results. The usefulness of the combined analysis is demonstrated using the example of labeling unknown data sets. The usage scenario also shows that identified clusters and outliers can share joint areas of the data space.},
booktitle = {Proceedings of the EuroVis Workshop on Visual Analytics},
pages = {19–23},
numpages = {5},
location = {Barcelona, Spain},
series = {EuroVA '17}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2017b"
    >
        
            <img
                id="imagebernard2017b"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2017b', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/bernard2017b.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2017b', 'small'); toggleImageSize(imagebernard2017b);"
                title="Click to show details"
            >
                A Unified Process for Visual-Interactive Labeling
            </h3>  <a class="anchor" name="bernard2017b"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                Matthias Zeppelzauer, Michael Sedlmair, Wolfgang Aigner
            </div>
            <div>
                <span class="publication">EuroVA 2017</span>
                
                <a href="../pdf/bernard2017b.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.2312/eurova.20171123" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Assigning labels to data instances is a prerequisite for many machine learning tasks. Similarly, labeling is applied in visual-interactive analysis approaches. However, the strategies for creating labels often differ in the two fields. In this paper, we study the process of labeling data instances with the user in the loop, from both the machine learning and visual-interactive perspective. Based on a review of differences and commonalities, we propose the 'Visual-Interactive Labeling' (VIAL) process, conflating the strengths of both. We describe the six major steps of the process and highlight their related challenges.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.2312/eurova.20171123,
author = {Bernard, J\"{u}rgen and Zeppelzauer, Matthias and Sedlmair, Michael and Aigner, Wolfgang},
title = {A Unified Process for Visual-Interactive Labeling},
year = {2017},
publisher = {Eurographics Association},
address = {Goslar, DEU},
url = {https://doi.org/10.2312/eurova.20171123},
doi = {10.2312/eurova.20171123},
abstract = {Assigning labels to data instances is a prerequisite for many machine learning tasks. Similarly, labeling is applied in visual-interactive analysis approaches. However, the strategies for creating labels often differ in the two fields. In this paper, we study the process of labeling data instances with the user in the loop, from both the machine learning and visual-interactive perspective. Based on a review of differences and commonalities, we propose the 'Visual-Interactive Labeling' (VIAL) process, conflating the strengths of both. We describe the six major steps of the process and highlight their related challenges.},
booktitle = {Proceedings of the EuroVis Workshop on Visual Analytics},
pages = {73–77},
numpages = {5},
location = {Barcelona, Spain},
series = {EuroVA '17}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papertorsneyweir2017sliceplorer"
    >
        
            <img
                id="imagetorsneyweir2017sliceplorer"
                title="Click to enlarge and show details"
                onclick="toggleClass('papertorsneyweir2017sliceplorer', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/torsneyweir2017sliceplorer.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papertorsneyweir2017sliceplorer', 'small'); toggleImageSize(imagetorsneyweir2017sliceplorer);"
                title="Click to show details"
            >
                Sliceplorer: 1D Slices for Multi-dimensional Continuous Functions
            </h3>  <a class="anchor" name="torsneyweir2017sliceplorer"></a>
            <div class="authors">
                <span class="firstAuthor">Thomas Torsney-Weir</span>,
                Michael Sedlmair, Torsten Möller
            </div>
            <div>
                <span class="publication">CGF 2017</span>
                
                <a href="../pdf/torsneyweir2017sliceplorer.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1111/cgf.13177" target="_blank">website</a>
                <a href="https://www.youtube.com/watch?v=VSHadt-jB-s" target="_blank">video</a>
                <a href="http://sliceplorer.cs.univie.ac.at/evaluation/index.html" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Multi-dimensional continuous functions are commonly visualized with 2D slices or topological views. Here, we explore 1D slices as an alternative approach to show such functions. Our goal with 1D slices is to combine the benefits of topological views, that is, screen space efficiency, with those of slices, that is a close resemblance of the underlying function. We compare 1D slices to 2D slices and topological views, first, by looking at their performance with respect to common function analysis tasks. We also demonstrate 3 usage scenarios: the 2D sinc function, neural network regression, and optimization traces. Based on this evaluation, we characterize the advantages and drawbacks of each of these approaches, and show how interaction can be used to overcome some of the shortcomings.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{10.1111/cgf.13177,
author = {Torsney-Weir, T. and Sedlmair, M. and M\"{o}ller, T.},
title = {Sliceplorer: 1D Slices for Multi-Dimensional Continuous Functions},
year = {2017},
issue_date = {June 2017},
publisher = {The Eurographs Association &amp; John Wiley &amp; Sons, Ltd.},
address = {Chichester, GBR},
volume = {36},
number = {3},
issn = {0167-7055},
url = {https://doi.org/10.1111/cgf.13177},
doi = {10.1111/cgf.13177},
abstract = {Multi-dimensional continuous functions are commonly visualized with 2D slices or topological views. Here, we explore 1D slices as an alternative approach to show such functions. Our goal with 1D slices is to combine the benefits of topological views, that is, screen space efficiency, with those of slices, that is a close resemblance of the underlying function. We compare 1D slices to 2D slices and topological views, first, by looking at their performance with respect to common function analysis tasks. We also demonstrate 3 usage scenarios: the 2D sinc function, neural network regression, and optimization traces. Based on this evaluation, we characterize the advantages and drawbacks of each of these approaches, and show how interaction can be used to overcome some of the shortcomings.},
journal = {Comput. Graph. Forum},
month = {jun},
pages = {167–177},
numpages = {11},
keywords = {Categories and Subject Descriptors according to ACM CCS, I.3.3 [Computer Graphics]: Picture/Image Generation-Line and curve generation}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperrudkowsy2017sentiment"
    >
        
            <img
                id="imagerudkowsy2017sentiment"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperrudkowsy2017sentiment', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/rudkowsy2017sentiment.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperrudkowsy2017sentiment', 'small'); toggleImageSize(imagerudkowsy2017sentiment);"
                title="Click to show details"
            >
                Supervised Sentiment Analysis of Parliamentary Speeches and News Reports
            </h3>  <a class="anchor" name="rudkowsy2017sentiment"></a>
            <div class="authors">
                <span class="firstAuthor">Elena Rudkowsky</span>,
                Martin Haselmayer, Matthias Wastian, Marcelo Jenny, Stefan Emrich, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ICA 2017</span>
                
                <a href="../pdf/rudkowsy2017sentiment.pdf" target="_blank">PDF</a>
                
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we use several supervised machine learning approaches and compare their success in predicting the sentiment of Austrian parliamentary speeches and news reports (German language). Prediction results in learning- based sentiment analysis vary strongly. They depend on the choice of algorithm and its parameterization, the quality and quantity of available training data as well as the selection of appropriate input feature representations. Our training data contains human-annotated sentiment scores at the phrase and sentence level. Going beyond the dominant bag-of-words modeling approach in traditional natural language processing, we also test sentiment analysis for neural network-based distributed representations of words. The latter reflect syntactic as well as semantic relatedness, but require huge amounts of training examples. We test both approaches with heterogeneous textual data, compare their success rates and provide conclusions on how to improve the sentiment analysis of political communication.

            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{rudkowskysupervised,
  title={Supervised Sentiment Analysis of Parliamentary Speeches and News Reports},
  author={Rudkowsky, Elena and Haselmayer, Martin and Wastian, Matthias and Jenny, Marcelo and Emrich, {\v{S}}tefan and Sedlmair, Michael}
}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperwang2017tvcg"
    >
        
            <img
                id="imagewang2017tvcg"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperwang2017tvcg', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/wang2017tvcg.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperwang2017tvcg', 'small'); toggleImageSize(imagewang2017tvcg);"
                title="Click to show details"
            >
                A Perception-Driven Approach to Supervised Dimensionality Reduction for Visualization
            </h3>  <a class="anchor" name="wang2017tvcg"></a>
            <div class="authors">
                <span class="firstAuthor">Yunhai Wang</span>,
                Kang Feng, Xiaowei Chu, Jian Zhang, Chi-Wing Fu, Michael Sedlmair, Xiaohui Yu, Baoquan Chen
            </div>
            <div>
                <span class="publication">TVCG 2017</span>
                
                <a href="../pdf/wang2017tvcg.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2017.2701829" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Dimensionality reduction (DR) is a common strategy for visual analysis of labeled high-dimensional data. Low-dimensional representations of the data help, for instance, to explore the class separability and the spatial distribution of the data. Widely-used unsupervised DR methods like PCA do not aim to maximize the class separation, while supervised DR methods like LDA often assume certain spatial distributions and do not take perceptual capabilities of humans into account. These issues make them ineffective for complicated class structures. Towards filling this gap, we present a perception-driven linear dimensionality reduction approach that maximizes the perceived class separation in projections. Our approach builds on recent developments in perception-based separation measures that have achieved good results in imitating human perception. We extend these measures to be density-aware and incorporate them into a customized simulated annealing algorithm, which can rapidly generate a near optimal DR projection. We demonstrate the effectiveness of our approach by comparing it to state-of-the-art DR methods on 93 datasets, using both quantitative measure and human judgments. We also provide case studies with class-imbalanced and unlabeled data.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{7920403,
  author={Wang, Yunhai and Feng, Kang and Chu, Xiaowei and Zhang, Jian and Fu, Chi-Wing and Sedlmair, Michael and Yu, Xiaohui and Chen, Baoquan},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={A Perception-Driven Approach to Supervised Dimensionality Reduction for Visualization}, 
  year={2018},
  volume={24},
  number={5},
  pages={1828-1840},
  doi={10.1109/TVCG.2017.2701829}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperjenny2017incivility"
    >
        
            <img
                id="imagejenny2017incivility"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperjenny2017incivility', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/jenny2017incivility.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperjenny2017incivility', 'small'); toggleImageSize(imagejenny2017incivility);"
                title="Click to show details"
            >
                Incivility in Austrian parliamentary debates: A supervised sentiment analysis of parliamentary speeches
            </h3>  <a class="anchor" name="jenny2017incivility"></a>
            <div class="authors">
                <span class="firstAuthor">Marcelo Jenny</span>,
                Martin Haselmayer, Elena Rudkowsky, Matthias Wastian, Stefan Emrich, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ECPR 2017</span>
                
                <a href="../pdf/jenny2017incivility.pdf" target="_blank">PDF</a>
                
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Incivility of political communication has become a major topic in public and scientific discourse (e.g. Herbst 2010; Berry and Sobieraj 2013), and it is often seen as a cause of increasing political polarization, lower electoral turnout and voter disaffection with politics and democracy in general (Jamieson 1992; Kahn and Kenny 1999; Mutz and Reeves 2005, Mutz 2007; Brooks and Geer 2007; Lau and Rovner 2009; Harcourt 2012). However, there is no agreement on the definition or measurement of incivility. Our paper presents an automated sentiment analysis to identify uncivil language and to measure the level of (in)civility in parliamentary speeches. Substantively, we study incivility in the Austrian national parliament during the last two decades (1996-2013) and explore some of the political, institutional and individual factors that affect the level of incivility shown in parliamentary debates. We check whether government/opposition status, the parliamentary role, the type of debate and closeness to the next election has an effect on the level of civility observed in parliament.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{jenny2017incivility,
  title={Incivility in Austrian Parliamentary Debates: A supervised sentiment analysis of parliamentary speeches},
  author={Jenny, Marcelo and Haselmayer, Martin and Rudkowsky, Elena and Wastian, Matthias and Emrich, Stefan and Sedlmair, Michael},
  booktitle={European Consortium for Political Research Joint Workshops, Nottingham, UK},
  year={2017}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2016
    </h2>
    
    <div
        class="paper small"
        id="paperisenberg2017tvcg"
    >
        
            <img
                id="imageisenberg2017tvcg"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperisenberg2017tvcg', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/isenberg2017tvcg.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperisenberg2017tvcg', 'small'); toggleImageSize(imageisenberg2017tvcg);"
                title="Click to show details"
            >
                vispubdata.org: A Metadata Collection about IEEE Visualization (VIS) Publications
            </h3>  <a class="anchor" name="isenberg2017tvcg"></a>
            <div class="authors">
                <span class="firstAuthor">Petra Isenberg</span>,
                Florian Heimerl, Steffen Koch, Tobias Isenberg, Panpan Xu, Chad Stolper, Michael Sedlmair, Jian Chen, Torsten Möller, John Stasko
            </div>
            <div>
                <span class="publication">TVCG 2016</span>
                
                <a href="../pdf/isenberg2017tvcg.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2016.2615308" target="_blank">website</a>
                
                <a href="https://vispubdata.org" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{7583708,
  author={Isenberg, Petra and Heimerl, Florian and Koch, Steffen and Isenberg, Tobias and Xu, Panpan and Stolper, Charles D. and Sedlmair, Michael and Chen, Jian and Möller, Torsten and Stasko, John},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Vispubdata.org: A Metadata Collection About IEEE Visualization (VIS) Publications}, 
  year={2017},
  volume={23},
  number={9},
  pages={2199-2206},
  doi={10.1109/TVCG.2016.2615308}}
</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperisenberg2016tvcg"
    >
        
            <img
                id="imageisenberg2016tvcg"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperisenberg2016tvcg', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/isenberg2016tvcg.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperisenberg2016tvcg', 'small'); toggleImageSize(imageisenberg2016tvcg);"
                title="Click to show details"
            >
                Visualization as Seen Through its Research Paper Keywords
            </h3>  <a class="anchor" name="isenberg2016tvcg"></a>
            <div class="authors">
                <span class="firstAuthor">Petra Isenberg</span>,
                Tobias Isenberg, Michael Sedlmair, Jian Chen, Möller Torsten
            </div>
            <div>
                <span class="publication">TVCG 2016</span>
                
                <a href="../pdf/isenberg2016tvcg.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2016.2598827" target="_blank">website</a>
                
                <a href="http://tobias.isenberg.cc/uploads/VideosAndDemos/Isenberg_2017_VST_additional.zip" target="_blank">supplemental</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present the results of a comprehensive multi-pass analysis of visualization paper keywords supplied by authors for their papers published in the IEEE Visualization conference series (now called IEEE VIS) between 1990-2015. From this analysis we derived a set of visualization topics that we discuss in the context of the current taxonomy that is used to categorize papers and assign reviewers in the IEEE VIS reviewing process. We point out missing and overemphasized topics in the current taxonomy and start a discussion on the importance of establishing common visualization terminology. Our analysis of research topics in visualization can, thus, serve as a starting point to (a) help create a common vocabulary to improve communication among different visualization sub-groups, (b) facilitate the process of understanding differences and commonalities of the various research sub-fields in visualization, (c) provide an understanding of emerging new research trends, (d) facilitate the crucial step of finding the right reviewers for research submissions, and (e) it can eventually lead to a comprehensive taxonomy of visualization research. One additional tangible outcome of our work is an online query tool (http://keyvis.org/) that allows visualization researchers to easily browse the 3952 keywords used for IEEE VIS papers since 1990 to find related work or make informed keyword choices.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{7539364,
  author={Isenberg, Petra and Isenberg, Tobias and Sedlmair, Michael and Chen, Jian and Möller, Torsten},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Visualization as Seen through its Research Paper Keywords}, 
  year={2017},
  volume={23},
  number={1},
  pages={771-780},
  doi={10.1109/TVCG.2016.2598827}}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papersacha2016vast"
    >
        
            <img
                id="imagesacha2016vast"
                title="Click to enlarge and show details"
                onclick="toggleClass('papersacha2016vast', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/sacha2016vast.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papersacha2016vast', 'small'); toggleImageSize(imagesacha2016vast);"
                title="Click to show details"
            >
                Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis
            </h3>  <a class="anchor" name="sacha2016vast"></a>
            <div class="authors">
                <span class="firstAuthor">Dominik Sacha</span>,
                Leishi Zhang, Michael Sedlmair, John Aldo Lee, Jaakko Peltonen, Daniel Weiskopf, Stephen North, Daniel A Keim
            </div>
            <div>
                <span class="publication">TVCG 2016</span>
                
                <a href="../pdf/sacha2016vast.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2016.2598495" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Dimensionality Reduction (DR) is a core building block in visualizing multidimensional data. For DR techniques to be useful in exploratory data analysis, they need to be adapted to human needs and domain-specific problems, ideally, interactively, and on-the-fly. Many visual analytics systems have already demonstrated the benefits of tightly integrating DR with interactive visualizations. Nevertheless, a general, structured understanding of this integration is missing. To address this, we systematically studied the visual analytics and visualization literature to investigate how analysts interact with automatic DR techniques. The results reveal seven common interaction scenarios that are amenable to interactive control such as specifying algorithmic constraints, selecting relevant features, or choosing among several DR algorithms. We investigate specific implementations of visual analysis systems integrating DR, and analyze ways that other machine learning methods have been combined with DR. Summarizing the results in a “human in the loop” process model provides a general lens for the evaluation of visual interactive DR systems. We apply the proposed model to study and classify several systems previously described in the literature, and to derive future research opportunities.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@ARTICLE{7536217,
  author={Sacha, Dominik and Zhang, Leishi and Sedlmair, Michael and Lee, John A. and Peltonen, Jaakko and Weiskopf, Daniel and North, Stephen C. and Keim, Daniel A.},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis}, 
  year={2017},
  volume={23},
  number={1},
  pages={241-250},
  doi={10.1109/TVCG.2016.2598495}}
</textarea>
            </div>
            
        </div>
    </div>
    
            </article>
        </div>
    </main>
</body>
</html>