<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Michael Sedlmair | VISVAR Research Group, University of Stuttgart</title>
    <link rel="stylesheet" href="../style.css">
    <script src="../script.js"></script>
    <link rel="shortcut icon" href="../img/favicon.png">
    <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
</head>
<body>
    <a class="anchor" name="top"></a>
    <main>
        <div>
            
<header>
    <div>
        <a href="https://visvar.github.io/">
            <h1 class="h1desktop">
                <div>
                    VISVAR
                </div>
                <div>
                    Research
                </div>
                <div>
                    Group
                </div>
            </h1>
            <h1 class="h1mobile">
                VISVAR
            </h1>
        </a>
    </div>
    <div>
        <nav>
            <ul>
                <li>
                    <a href="https://visvar.github.io/#aboutus">about VISVAR</a>
                </li>
                <li>
                    <a href="https://visvar.github.io/#publications">all publications</a>
                </li>
                <li class="memberNav">
                    <a href="https://visvar.github.io/#members">members</a>
                </li>
                <ul class="memberNav">
                    
                        <li>
                            <a href="https://visvar.github.io/members/aimee_sousa_calepso.html">
                                Aimee Sousa Calepso
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/alexander_achberger.html">
                                Alexander Achberger
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/frank_heyen.html">
                                Frank Heyen
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/katrin_angerbauer.html">
                                Katrin Angerbauer
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/melissa_reinelt.html">
                                Melissa Reinelt
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/michael_sedlmair.html">
                                Michael Sedlmair
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/natalie_hube.html">
                                Natalie Hube
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/quynh_ngo.html">
                                Quynh Ngo
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/rene_cutura.html">
                                Rene Cutura
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/ruben_bauer.html">
                                Ruben Bauer
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/xingyao_yu.html">
                                Xingyao Yu
                            </a>
                        </li>
                    
                </ul>
            </ul>
        </nav>
    </div>
</header>
        </div>
        <div>
            <article> <a class="anchor" name="aboutus"></a>
                <h1>
    Prof. Dr. Michael Sedlmair
</h1>

<div class="aboutMember">

    <div class="avatarAndBio">
        <img class="avatar" src="../img/michael_sedlmair.jpg" />

        <div class="bio">
            <p>
            </p>
        </div>
    </div>

    <p>
        <a href="https://www.vis.uni-stuttgart.de/en/institute/team/Sedlmair-00002/">Institute website</a>
    </p>

    <h2>Research Interests</h2>
    <ul>
        <li>Visualization &amp; visual analytics</li>
        <li>VR/AR</li>
        <li>HCI</li>
    </ul>

</div>

            </article>
            <article> <a class="anchor" name="publications"></a>
                <h1>Publications</h1>
                
    <div
        class="paper small"
        id="paperrau2021visual"
    >
        <h2
           onclick="toggleClass('paperrau2021visual', 'small'); toggleImageSize(imagerau2021visual);"
        >
            Visual Support for Human-AI Co-Composition
        </h2>
        
            <img
                id="imagerau2021visual"
                onclick="toggleClass('paperrau2021visual', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/rau2021visual.png"
            />
        <div class="metaData ">
            <div class="authors">
                <span class="firstAuthor">Simeon Rau, Frank Heyen</span>,
                Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMIR 2021</span>
                <span class="publication">Late-Breaking Demo Poster</span>
                <a href="../pdf/rau2021visual.pdf" target="_blank">PDF</a>
                <a href="https://archives.ismir.net/ismir2021/latebreaking/000014.pdf" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose a visual approach for AI-assisted music composition, where the user interactively generates, selects, and adapts short melodies. Based on an entered start melody, we automatically generate multiple continuation samples. Repeating this step and in turn generating continuations for these samples results in a tree or graph of melodies. We visualize this structure with two visualizations, where nodes display the piano roll of the corresponding sample. By interacting with these visualizations, the user can quickly listen to, choose, and adapt melodies, to iteratively create a composition. A third visualization provides an overview over larger numbers of samples, allowing for insights into the AI's predictions and the sample space.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{rau2021visual,
  title={Visual Support for Human-{AI} Co-Composition},
  author={Rau, Simeon and Heyen, Frank and Sedlmair, Michael},
  year={2021},
  booktitle={Extended Abstracts for the Late-Breaking Demo Session of the 22nd Int. Society for Music Information Retrieval Conf. (ISMIR)},
  url={https://archives.ismir.net/ismir2021/latebreaking/000014.pdf}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was funded by the Cyber Valley Research Fund – Project InstruData.
            </div>
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperhube2021vr"
    >
        <h2
           onclick="toggleClass('paperhube2021vr', 'small'); toggleImageSize(imagehube2021vr);"
        >
            VR Collaboration in Large Companies: An Interview Study on the Role of Avatars
        </h2>
        
        <div class="metaData noImage">
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Katrin Angerbauer, Daniel Pohlandt, Krešimir Vidačković, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2021</span>
                <span class="publication">Poster / Short Paper</span>
                
                <a href="" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Collaboration is essential in companies and often physical presence is required, thus, more and more Virtual Reality (VR) systems are used to work together remotely. To support social interaction, human representations in form of avatars are used in collaborative virtual environment (CVE) tools. However, up to now, the avatar representations often are limited in their design and functionality, which may hinder effective collaboration. In our interview study, we explored the status quo of VR collaboration in a large automotive company setting with a special focus on the role of avatars. We collected interview data from 21 participants, from which we identified challenges of current avatar representations used in our setting. Based on these findings, we discuss design suggestions for avatars in a company setting, which aim to improve social interaction. As opposed to state-of-the-art research, we found that users within the context of a large automotive company have an altered need with respect to avatar representations.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperrijken2021illegible"
    >
        <h2
           onclick="toggleClass('paperrijken2021illegible', 'small'); toggleImageSize(imagerijken2021illegible);"
        >
            Illegible Semantics: Exploring the Design Space of Metal Logos
        </h2>
        
            <img
                id="imagerijken2021illegible"
                onclick="toggleClass('paperrijken2021illegible', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/rijken2021illegible.png"
            />
        <div class="metaData ">
            <div class="authors">
                <span class="firstAuthor">Gerrit J. Rijken</span>,
                Rene Cutura, Frank Heyen, Michael Sedlmair, Michael Correll, Jason Dykes, Noeska Smit
            </div>
            <div>
                <span class="publication">alt.VIS 2021</span>
                
                <a href="https://arxiv.org/ftp/arxiv/papers/2109/2109.01688.pdf" target="_blank">PDF</a>
                <a href="https://arxiv.org/abs/2109.01688" target="_blank">publisher website</a>
                <a href="https://www.youtube.com/watch?v=BZOdIhU-mrA" target="_blank">video</a>
                <a href="http://illegiblesemantics.com" target="_blank">supplemental material</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The logos of metal bands can be by turns gaudy, uncouth, or nearly illegible. Yet, these logos work: they communicate sophisticated notions of genre and emotional affect. In this paper we use the design considerations of metal logos to explore the space of “illegible semantics”: the ways that text can communicate information at the cost of readability, which is not always the most important objective. In this work, drawing on formative visualization theory, professional design expertise, and empirical assessments of a corpus ofmetal band logos, we describe a design space of metal logos and present a tool through which logo characteristics can be explored through visualization. We investigate ways in which logo designers imbue their text with meaning and consider opportunities and implications for visualization more widely.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@misc{rijken2021illegible,
      title={Illegible Semantics: Exploring the Design Space of Metal Logos}, 
      author={Gerrit J. Rijken and Rene Cutura and Frank Heyen and Michael Sedlmair and Michael Correll and Jason Dykes and Noeska Smit},
      year={2021},
      eprint={2109.01688},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}</textarea>
            </div>
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="papermorariu2021dumbledr"
    >
        <h2
           onclick="toggleClass('papermorariu2021dumbledr', 'small'); toggleImageSize(imagemorariu2021dumbledr);"
        >
            DumbleDR: Predicting User Preferences of Dimensionality Reduction Projection Quality
        </h2>
        
        <div class="metaData noImage">
            <div class="authors">
                <span class="firstAuthor">Cristina Morariu</span>,
                Adrien Bibal, Rene Cutura, Benoît Frénay, Michael Sedlmair
            </div>
            <div>
                <span class="publication">arXiv 2021</span>
                
                <a href="https://arxiv.org/pdf/2105.09275.pdf" target="_blank">PDF</a>
                <a href="https://arxiv.org/abs/2105.09275" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional parametrization (e.g. t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D projections is usually qualitatively decided, by setting projections side-by-side and letting human judgment decide which projection is the best. In this work, we propose a quantitative way of evaluating projections, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select 'good' and 'misleading' views between scatterplots of low-level projections of image datasets, simulating the way people usually select projections. We use the study data as labels for a set of quality metrics whose purpose is to discover and quantify what exactly people are looking for when deciding between projections. With this proxy for human judgments, we use it to rank projections on new datasets, explain why they are relevant, and quantify the degree of subjectivity in projections selected.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperyu2020perspective"
    >
        <h2
           onclick="toggleClass('paperyu2020perspective', 'small'); toggleImageSize(imageyu2020perspective);"
        >
            Perspective Matters: Design Implications for Motion Guidance in Mixed Reality
        </h2>
        
            <img
                id="imageyu2020perspective"
                onclick="toggleClass('paperyu2020perspective', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/yu2020perspective.png"
            />
        <div class="metaData ">
            <div class="authors">
                <span class="firstAuthor">Xingyao Yu</span>,
                Katrin Angerbauer, Peter Mohr, Denis Kalkofen, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2020</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/yu2020perspective.pdf" target="_blank">PDF</a>
                <a href="https://ieeexplore.ieee.org/abstract/document/9284729" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We investigate how Mixed Reality (MR) can be used to guide human body motions, such as in physiotherapy, dancing, or workout applications. While first MR prototypes have shown promising results, many dimensions of the design space behind such applications remain largely unexplored. To better understand this design space, we approach the topic from different angles by contributing three user studies. In particular, we take a closer look at the influence of the perspective, the characteristics of motions, and visual guidance on different user performance measures. Our results indicate that a first-person perspective performs best for all visible motions, whereas the type of visual instruction plays a minor role. From our results we compile a set of considerations that can guide future work on the design of instructions, evaluations, and the technical setup of MR motion guidance systems.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperhube2020comparing"
    >
        <h2
           onclick="toggleClass('paperhube2020comparing', 'small'); toggleImageSize(imagehube2020comparing);"
        >
            Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion
        </h2>
        
        <div class="metaData noImage">
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Oliver Lenz, Lars Engeln, Rainer Groh, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2020</span>
                <span class="publication">Poster / Short Paper</span>
                
                <a href="https://ieeexplore.ieee.org/abstract/document/9288476/" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperheyen2020supporting"
    >
        <h2
           onclick="toggleClass('paperheyen2020supporting', 'small'); toggleImageSize(imageheyen2020supporting);"
        >
            Supporting Music Education through Visualizations of MIDI Recordings
        </h2>
        
            <img
                id="imageheyen2020supporting"
                onclick="toggleClass('paperheyen2020supporting', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/heyen2020supporting.png"
            />
        <div class="metaData ">
            <div class="authors">
                <span class="firstAuthor">Frank Heyen</span>,
                Michael Sedlmair
            </div>
            <div>
                <span class="publication">VIS 2020</span>
                <span class="publication">Poster</span>
                <a href="../pdf/heyen2020supporting.pdf" target="_blank">PDF</a>
                <a href="https://vis2020-ieee.ipostersessions.com/default.aspx?s=82-F0-FF-F9-29-B9-B4-7F-FE-F3-A9-1D-4A-B7-4F-32" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Musicians mostly have to rely on their ears when they want to analyze what they play, for example to detect errors. Since hearing is sequential, it is not possible to quickly grasp an overview over one or multiple recordings of a whole piece of music at once. We therefore propose various visualizations that allow analyzing errors and stylistic variance. Our current approach focuses on rhythm and uses MIDI data for simplicity.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperbalestrucci2020pipelines"
    >
        <h2
           onclick="toggleClass('paperbalestrucci2020pipelines', 'small'); toggleImageSize(imagebalestrucci2020pipelines);"
        >
            Pipelines Bent, Pipelines Broken: Interdisciplinary Self-Reflection on the Impact of COVID-19 on Current and Future Research (Position Paper)
        </h2>
        
        <div class="metaData noImage">
            <div class="authors">
                <span class="firstAuthor">Priscilla Balestrucci</span>,
                Katrin Angerbauer, Cristina Morariu, Robin Welsch, Lewis L Chuang, Daniel Weiskopf, Marc O Ernst, Michael Sedlmair
            </div>
            <div>
                <span class="publication">BELIV 2020</span>
                <span class="publication">Workshop</span>
                
                <a href="https://ieeexplore.ieee.org/abstract/document/9307759" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Among the many changes brought about by the COVID-19 pandemic, one of the most pressing for scientific research concerns user testing. For the researchers who conduct studies with human participants, the requirements for social distancing have created a need for reflecting on methodologies that previously seemed relatively straightforward. It has become clear from the emerging literature on the topic and from first-hand experiences of researchers that the restrictions due to the pandemic affect every aspect of the research pipeline. The current paper offers an initial reflection on user-based research, drawing on the authors' own experiences and on the results of a survey that was conducted among researchers in different disciplines, primarily psychology, human-computer interaction (HCI), and visualization communities. While this sampling of researchers is by no means comprehensive, the multi-disciplinary approach and the consideration of different aspects of the research pipeline allow us to examine current and future challenges for user-based research. Through an exploration of these issues, this paper also invites others in the VIS-as well as in the wider-research community, to reflect on and discuss the ways in which the current crisis might also present new and previously unexplored opportunities.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="papercutura2020druidjs"
    >
        <h2
           onclick="toggleClass('papercutura2020druidjs', 'small'); toggleImageSize(imagecutura2020druidjs);"
        >
            DRUIDJS — A JavaScript Library for Dimensionality Reduction
        </h2>
        
            <img
                id="imagecutura2020druidjs"
                onclick="toggleClass('papercutura2020druidjs', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/cutura2020druidjs.png"
            />
        <div class="metaData ">
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Christoph Kralj, Michael Sedlmair
            </div>
            <div>
                <span class="publication">VIS 2020</span>
                <span class="publication">Short Paper</span>
                <a href="../pdf/cutura2020druidjs.pdf" target="_blank">PDF</a>
                <a href="https://ieeexplore.ieee.org/abstract/document/9331283" target="_blank">publisher website</a>
                <a href="https://youtu.be/LyiqHl4rq34" target="_blank">video</a>
                <a href="https://renecutura.eu/pdfs/Druid_Supp.pdf" target="_blank">supplemental material</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Dimensionality reduction (DR) is a widely used technique for visualization. Nowadays, many of these visualizations are developed for the web, most commonly using JavaScript as the underlying programming language. So far, only few DR methods have a JavaScript implementation though, necessitating developers to write wrappers around implementations in other languages. In addition, those DR methods that exist in JavaScript libraries, such as PCA, t-SNE, and UMAP, do not offer consistent programming interfaces, hampering the quick integration of different methods. Toward a coherent and comprehensive DR programming framework, we developed an open source JavaScript library named DruidJS. Our library contains implementations of ten different DR algorithms, as well as the required linear algebra techniques, tools, and utilities.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2020druid,
  title={{DRUIDJS — A JavaScript Library for Dimensionality Reduction}},
  author={Cutura, Rene and Kralj, Christoph and Sedlmair, Michael},
  booktitle={2020 IEEE Visualization Conference (VIS)},
  pages={111--115},
  year={2020},
  organization={IEEE}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was supported by the BMVIT ICT of the Future program via the ViSciPub project (no. 867378) and handled by the FFG.
            </div>
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperweiß2020revisited"
    >
        <h2
           onclick="toggleClass('paperweiß2020revisited', 'small'); toggleImageSize(imageweiß2020revisited);"
        >
            Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes
        </h2>
        
        <div class="metaData noImage">
            <div class="authors">
                <span class="firstAuthor">Maximilian Weiß</span>,
                Katrin Angerbauer, Alexandra Voit, Magdalena Schwarzl, Michael Sedlmair, Sven Mayer
            </div>
            <div>
                <span class="publication">VIS 2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://ieeexplore.ieee.org/abstract/document/9225008" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperheyen2020clavis"
    >
        <h2
           onclick="toggleClass('paperheyen2020clavis', 'small'); toggleImageSize(imageheyen2020clavis);"
        >
            ClaVis: An Interactive Visual Comparison System for Classifiers
        </h2>
        
            <img
                id="imageheyen2020clavis"
                onclick="toggleClass('paperheyen2020clavis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/heyen2020clavis.png"
            />
        <div class="metaData ">
            <div class="authors">
                <span class="firstAuthor">Frank Heyen</span>,
                Tanja Munz, Michael Neumann, Daniel Ortega, Ngoc Thang Vu, Daniel Weiskopf, Michael Sedlmair
            </div>
            <div>
                <span class="publication">AVI 2020</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/heyen2020clavis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3399715.3399814" target="_blank">publisher website</a>
                
                <a href="https://github.com/fheyen/clavis" target="_blank">supplemental material</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose ClaVis, a visual analytics system for comparative analysis of classification models. ClaVis allows users to visually compare the performance and behavior of tens to hundreds of classifiers trained with different hyperparameter configurations. Our approach is plugin-based and classifier-agnostic and allows users to add their own datasets and classifier implementations. It provides multiple visualizations, including a multivariate ranking, a similarity map, a scatterplot that reveals correlations between parameters and scores, and a training history chart. We demonstrate the effectivity of our approach in multiple case studies for training classification models in the domain of natural language processing.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.1145/3399715.3399814,
author = {Heyen, Frank and Munz, Tanja and Neumann, Michael and Ortega, Daniel and Vu, Ngoc Thang and Weiskopf, Daniel and Sedlmair, Michael},
title = {ClaVis: An Interactive Visual Comparison System for Classifiers},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399814},
doi = {10.1145/3399715.3399814},
abstract = {We propose ClaVis, a visual analytics system for comparative analysis of classification
models. ClaVis allows users to visually compare the performance and behavior of tens
to hundreds of classifiers trained with different hyperparameter configurations. Our
approach is plugin-based and classifier-agnostic and allows users to add their own
datasets and classifier implementations. It provides multiple visualizations, including
a multivariate ranking, a similarity map, a scatterplot that reveals correlations
between parameters and scores, and a training history chart. We demonstrate the effectivity
of our approach in multiple case studies for training classification models in the
domain of natural language processing.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {9},
numpages = {9},
keywords = {visual analytics, Visualization, machine learning, classifier comparison},
location = {Salerno, Italy},
series = {AVI '20}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (A08) and under Germany’s Excellence Strategy – EXC-2075 – 39074001
            </div>
        </div>
    </div>
    
    <div
        class="paper small"
        id="papercutura2020comparing"
    >
        <h2
           onclick="toggleClass('papercutura2020comparing', 'small'); toggleImageSize(imagecutura2020comparing);"
        >
            Comparing and Exploring High-Dimensional Data with Dimensionality Reduction Algorithms and Matrix Visualizations
        </h2>
        
            <img
                id="imagecutura2020comparing"
                onclick="toggleClass('papercutura2020comparing', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/cutura2020comparing.png"
            />
        <div class="metaData ">
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Michaël Aupetit, Jean-Daniel Fekete, Michael Sedlmair
            </div>
            <div>
                <span class="publication">AVI  2020</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/cutura2020comparing.pdf" target="_blank">PDF</a>
                <a href="https://dl.acm.org/doi/abs/10.1145/3399715.3399875" target="_blank">publisher website</a>
                <a href="https://youtu.be/UPkH7rc0ulU" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose Compadre, a tool for visual analysis for comparing distances of high-dimensional (HD) data and their low-dimensional projections. At the heart is a matrix visualization to represent the discrepancy between distance matrices, linked side-by-side with 2D scatterplot projections of the data. Using different examples and datasets, we illustrate how this approach fosters (1) evaluating dimensionality reduction techniques w.r.t. how well they project the HD data, (2) comparing them to each other side-by-side, and (3) evaluate important data features through subspace comparison. We also present a case study, in which we analyze IEEE VIS authors from 1990 to 2018, and gain new insights on the relationships between coauthors, citations, and keywords. The coauthors are projected as accurately with UMAP as with t-SNE but the projections show different insights. The structure of the citation subspace is very different from the coauthor subspace. The keyword subspace is noisy yet consistent among the three IEEE VIS sub-conferences.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2020comparing,
  title={Comparing and exploring high-dimensional data with dimensionality reduction algorithms and matrix visualizations},
  author={Cutura, Rene and Aupetit, Micha{\"e}l and Fekete, Jean-Daniel and Sedlmair, Michael},
  booktitle={Proc. Intl. Conf. on Advanced Visual Interfaces (AVI)},
  pages={1--9},
  year={2020},
  doi={10.1145/3399715.3399875}}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was supported by the BMVIT ICT of the Future program via the ViSciPub project (no. 867378) and handled by the FFG.
            </div>
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperachberger2020caarvida"
    >
        <h2
           onclick="toggleClass('paperachberger2020caarvida', 'small'); toggleImageSize(imageachberger2020caarvida);"
        >
            Caarvida: Visual Analytics for Test Drive Videos
        </h2>
        
        <div class="metaData noImage">
            <div class="authors">
                <span class="firstAuthor">Alexander Achberger</span>,
                Rene Cutura, Oguzhan Türksoy, Michael Sedlmair
            </div>
            <div>
                <span class="publication">AVI  2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3399715.3399862" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperstreichert2020comparing"
    >
        <h2
           onclick="toggleClass('paperstreichert2020comparing', 'small'); toggleImageSize(imagestreichert2020comparing);"
        >
            Comparing Input Modalities for Shape Drawing Tasks
        </h2>
        
        <div class="metaData noImage">
            <div class="authors">
                <span class="firstAuthor">Annalena Streichert</span>,
                Katrin Angerbauer, Magdalena Schwarzl, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ETVIS 2020</span>
                <span class="publication">Workshop / Short Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3379156.3391830" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N= 20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="papermerino2020toward"
    >
        <h2
           onclick="toggleClass('papermerino2020toward', 'small'); toggleImageSize(imagemerino2020toward);"
        >
            Toward Agile Situated Visualization: An Exploratory User Study
        </h2>
        
            <img
                id="imagemerino2020toward"
                onclick="toggleClass('papermerino2020toward', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/merino2020toward.png"
            />
        <div class="metaData ">
            <div class="authors">
                <span class="firstAuthor">Leonel Merino</span>,
                Boris Sotomayor-Gómez, Xingyao Yu, Ronie Salgado, Alexandre Bergel, Michael Sedlmair, Daniel Weiskopf
            </div>
            <div>
                <span class="publication">CHI 2020</span>
                <span class="publication">Extended Abstract</span>
                <a href="../pdf/merino2020toward.pdf" target="_blank">PDF</a>
                <a href="https://dl.acm.org/doi/abs/10.1145/3334480.3383017" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median ≥ 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperkurzhals2020a"
    >
        <h2
           onclick="toggleClass('paperkurzhals2020a', 'small'); toggleImageSize(imagekurzhals2020a);"
        >
            A View on the Viewer: Gaze-Adaptive Captions for Videos
        </h2>
        
        <div class="metaData noImage">
            <div class="authors">
                <span class="firstAuthor">Kuno Kurzhals</span>,
                Fabian Göbel, Katrin Angerbauer, Michael Sedlmair, Martin Raubal
            </div>
            <div>
                <span class="publication">CHI 2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376266" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="paperkraus2020assessing"
    >
        <h2
           onclick="toggleClass('paperkraus2020assessing', 'small'); toggleImageSize(imagekraus2020assessing);"
        >
            Assessing 2D and 3D Heatmaps for Comparative Analysis: An Empirical Study
        </h2>
        
        <div class="metaData noImage">
            <div class="authors">
                <span class="firstAuthor">Matthias Kraus</span>,
                Katrin Angerbauer, Juri Buchmüller, Daniel Schweitzer, Daniel A Keim, Michael Sedlmair, Johannes Fuchs
            </div>
            <div>
                <span class="publication">CHI 2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376675" target="_blank">publisher website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Heatmaps are a popular visualization technique that encode 2D density distributions using color or brightness. Experimental studies have shown though that both of these visual variables are inaccurate when reading and comparing numeric data values. A potential remedy might be to use 3D heatmaps by introducing height as a third dimension to encode the data. Encoding abstract data in 3D, however, poses many problems, too. To better understand this tradeoff, we conducted an empirical study (N=48) to evaluate the user performance of 2D and 3D heatmaps for comparative analysis tasks. We test our conditions on a conventional 2D screen, but also in a virtual reality environment to allow for real stereoscopic vision. Our main results show that 3D heatmaps are superior in terms of error rate when reading and comparing single data items. However, for overview tasks, the well-established 2D heatmap performs better.
            </div>
            
            
        </div>
    </div>
    
    <div
        class="paper small"
        id="papercutura2018viscoder"
    >
        <h2
           onclick="toggleClass('papercutura2018viscoder', 'small'); toggleImageSize(imagecutura2018viscoder);"
        >
            VisCoDeR: A Tool for Visually Comparing Dimensionality Reduction Algorithms
        </h2>
        
            <img
                id="imagecutura2018viscoder"
                onclick="toggleClass('papercutura2018viscoder', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/cutura2018viscoder.png"
            />
        <div class="metaData ">
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Stefan Holzer, Michaël Aupetit, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ESANN 2018</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/cutura2018viscoder.pdf" target="_blank">PDF</a>
                <a href="" target="_blank">publisher website</a>
                <a href="https://youtu.be/gg2pgv0xwmc" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose VisCoDeR, a tool that leverages comparative visualization to support learning and analyzing different dimensionality reduction (DR) methods. VisCoDeR fosters two modes. The Discover mode allows qualitatively comparing several DR results by juxtaposing and linking the resulting scatterplots. The Explore mode allows for analyzing hundreds of differently parameterized DR results in a quantitative way. We present use cases that show that our approach helps to understand similarities and differences between DR algorithms.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2018viscoder,
  title={{VisCoDeR: A Tool for Visually Comparing Dimensionality Reduction Algorithms}},
  author={Cutura, Rene and Holzer, Stefan and Aupetit, Micha{\"e}l and Sedlmair, Michael},
  booktitle={Euro. Symp. on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
  pages={641--646}
  year={2018}
}</textarea>
            </div>
            
        </div>
    </div>
    
            </article>
        </div>
    </main>
</body>
</html>