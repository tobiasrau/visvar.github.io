<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Xingyao Yu | VISVAR Research Group, University of Stuttgart</title>
  <link rel="stylesheet" href="../style.css">
  <script src="../script.js"></script>
  <link rel="shortcut icon" href="../img/favicon.png">
  <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
  <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
</head>
<body>
  <a class="anchor" name="top"></a>
  <main>
    <div>
      
<header>
  <div>
    <a href="https://visvar.github.io/">
      <h1 class="h1desktop"><div>VISVAR</div><div>Research</div><div>Group</div></h1>
      <h1 class="h1mobile">VISVAR</h1>
    </a>
  </div>
  <div>
    <nav>
      <ul>
        <li>
          <a href="https://visvar.github.io/#aboutus">about VISVAR</a>
        </li>
        <li>
          <a href="https://visvar.github.io/#publications">publications</a>
        </li>
        <li class="memberNav">
          <a href="https://visvar.github.io/#members">members</a>
        </li>
        <ul class="memberNav">
          
            <li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>
          
            <li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>
          
            <li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>
          
            <li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>
          
            <li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>
          
            <li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>
          
            <li><a href="https://visvar.github.io/members/markus_wieland.html">Markus Wieland</a></li>
          
            <li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>
          
            <li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>
          
            <li><a href="https://visvar.github.io/members/nina_doerr.html">Nina Dörr</a></li>
          
            <li><a href="https://visvar.github.io/members/quynh_quang_ngo.html">Quynh Quang Ngo</a></li>
          
            <li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>
          
            <li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>
          
            <li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>
          
            <li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>
          
            <li><a href="https://visvar.github.io/members/tobias_rau.html">Tobias Rau</a></li>
          
            <li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>
          
        </ul>
      </ul>
    </nav>
  </div>
</header>
    </div>
    <div>
      <article> <a class="anchor" name="aboutus"></a>
        <h1>
  Xingyao Yu, M.Sc.
</h1>

<div class="aboutMember">

  <div class="avatarAndBio">
    <img class="avatar" src="../img/people/xingyao_yu.jpg" />

    <div class="bio">
      <p>
        I am Xingyao Yu, a Ph.D. student in Visualization Research Center (VISUS) at University of Stuttgart and
        at Stuttgart Center of Simulation Science (SimTech).
        I got my Bachelor's degree in Applied Physics and Master's degree in Optical Engineering, both at
        Beijing Institute of Technology.
      </p>
      <p>
        My research interests include wearable interaction, virtual reality, and augmented reality.
        Specifically, I have been working on the motion guidance process and biomechanical visualization in
        VR/AR environments.
        Future work: Leverage VR/AR and ML/AI for this purpose.
      </p>
    </div>
  </div>

  <p>
    <a href="https://www.visus.uni-stuttgart.de/en/institute/team/Yu-00009/" target="_blank">Institute website</a>
  </p>

  <h2>Research Interests</h2>
  <ul>
    <li>VR/AR</li>
    <li>Immersive &amp; on-body visualization</li>
    <li>HCI</li>
  </ul>

  <h2>More</h2>
  <ul>
    <li>
      <a href="https://scholar.google.com/citations?user=9anpRnwAAAAJ" target="_blank">Google Scholar</a>
    </li>
    <li>
      <a href="https://orcid.org/0000-0002-4249-1755" target="_blank">ORCID</a>
    </li>
  </ul>

</div>

      </article>
      <article> <a class="anchor" name="publications"></a>
        <h1>Publications</h1>
        
  <h2>2022</h2>
  <div class="paper small" id="papergebhardt2022molecusense">
    
      <img
        id="imagegebhardt2022molecusense"
        title="Click to enlarge and show details"
        onclick="toggleClass('papergebhardt2022molecusense', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/gebhardt2022molecusense.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('papergebhardt2022molecusense', 'small'); toggleImageSize(imagegebhardt2022molecusense)"
        title="Click to show details"
      >
        MolecuSense: Using Force-Feedback Gloves for Creating and Interacting with Ball-and-Stick Molecules in VR<a class="anchor" name="gebhardt2022molecusense"></a>
      </h3>
      <div>
        Patrick Gebhardt, Xingyao Yu, Andreas Köhn, Michael Sedlmair
      </div>
      <div>
        arXiv (2022) Short Paper
        <a href="https://doi.org/10.48550/arXiv.2203.09577" target="_blank">website</a>
        <a href="https://arxiv.org/pdf/2203.09577.pdf" target="_blank">PDF</a>
        
        
        <a href="https://visvar.github.io/pub/gebhardt2022molecusense.html" target="_blank">direct link</a>
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We contribute MolecuSense, a virtual version of a physical molecule construction kit, based on visualization in Virtual Reality (VR) and interaction with force-feedback gloves. Targeting at chemistry education, our goal is to make virtual molecule structures more tangible. Results of an initial user study indicate that the VR molecular construction kit was positively received. Compared to a physical construction kit, the VR molecular construction kit is on the same level in terms of natural interaction. Besides, it fosters the typical digital advantages though, such as saving, exporting, and sharing of molecules. Feedback from the study participants has also revealed potential future avenues for tangible molecule visualizations. </div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@misc{https://doi.org/10.48550/arxiv.2203.09577,
  doi = {10.48550/ARXIV.2203.09577},
  url = {https://arxiv.org/abs/2203.09577},
  author = {Gebhardt, Patrick and Yu, Xingyao and Köhn, Andreas and Sedlmair, Michael},
  keywords = {Human-Computer Interaction (cs.HC), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {MolecuSense: Using Force-Feedback Gloves for Creating and Interacting with Ball-and-Stick Molecules in VR},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}</textarea></div>
      
    </div>
  </div>
  
  <h2>2020</h2>
  <div class="paper small" id="paperyu2020perspective">
    
      <img
        id="imageyu2020perspective"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperyu2020perspective', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/yu2020perspective.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperyu2020perspective', 'small'); toggleImageSize(imageyu2020perspective)"
        title="Click to show details"
      >
        Perspective Matters: Design Implications for Motion Guidance in Mixed Reality<a class="anchor" name="yu2020perspective"></a>
      </h3>
      <div>
        Xingyao Yu, Katrin Angerbauer, Peter Mohr, Denis Kalkofen, Michael Sedlmair
      </div>
      <div>
        ISMAR (2020) Full Paper
        <a href="https://doi.org/10.1109/ISMAR50242.2020.00085" target="_blank">website</a>
        <a href="../pdf/yu2020perspective.pdf" target="_blank">PDF</a>
        
        
        <a href="https://visvar.github.io/pub/yu2020perspective.html" target="_blank">direct link</a>
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We investigate how Mixed Reality (MR) can be used to guide human body motions, such as in physiotherapy, dancing, or workout applications. While first MR prototypes have shown promising results, many dimensions of the design space behind such applications remain largely unexplored. To better understand this design space, we approach the topic from different angles by contributing three user studies. In particular, we take a closer look at the influence of the perspective, the characteristics of motions, and visual guidance on different user performance measures. Our results indicate that a first-person perspective performs best for all visible motions, whereas the type of visual instruction plays a minor role. From our results we compile a set of considerations that can guide future work on the design of instructions, evaluations, and the technical setup of MR motion guidance systems.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@INPROCEEDINGS{9284729,
  author={Yu, Xingyao and Angerbauer, Katrin and Mohr, Peter and Kalkofen, Denis and Sedlmair, Michael},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Perspective Matters: Design Implications for Motion Guidance in Mixed Reality}, 
  year={2020},
  pages={577-587},
  doi={10.1109/ISMAR50242.2020.00085}}</textarea></div>
      
    </div>
  </div>
  
  
  <div class="paper small" id="papermerino2020toward">
    
      <img
        id="imagemerino2020toward"
        title="Click to enlarge and show details"
        onclick="toggleClass('papermerino2020toward', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/merino2020toward.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('papermerino2020toward', 'small'); toggleImageSize(imagemerino2020toward)"
        title="Click to show details"
      >
        Toward Agile Situated Visualization: An Exploratory User Study<a class="anchor" name="merino2020toward"></a>
      </h3>
      <div>
        Leonel Merino, Boris Sotomayor-Gómez, Xingyao Yu, Ronie Salgado, Alexandre Bergel, Michael Sedlmair, Daniel Weiskopf
      </div>
      <div>
        CHI (2020) Extended Abstract
        <a href="https://doi.org/10.1145/3334480.3383017" target="_blank">website</a>
        <a href="../pdf/merino2020toward.pdf" target="_blank">PDF</a>
        
        
        <a href="https://visvar.github.io/pub/merino2020toward.html" target="_blank">direct link</a>
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median ≥ 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3334480.3383017,
author = {Merino, Leonel and Sotomayor-G\'{o}mez, Boris and Yu, Xingyao and Salgado, Ronie and Bergel, Alexandre and Sedlmair, Michael and Weiskopf, Daniel},
title = {Toward Agile Situated Visualization: An Exploratory User Study},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3383017},
doi = {10.1145/3334480.3383017},
abstract = {We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median ≥ 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–7},
numpages = {7},
keywords = {situated visualization, augmented reality, user study},
location = {Honolulu, HI, USA},
series = {CHI EA '20}
}</textarea></div>
      
    </div>
  </div>
  
  <h2>2018</h2>
  <div class="paper small" id="paperyu2018effect">
    
      <img
        id="imageyu2018effect"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperyu2018effect', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/yu2018effect.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperyu2018effect', 'small'); toggleImageSize(imageyu2018effect)"
        title="Click to show details"
      >
        Effect of Using HMDs for One Hour on Preteens Visual Fatigue<a class="anchor" name="yu2018effect"></a>
      </h3>
      <div>
        Xingyao Yu, Dongdong Weng, Jie Guo, Haiyan Jiang, Yihua Bao
      </div>
      <div>
        ISMAR (2018) Poster
        <a href="https://doi.org/10.1109/ISMAR-Adjunct.2018.00042" target="_blank">website</a>
        <a href="../pdf/yu2018effect.pdf" target="_blank">PDF</a>
        
        
        <a href="https://visvar.github.io/pub/yu2018effect.html" target="_blank">direct link</a>
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We designed a within-subject experiment to compare visual discomfort to preteen users caused by using head-mounted displays (HMD) and tablet computers for an hour. 18 participants younger than 13 years old were recruited to fulfill a series of similar painting tasks under both display conditions. Visual fatigue was measured with visual analog scale before and after experiment and during the break of experiment. The results indicated that HMD had a trend to bring higher visual fatigue than tablet computer during the exposure of 1 hour. Although the symptoms of visual discomfort disappeared after resting, there is need for preteen-specific head-mounted displays.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@INPROCEEDINGS{8699249,
  author={Yu, Xingyao and Weng, Dongdong and Guo, Jie and Jiang, Haiyan and Bao, Yihua},
  booktitle={2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Effect of Using HMDs for One Hour on Preteens Visual Fatigue}, 
  year={2018},
  pages={93-96},
  doi={10.1109/ISMAR-Adjunct.2018.00042}}</textarea></div>
      
    </div>
  </div>
  
  <h2>2016</h2>
  <div class="paper small" id="paperyu2016reduce">
    
      <img
        id="imageyu2016reduce"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperyu2016reduce', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/yu2016reduce.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperyu2016reduce', 'small'); toggleImageSize(imageyu2016reduce)"
        title="Click to show details"
      >
        Reduce Simulator Sickness by Overwritten Symbol in Smartphone-Based VR System<a class="anchor" name="yu2016reduce"></a>
      </h3>
      <div>
        Xingyao Yu, Dongdong Weng, Li Cai
      </div>
      <div>
        IEEE ICVRV (2016) Workshop / Short Paper
        <a href="https://doi.org/10.1109/ICVRV.2016.78" target="_blank">website</a>
        <a href="../pdf/yu2016reduce.pdf" target="_blank">PDF</a>
        
        
        <a href="https://visvar.github.io/pub/yu2016reduce.html" target="_blank">direct link</a>
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">The aim of this paper is to reduce simulator sickness caused by the low refresh rate of display in smartphone-based VR system. Without regard to the improvement of hardware, the method proposed in this paper reduces simulator sickness by adding static symbol on the screen of the smartphone. A series of user-participation experiments were done to validate the effectiveness of the method. Participants' responses to the symbol with different textures (cross or Minion logo) and in different positions (the center or near the corners) were assessed by Simulator Sickness Questionnaire (SSQ). The preliminary results demonstrate that the existence, the position and complexity of the symbols can be factors in relieving symptoms of simulator sickness.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@INPROCEEDINGS{7938233,
  author={Yu, Xingyao and Weng, Dongdong and Cai, Li},
  booktitle={2016 International Conference on Virtual Reality and Visualization (ICVRV)}, 
  title={Reduce Simulator Sickness by Overwritten Symbol in Smartphone-Based VR System}, 
  year={2016},
  pages={426-429},
  doi={10.1109/ICVRV.2016.78}}</textarea></div>
      
    </div>
  </div>
  
      </article>
    </div>
  </main>
</body>
</html>