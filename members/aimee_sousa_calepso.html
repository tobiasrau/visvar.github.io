<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Aimee Sousa Calepso, M.Sc. | VISVAR Research Group, University of Stuttgart</title>
  <link rel="stylesheet" href="../style.css">
  <script src="../script.js"></script>
  <link rel="shortcut icon" href="../img/favicon.png">
  <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
  <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
</head>
<body>
  <a class="anchor" name="top"></a>
  <main>
    
<div>
<header>
<div>
<a href="https://visvar.github.io/">
<h1 class="h1desktop"><div>VISVAR</div><div>Research</div><div>Group</div></h1>
<h1 class="h1mobile">VISVAR</h1>
</a>
</div>
<div>
<nav>
<ul>
<li><a href="https://visvar.github.io/#aboutus">about VISVAR</a></li>
<li><a href="https://visvar.github.io/#publications">publications</a></li>
<li class="memberNav"><a href="https://visvar.github.io/#members">members</a></li>
<ul class="memberNav">

<li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>

<li><a href="https://visvar.github.io/members/quynh_quang_ngo.html">Quynh Quang Ngo</a></li>

<li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>

<li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>

<li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>

<li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>

<li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>

<li><a href="https://visvar.github.io/members/markus_wieland.html">Markus Wieland</a></li>

<li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>

<li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>

<li><a href="https://visvar.github.io/members/nina_doerr.html">Nina Dörr</a></li>

<li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>

<li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>

<li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>

<li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>

<li><a href="https://visvar.github.io/members/tobias_rau.html">Tobias Rau</a></li>

<li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>

</ul>
</ul>
</nav>
</div>
</header>
</div>
    <div>
      <article><a class="anchor" name="aboutus"></a>
        <h1>Aimee Sousa Calepso, M.Sc.</h1>
        <div class="aboutMember">
          <div class="avatarAndBio">
            <img class="avatar" src="../img/people/aimee_sousa_calepso.jpg" />
            <div class="bio">
    <p>
      I'm currently a Ph.D. student and Research Assistant in Visualisierungsinstitut (VISUS), Universität Stuttgart.
      I hold a Masters degree in Computer Science from Universidade Federal do Rio Grande do Sul.
    </p>
    <p>
      My main research interests include situated visualization using wearable Augmented Reality devices and other applications.
      I also work in the Cluster of Excellence Integrative Computational Design and Construction for Architecture (<a href="https://www.intcdc.uni-stuttgart.de" target="_blank" rel="noreferrer">IntCDC</a>), helping to build solutions for the architechture and construction industry involving immersive experiences.
    </p>
    </div>
          </div>
          <div class="furtherInfo">
            <div>
              <h2>Research Interests</h2>
              <ul>
                <li>Situated visualization</li>
<li>XR applied to the construction industry</li>
<li>User studies</li>
<li>HCI</li>
              </ul>
            </div>
            <div>
              <h2>Links</h2>
              <ul>
                <li><a href="https://www.visus.uni-stuttgart.de/en/institute/team/Sousa-Calepso/" target="_blank" rel="noreferrer">University of Stuttgart website</a></li>
<li><a href="https://scholar.google.com/citations?user=04xWymMAAAAJ" target="_blank" rel="noreferrer">Google Scholar</a></li>
<li><a href="https://orcid.org/0000-0002-6625-0585" target="_blank" rel="noreferrer">ORCID</a></li>
              </ul>
            </div>
            
            <div>
            <h2>Projects &amp; Funding</h2>
            <ul>
              <li><a href="https://www.intcdc.uni-stuttgart.de" target="_blank" rel="noreferrer">IntCDC</a></li>
            </ul>
          </div>
            
          </div>
        </div>
      </article>
      <article> <a class="anchor" name="publications"></a>
        <h1>Publications</h1>
        
  <h2 class="yearHeading">2023</h2>
  <div class="paper small" id="paperwortmeier2023configuring">
    
    <div class="metaData noImage">
      <h3
        onclick="toggleClass('paperwortmeier2023configuring', 'small'); toggleImageSize(imagewortmeier2023configuring)"
        title="Click to show details"
      >
        Configuring Augmented Reality Users: Analysing YouTube Commercials to Understand Industry Expectations<a class="anchor" name="wortmeier2023configuring"></a>
      </h3>
      <div>
        Ann-Kathrin Wortmeier, Aimee Sousa Calepso, Cordula Kropp, Michael Sedlmair, Daniel Weiskopf
      </div>
      <div>
        Behaviour & Information Technology (2023) 
        <a href="https://visvar.github.io/pub/wortmeier2023configuring.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1080/0144929X.2022.2163693" target="_blank">DOI</a>
        
        
        
        <a href="https://www.tandfonline.com/doi/figure/10.1080/0144929X.2022.2163693?scroll=top&needAccess=true&role=tab" target="_blank">supplemental</a>
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Commercial videos are often used to familiarise potential buyers and users with new technologies and their possibilities. In addition, presenting visions of future applications is a way to configure users and define social worlds of technology use. We analyse 30 YouTube videos featuring augmented reality (AR) devices in industrial manufacturing and construction, to explore how these commercial videos situate AR technology and future users by showcasing techno-euphoric promises and imagined use cases. With a video analysis based on Grounded Theory and Situational Analysis, we untangle the promises of AR for manufacturing and construction work; second, we present two prevailing configurations of AR users: ‘experts in situ’ and ‘smart dummies’; and third, we discuss how YouTube videos put forward developmental expectations. In addition, we identify discrepancies between expectations and foreseeable requirements in construction work. Finally, our research could contribute to a more holistic understanding of workplaces and socially robust AR applications.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@article{doi:10.1080/0144929X.2022.2163693,
    title        = {Configuring augmented reality users: analysing YouTube commercials to understand industry expectations},
    author       = {Ann-Kathrin Wortmeier and Aim\'{e}e Sousa Calepso and Cordula Kropp and Michael Sedlmair and Daniel Weiskopf},
    year         = {2023},
    journal      = {Behaviour \& Information Technology},
    publisher    = {Taylor \& Francis},
    pages        = {1--16},
    doi          = {10.1080/0144929X.2022.2163693},
    url          = {https://doi.org/10.1080/0144929X.2022.2163693},
    eprint       = {https://doi.org/10.1080/0144929X.2022.2163693},
    abstract     = {Commercial videos are often used to familiarise potential buyers and users with new technologies and their possibilities. In addition, presenting visions of future applications is a way to configure users and define social worlds of technology use. We analyse 30 YouTube videos featuring augmented reality (AR) devices in industrial manufacturing and construction, to explore how these commercial videos situate AR technology and future users by showcasing techno-euphoric promises and imagined use cases. With a video analysis based on Grounded Theory and Situational Analysis, we untangle the promises of AR for manufacturing and construction work; second, we present two prevailing configurations of AR users: `experts in situ' and `smart dummies'; and third, we discuss how YouTube videos put forward developmental expectations. In addition, we identify discrepancies between expectations and foreseeable requirements in construction work. Finally, our research could contribute to a more holistic understanding of workplaces and socially robust AR applications.}
}
</textarea></div>
      
    </div>
  </div>
  
  <h2 class="yearHeading">2022</h2>
  <div class="paper small" id="papercalepso2022cardlearner">
    
      <img
        id="imagecalepso2022cardlearner"
        title="Click to enlarge and show details"
        onclick="toggleClass('papercalepso2022cardlearner', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/calepso2022cardlearner.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('papercalepso2022cardlearner', 'small'); toggleImageSize(imagecalepso2022cardlearner)"
        title="Click to show details"
      >
        cARdLearner: Using Expressive Virtual Agents when Learning Vocabulary in Augmented Reality<a class="anchor" name="calepso2022cardlearner"></a>
      </h3>
      <div>
        Aimee Sousa Calepso, Natalie Hube, Noah Berenguel Senn, Vincent Brandt, Michael Sedlmair
      </div>
      <div>
        CHI (2022) Late-Breaking Work
        <a href="https://visvar.github.io/pub/calepso2022cardlearner.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3491101.3519631" target="_blank">DOI</a>
        
        <a href="../pdf/calepso2022cardlearner.pdf" target="_blank">PDF</a>
        
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Augmented reality (AR) has a diverse range of applications, including language teaching. When studying a foreign language, one of the biggest challenges learners face is memorizing new vocabulary. While augmented holograms are a promising means of supporting this memorization process, few studies have explored their potential in the language learning context. We demonstrate the possibility of using flashcard along with an expressive holographic agent on vocabulary learning. Users scan a flashcard and play an animation that is connected with an emotion related to the word they are seeing. Our goal is to propose an alternative to the traditional use of flashcards, and also introduce another way of using AR in the association process.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3491101.3519631,
    title        = {CARdLearner: Using Expressive Virtual Agents When Learning Vocabulary in Augmented Reality},
    author       = {Sousa Calepso, Aimee and Hube, Natalie and Berenguel Senn, Noah and Brandt, Vincent and Sedlmair, Michael},
    year         = {2022},
    booktitle    = {CHI Conference on Human Factors in Computing Systems Extended Abstracts},
    publisher    = {Association for Computing Machinery},
    series       = {CHI EA '22},
    doi          = {10.1145/3491101.3519631},
    url          = {https://doi.org/10.1145/3491101.3519631},
    abstract     = {Augmented reality (AR) has a diverse range of applications, including language teaching. When studying a foreign language, one of the biggest challenges learners face is memorizing new vocabulary. While augmented holograms are a promising means of supporting this memorization process, few studies have explored their potential in the language learning context. We demonstrate the possibility of using flashcard along with an expressive holographic agent on vocabulary learning. Users scan a flashcard and play an animation that is connected with an emotion related to the word they are seeing. Our goal is to propose an alternative to the traditional use of flashcards, and also introduce another way of using AR in the association process.},
    articleno    = {245},
    numpages     = {6},
    keywords     = {contextual learning, augmented reality, language learning}
}
</textarea></div>
      
    </div>
  </div>
  
  
  <div class="paper small" id="paperfleck2022ragrug">
    
      <img
        id="imagefleck2022ragrug"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperfleck2022ragrug', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/fleck2022ragrug.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperfleck2022ragrug', 'small'); toggleImageSize(imagefleck2022ragrug)"
        title="Click to show details"
      >
        RagRug: A Toolkit for Situated Analytics<a class="anchor" name="fleck2022ragrug"></a>
      </h3>
      <div>
        Philipp Fleck, Aimee Sousa Calepso, Sebastian Hubenschmid, Michael Sedlmair, Dieter Schmalstieg
      </div>
      <div>
        TVCG (2022) Full Paper
        <a href="https://visvar.github.io/pub/fleck2022ragrug.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1109/TVCG.2022.3157058" target="_blank">DOI</a>
        
        <a href="../pdf/fleck2022ragrug.pdf" target="_blank">PDF</a>
        <a href="https://www.youtube.com/watch?v=mFxSdvQhSVU" target="_blank">video</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We present RagRug, an open-source toolkit for situated analytics. The abilities of RagRug go beyond previous immersive analytics toolkits by focusing on specific requirements emerging when using augmented reality (AR) rather than virtual reality. RagRug combines state of the art visual encoding capabilities with a comprehensive physical-virtual model, which lets application developers systematically describe the physical objects in the real world and their role in AR. We connect AR visualization with data streams from the Internet of Things using distributed dataflow. To this aim, we use reactive programming patterns so that visualizations become context-aware, i.e., they adapt to events coming in from the environment. The resulting authoring system is low-code; it emphasises describing the physical and the virtual world and the dataflow between the elements contained therein. We describe the technical design and implementation of RagRug, and report on five example applications illustrating the toolkit's abilities.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@article{fleck2022ragrug,
    title        = {{RagRug}: A Toolkit for Situated Analytics},
    author       = {Fleck, Philipp and Calepso, Aimee Sousa and Hubenschmid, Sebastian and Sedlmair, Michael and Schmalstieg, Dieter},
    year         = {2022},
    journal      = {IEEE Transactions on Visualization and Computer Graphics},
    publisher    = {IEEE}
}
</textarea></div>
      
    </div>
  </div>
  
  
  <div class="paper small" id="paperabdelaal2022visualization">
    
      <img
        id="imageabdelaal2022visualization"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperabdelaal2022visualization', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/abdelaal2022visualization.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperabdelaal2022visualization', 'small'); toggleImageSize(imageabdelaal2022visualization)"
        title="Click to show details"
      >
        Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World<a class="anchor" name="abdelaal2022visualization"></a>
      </h3>
      <div>
        Moataz Abdelaal, Felix Amtsberg, Michael Becher, Rebeca Duque Estrada, Fabian Kannenberg, Aimee Sousa Calepso, Hans Jakob Wagner, Guido Reina, Michael Sedlmair, Achim Menges, Daniel Weiskopf
      </div>
      <div>
        CG&A (2022) 
        <a href="https://visvar.github.io/pub/abdelaal2022visualization.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1109/MCG.2022.3149837" target="_blank">DOI</a>
        
        <a href="../pdf/abdelaal2022visualization.pdf" target="_blank">PDF</a>
        
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Our built world is one of the most important factors for a livable future, accounting for massive impact on resource and energy use, as well as climate change, but also the social and economic aspects that come with population growth. The architecture, engineering, and construction industry is facing the challenge that it needs to substantially increase its productivity, let alone the quality of buildings of the future. In this article, we discuss these challenges in more detail, focusing on how digitization can facilitate this transformation of the industry, and link them to opportunities for visualization and augmented reality research. We illustrate solution strategies for advanced building systems based on wood and fiber.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@article{abdelaal2022visualization,
    title        = {Visualization for Architecture, Engineering, and Construction: Shaping the Future of Our Built World},
    author       = {Abdelaal, Moataz and Amtsberg, Felix and Becher, Michael and Estrada, Rebeca Duque and Kannenberg, Fabian and Calepso, Aim\&\#x00E9;e Sousa and Wagner, Hans Jakob and Reina, Guido and Sedlmair, Michael and Menges, Achim and Weiskopf, Daniel},
    year         = {2022},
    month        = {March},
    journal      = {IEEE Computer Graphics and Applications},
    volume       = {42},
    number       = {2},
    pages        = {10--20},
    doi          = {10.1109/MCG.2022.3149837},
    issn         = {1558-1756},
    abstract     = {Our built world is one of the most important factors for a livable future, accounting for massive impact on resource and energy use, as well as climate change, but also the social and economic aspects that come with population growth. The architecture, engineering, and construction industry is facing the challenge that it needs to substantially increase its productivity, let alone the quality of buildings of the future. In this article, we discuss these challenges in more detail, focusing on how digitization can facilitate this transformation of the industry, and link them to opportunities for visualization and augmented reality research. We illustrate solution strategies for advanced building systems based on wood and fiber.}
}
</textarea></div>
      
    </div>
  </div>
  
  <h2 class="yearHeading">2021</h2>
  <div class="paper small" id="paperkrauter2021muc">
    
      <img
        id="imagekrauter2021muc"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperkrauter2021muc', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/krauter2021muc.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperkrauter2021muc', 'small'); toggleImageSize(imagekrauter2021muc)"
        title="Click to show details"
      >
        Don’t Catch It: An Interactive Virtual-Reality Environment to Learn About COVID-19 Measures Using Gamification Elements<a class="anchor" name="krauter2021muc"></a>
      </h3>
      <div>
        Christian Krauter, Jonas Vogelsang, Aimee Sousa Calepso, Katrin Angerbauer, Michael Sedlmair
      </div>
      <div>
        MuC (2021) Full Paper
        <a href="https://visvar.github.io/pub/krauter2021muc.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3473856.3474031" target="_blank">DOI</a>
        
        <a href="../pdf/krauter2021muc.pdf" target="_blank">PDF</a>
        
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">The world is still under the influence of the COVID-19 pandemic. Even though vaccines are deployed as rapidly as possible, it is still necessary to use other measures to reduce the spread of the virus. Measures such as social distancing or wearing a mask receive a lot of criticism. Therefore, we want to demonstrate a serious game to help the players understand these measures better and show them why they are still necessary. The player of the game has to avoid other agents to keep their risk of a COVID-19 infection low. The game uses Virtual Reality through a Head-Mounted-Display to deliver an immersive and enjoyable experience. Gamification elements are used to engage the user with the game while they explore various environments. We also implemented visualizations that help the user with social distancing.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{krauter2021muc,
    title        = {Don't Catch It: An Interactive Virtual-Reality Environment to Learn About {COVID-19} Measures Using Gamification Elements},
    author       = {Christian Krauter and Jonas Vogelsang and Aimee Sousa Calepso and Katrin Angerbauer and Michael Sedlmair},
    year         = {2021},
    booktitle    = {Mensch und Computer},
    publisher    = {ACM},
    pages        = {593--596},
    doi          = {10.1145/3473856.3474031},
    url          = {https://doi.org/10.1145/3473856.3474031}
}
</textarea></div>
      
    </div>
  </div>
  
      </article>
    </div>
  </main>
</body>
</html>