<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Natalie Hube | VISVAR Research Group, University of Stuttgart</title>
    <link rel="stylesheet" href="../style.css">
    <script src="../script.js"></script>
    <link rel="shortcut icon" href="../img/favicon.png">
    <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
</head>
<body>
    <a class="anchor" name="top"></a>
    <main>
        <div>
            
<header>
    <div>
        <a href="https://visvar.github.io/">
            <h1 class="h1desktop">
                <div>
                    VISVAR
                </div>
                <div>
                    Research
                </div>
                <div>
                    Group
                </div>
            </h1>
            <h1 class="h1mobile">
                VISVAR
            </h1>
        </a>
    </div>
    <div>
        <nav>
            <ul>
                <li>
                    <a href="https://visvar.github.io/#aboutus">about VISVAR</a>
                </li>
                <li>
                    <a href="https://visvar.github.io/#publications">publications</a>
                </li>
                <li class="memberNav">
                    <a href="https://visvar.github.io/#members">members</a>
                </li>
                <ul class="memberNav">
                    
                        <li>
                            <a href="https://visvar.github.io/members/aimee_sousa_calepso.html">
                                Aimee Sousa Calepso
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/alexander_achberger.html">
                                Alexander Achberger
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/frank_heyen.html">
                                Frank Heyen
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/jonas_haischt.html">
                                Jonas Haischt
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/katrin_angerbauer.html">
                                Katrin Angerbauer
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/melissa_reinelt.html">
                                Melissa Reinelt
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/michael_sedlmair.html">
                                Michael Sedlmair
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/natalie_hube.html">
                                Natalie Hube
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/quynh_ngo.html">
                                Quynh Ngo
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/rene_cutura.html">
                                Rene Cutura
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/ruben_bauer.html">
                                Ruben Bauer
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/sebastian_rigling.html">
                                Sebastian Rigling
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/simeon_rau.html">
                                Simeon Rau
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/xingyao_yu.html">
                                Xingyao Yu
                            </a>
                        </li>
                    
                </ul>
            </ul>
        </nav>
    </div>
</header>
        </div>
        <div>
            <article> <a class="anchor" name="aboutus"></a>
                <h1>
  Natalie Hube, M.Sc.
</h1>

<div class="aboutMember">

  <div class="avatarAndBio">
    <img class="avatar" src="../img/people/natalie_hube.jpg" />

    <div class="bio">
      <p>
        As part of my research, I focus on the usage of human representations in Virtual & Augmented Reality for
        international collaboration.
        Further areas of research interest include HCI, information visualization, and computer graphics.
        I am a PhD student at Mercedes-Benz AG, Virtual Reality Center, in Stuttgart and am supervised by the
        University of Stuttgart, VISUS, Chair of Virtual Reality and Augmented Reality.
      </p>
      <p>
        Future work: Avatars for VR/AR, Collaboration in Immersive Environments.
      </p>
    </div>
  </div>

  <p>
    <a href="https://www.visus.uni-stuttgart.de/en/institute/team/Hube/" target="_blank">Institute website</a>
  </p>

  <h2>Research Interests</h2>
  <ul>
    <li>VR/AR Avatars</li>
    <li>VR/AR in general</li>
    <li>HCI</li>
  </ul>

  <h2>More</h2>
  <ul>
    <li>
      <a href="https://orcid.org/0000-0001-9094-0271" target="_blank">ORCID</a>
    </li>
  </ul>

</div>

            </article>
            <article> <a class="anchor" name="publications"></a>
                <h1>Publications</h1>
                
    
    <h2>
        2022
    </h2>
    
    <div
        class="paper small"
        id="papercalepso2022cardlearner"
    >
        
            <img
                id="imagecalepso2022cardlearner"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercalepso2022cardlearner', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/calepso2022cardlearner.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercalepso2022cardlearner', 'small'); toggleImageSize(imagecalepso2022cardlearner);"
                title="Click to show details"
            >
                cARdLearner: Using Expressive Virtual Agents when Learning Vocabulary in Augmented Reality
            </h3>  <a class="anchor" name="calepso2022cardlearner"></a>
            <div class="authors">
                <span class="firstAuthor">Aimee Sousa Calepso</span>,
                Natalie Hube, Noah Berenguel Senn, Vincent Brandt, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CHI 2022</span>
                <span class="publication">Late-Breaking Work</span>
                <a href="../pdf/calepso2022cardlearner.pdf" target="_blank">PDF</a>
                
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Augmented reality (AR) has a diverse range of applications, including language teaching. When studying a foreign language, one of the biggest challenges learners face is memorizing new vocabulary. While augmented holograms are a promising means of supporting this memorization process, few studies have explored their potential in the language learning context. We demonstrate the possibility of using flashcard along with an expressive holographic agent on vocabulary learning. Users scan a flashcard and play an animation that is connected with an emotion related to the word they are seeing. Our goal is to propose an alternative to the traditional use of flashcards, and also introduce another way of using AR in the association process.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2022using"
    >
        
            <img
                id="imagehube2022using"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2022using', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2022using.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2022using', 'small'); toggleImageSize(imagehube2022using);"
                title="Click to show details"
            >
                Using Expressive Avatars to Increase Emotion Recognition: A Pilot Study
            </h3>  <a class="anchor" name="hube2022using"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                 Kresimir Vidackovic, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CHI 2022</span>
                <span class="publication">Late-Breaking Work</span>
                <a href="../pdf/hube2022using.pdf" target="_blank">PDF</a>
                
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Virtual avatars are widely used for collaborating in virtual environments. Yet, often these avatars lack expressiveness to determine a state of mind. Prior work has demonstrated efective usage of determining emotions and animated lip movement through analyzing mere audio tracks of spoken words. To provide this information on a virtual avatar, we created a natural audio data set consisting of 17 audio fles from which we then extracted the underlying emotion and lip movement. To conduct a pilot study, we developed a prototypical system that displays the extracted visual parameters and then maps them on a virtual avatar while playing the corresponding audio fle. We tested the system with 5 participants in two conditions: (i) while seeing the virtual avatar only an audio fle was played. (ii) In addition to the audio fle, the extracted facial visual parameters were displayed on the virtual avatar. Our results suggest the validity of using additional visual parameters in the avatars face as it helps to determine emotions. We conclude with a brief discussion on the outcomes and their implications on future work.
            </div>
            
            
        </div>
    </div>
    
    
    <h2>
        2021
    </h2>
    
    <div
        class="paper small"
        id="paperhube2021ismar"
    >
        
            <img
                id="imagehube2021ismar"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2021ismar', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2021ismar.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2021ismar', 'small'); toggleImageSize(imagehube2021ismar);"
                title="Click to show details"
            >
                VR Collaboration in Large Companies: An Interview Study on the Role of Avatars
            </h3>  <a class="anchor" name="hube2021ismar"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Katrin Angerbauer, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2021</span>
                <span class="publication">Short Paper</span>
                <a href="../pdf/hube2021ismar.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Collaboration is essential in companies and often physical presence is required, thus, more and more Virtual Reality (VR) systems are used to work together remotely. To support social interaction, human representations in form of avatars are used in collaborative virtual environment (CVE) tools. However, up to now, the avatar representations often are limited in their design and functionality, which may hinder effective collaboration. In our interview study, we explored the status quo of VR collaboration in a large automotive company setting with a special focus on the role of avatars. We collected inter-view data from 21 participants, from which we identified challenges of current avatar representations used in our setting. Based on these findings, we discuss design suggestions for avatars in a company setting, which aim to improve social interaction. As opposed to state-of-the-art research, we found that users within the context of a large automotive company have an altered need with respect to avatar representations.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{hube2021ismar,
	Author = {Natalie Hube and Katrin Angerbauer and Daniel Pohlandt and Kresimir Vidackovic and Michael Sedlmair},
	Title = {VR Collaboration in Large Companies: An Interview Study on the Role of Avatars},
	Booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct, Short paper)},
	pages = {139--144},
	url = {https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037},
  	doi = {10.1109/ISMAR-Adjunct54149.2021.00037},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2021visap"
    >
        
            <img
                id="imagecutura2021visap"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2021visap', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/cutura2021visap.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2021visap', 'small'); toggleImageSize(imagecutura2021visap);"
                title="Click to show details"
            >
                DaRt: Generative Art using Dimensionality Reduction Algorithms
            </h3>  <a class="anchor" name="cutura2021visap"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Kathrin Angerbauer, Frank Heyen, Natalie Hube, Michael Sedlmair
            </div>
            <div>
                <span class="publication">VIS 2021</span>
                <span class="publication">Pictorial</span>
                <a href="../pdf/cutura2021visap.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/VISAP52981.2021.00013" target="_blank">website</a>
                <a href="https://youtu.be/pOcksJOiAPw" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Dimensionality Reduction (DR) is a popular technique that is often used in Machine Learning and Visualization communities to analyze high-dimensional data. The approach is empirically proven to be powerful for uncovering previously unseen structures in the data. While observing the results of the intermediate optimization steps of DR algorithms, we coincidently discovered the artistic beauty of the DR process. With enthusiasm for the beauty, we decided to look at DR from a generative art lens rather than their technical application aspects and use DR techniques to create artwork. Particularly, we use the optimization process to generate images, by drawing each intermediate step of the optimization process with some opacity over the previous intermediate result. As another alternative input, we used a neural-network model for face-landmark detection, to apply DR to portraits, while maintaining some facial properties, resulting in abstracted facial avatars. In this work, we provide such a collection of such artwork.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2021dart,
  title={{DaRt}: Generative Art using Dimensionality Reduction Algorithms},
  author={Cutura, Rene and Angerbauer, Katrin and Heyen, Frank and Hube, Natalie and Sedlmair, Michael},
  booktitle={2021 IEEE VIS Arts Program (VISAP)},
  pages={59--72},
  year={2021},
  organization={IEEE},
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 251654672 - TRR 161
            </div>
        </div>
    </div>
    
    
    <h2>
        2020
    </h2>
    
    <div
        class="paper small"
        id="paperhube2020comparing"
    >
        
            <img
                id="imagehube2020comparing"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2020comparing', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2020comparing.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2020comparing', 'small'); toggleImageSize(imagehube2020comparing);"
                title="Click to show details"
            >
                Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion
            </h3>  <a class="anchor" name="hube2020comparing"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Oliver Lenz, Lars Engeln, Rainer Groh, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2020</span>
                <span class="publication">Poster / Short Paper</span>
                
                <a href="https://doi.org/10.1109/ISMAR-Adjunct51615.2020.00023" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@INPROCEEDINGS{9288476,
  author={Hube, Natalie and Lenz, Oliver and Engeln, Lars and Groh, Rainer and Sedlmair, Michael},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion}, 
  year={2020},
  pages={30-35},
  doi={10.1109/ISMAR-Adjunct51615.2020.00023}}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2020mixed"
    >
        
            <img
                id="imagehube2020mixed"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2020mixed', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2020mixed.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2020mixed', 'small'); toggleImageSize(imagehube2020mixed);"
                title="Click to show details"
            >
                Mixed Reality based Collaboration for Design Processes
            </h3>  <a class="anchor" name="hube2020mixed"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Mathias Müller, Esther Lapczyna, Jan Wojdziak
            </div>
            <div>
                <span class="publication">i-com 2020</span>
                <span class="publication">Journal Paper</span>
                <a href="../pdf/hube2020mixed.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1515/icom-2020-0012" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Due to constantly and rapidly growing digitization, requirements for international cooperation are changing. Tools for collaborative work such as video telephony are already an integral part of today’s communication across companies. However, these tools are not sufficient to represent the full physical presence of an employee or a product as well as its components in another location, since the representation of information in a two-dimensional way and the resulting limited communication loses concrete objectivity. Thus, we present a novel object-centered approach that compromises of Augmented and Virtual Reality technology as well as design suggestions for remote collaboration. Furthermore, we identify current key areas for future research and specify a design space for the use of Augmented and Virtual Reality remote collaboration in the manufacturing process in the automotive industry.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{hube2020mixed,
  title={Mixed Reality based Collaboration for Design Processes},
  author={Hube, Natalie and M{\"u}ller, Mathias and Lapczyna, Esther and Wojdziak, Jan},
  journal={i-com},
  volume={19},
  number={2},
  pages={123--137},
  year={2020},
  publisher={De Gruyter Oldenbourg}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2018
    </h2>
    
    <div
        class="paper small"
        id="paperengeln2018immersive"
    >
        
            <img
                id="imageengeln2018immersive"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperengeln2018immersive', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/engeln2018immersive.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperengeln2018immersive', 'small'); toggleImageSize(imageengeln2018immersive);"
                title="Click to show details"
            >
                Immersive VisualAudioDesign: Spectral Editing in VR
            </h3>  <a class="anchor" name="engeln2018immersive"></a>
            <div class="authors">
                <span class="firstAuthor">Lars Engeln</span>,
                Natalie Hube, Rainer Groh
            </div>
            <div>
                <span class="publication">Audio Mostly 2018</span>
                <span class="publication">Full Paper</span>
                <a href="../pdf/engeln2018immersive.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3243274.3243279" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                VisualAudioDesign (VAD) is an attempt to design audio in a visual way. The frequency-domain visualized as a spectrogram construed as pixel data can be manipulated with image filters. Thereby, an approach is described to get away from direct DSP parameter manipulation to a more comprehensible sound design. Virtual Reality (VR) offers immersive insights into data and embodied interaction in the virtual environment. VAD and VR combined enrich spectral editing with a natural work-flow. Therefore, a design paper prototype for interaction with audio data in an virtual environment was used and examined.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2018towards"
    >
        
            <img
                id="imagehube2018towards"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2018towards', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2018towards.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2018towards', 'small'); toggleImageSize(imagehube2018towards);"
                title="Click to show details"
            >
                Towards augmented reality in quality assurance processes
            </h3>  <a class="anchor" name="hube2018towards"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Mathias Müller, Jan Wojdziak, Franziska Hannß, Rainer Groh
            </div>
            <div>
                <span class="publication">MMSys 2018</span>
                <span class="publication">Workshop Paper</span>
                <a href="../pdf/hube2018towards.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3210438.3210442" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Augmented reality (AR) has gained exceptional importance in supporting task performance. Particularly, in quality assurance (QA) processes in the automotive sector AR offers a diversity of use cases. In this paper we propose an interface design which projects information as a digital canvas on the surface of vehicle components. Based on a requirement analysis, we discuss design aspects and describe our application in applying the quality assurance process of a luxury automaker. The application includes a personal view on spatial information embedded in a guided interaction process as a design solution that can be applied to enhance QA processes.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2018the"
    >
        
            <img
                id="imagehube2018the"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2018the', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2018the.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2018the', 'small'); toggleImageSize(imagehube2018the);"
                title="Click to show details"
            >
                The Data in Your Hands: Exploring Novel Interaction Techniques and Data Visualization Approaches for Immersive Data Analytics
            </h3>  <a class="anchor" name="hube2018the"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Mathias Müller
            </div>
            <div>
                <span class="publication">AVI  2018</span>
                <span class="publication">Workshop Paper</span>
                <a href="http://ceur-ws.org/Vol-2108/paper2.pdf" target="_blank">PDF</a>
                
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we describe a concept for visualization and interaction with a large data set in an virtual environment. The core idea uses the traditional flat 2D representation as a base visualization but lets the user transform it into a spatial 3D visualizations on demand. Our visualization and interaction concept targets data analysts to use it for exploration and analysis, utilizing virtual reality to gain insight into complex data sets. The concept is based on the use of Parallel Sets for the representation of categorical data. By extending the conventional 2D Parallel Sets with a third dimension, correlations between path variables and the related number of items belonging to a specific node can be visualized. Furthermore, the concept uses virtual reality controllers in combination with a head-mounted display to control additional views. The purpose of the paper is to describe the core concepts and challenges for this type of spatial visualization and the related interaction design, including the use of gestures for direct manuipulation and a hand-attached menu for complex actions
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2018facilitating"
    >
        
            <img
                id="imagehube2018facilitating"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2018facilitating', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2018facilitating.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2018facilitating', 'small'); toggleImageSize(imagehube2018facilitating);"
                title="Click to show details"
            >
                Facilitating exploration on exhibitions with augmented reality
            </h3>  <a class="anchor" name="hube2018facilitating"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Mathias Müller, Rainer Groh
            </div>
            <div>
                <span class="publication">AVI 2018</span>
                <span class="publication">Poster Paper</span>
                <a href="../pdf/hube2018facilitating.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3206505.3206585" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                At exhibitions, visitors are usually in a completely unknown environment. Although visitors generally are informed about the topic before a visit, interests are still difficult to extract from the mass of exhibition stands and offers. In this paper we describe a concept using head-coupled AR together with recommender mechanisms for exhibitions. We present a conceptual development for a first prototype with focus on navigational aspects as well as explicit and implicit recommendations to generate input data for visually displayed recommendations.
            </div>
            
            
        </div>
    </div>
    
    
    <h2>
        2017
    </h2>
    
    <div
        class="paper small"
        id="papermüller2017a"
    >
        
            <img
                id="imagemüller2017a"
                title="Click to enlarge and show details"
                onclick="toggleClass('papermüller2017a', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/müller2017a.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papermüller2017a', 'small'); toggleImageSize(imagemüller2017a);"
                title="Click to show details"
            >
                A zoomable product browser for elastic displays
            </h3>  <a class="anchor" name="müller2017a"></a>
            <div class="authors">
                <span class="firstAuthor">Mathias Müller</span>,
                Mandy Keck, Thomas Gründer, Natalie Hube, Rainer Groh
            </div>
            <div>
                <span class="publication">xCoAX 2017</span>
                <span class="publication">Full Paper</span>
                <a href="http://2017.xcoax.org/pdf/xcoax2017-Muller.pdf" target="_blank">PDF</a>
                
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we present an interaction and visualization concept for elastic displays. The interaction concept was inspired by the search process of a rummage table to explore a large set of product data. The basic approach uses a similarity-based search pattern—based on a small set of items, the user refines the search result by examining similar items and exchanging them with items from the current result. A physically-based approach is used to interact with the data by deforming the surface of the elastic display. The presented visualization concept uses glyphs to directly compare items at a glance. Zoomable UI techniques controlled by the deformation of the elastic surface allow to display different levels of detail for each item.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2017additional"
    >
        
            <img
                id="imagehube2017additional"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2017additional', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2017additional.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2017additional', 'small'); toggleImageSize(imagehube2017additional);"
                title="Click to show details"
            >
                Additional On-Demand Dimension for Data Visualization
            </h3>  <a class="anchor" name="hube2017additional"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Mathias Müller, Rainer Groh
            </div>
            <div>
                <span class="publication">EuroVis 2017</span>
                <span class="publication">Short Paper</span>
                <a href="https://diglib.eg.org/bitstream/handle/10.2312/eurovisshort20171151/163-167.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.2312/eurovisshort.20171151" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we present a concept to interactively extend an 2d visualization by an additional on-demand dimension. We use categorical data in a multidimensional information space applied in a travel search scenario. Parallel sets are used as the basis for the visualization concept, since this is particularly suitable for the visualization of categorical data. The on-demand dimension expands the vertical axis of a parallel coordinate graph into depth axis and is intended to increase comparability of path variables with respect to the number of elements belonging to the respective parameter axis instead of direct comparability of individual paths and keep relations between the parallel sets. The presented implementation suits as foundation for further studies about the usefulness of a dynamic, on demand extension a of 2d visualizations into spatial visualizations. Furthermore, we present some additional approaches about the usage of the increased visualization space.
            </div>
            
            
        </div>
    </div>
    
    
    <h2>
        2016
    </h2>
    
    <div
        class="paper small"
        id="paperhube2016virtual"
    >
        
            <img
                id="imagehube2016virtual"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2016virtual', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="../img/small/hube2016virtual.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2016virtual', 'small'); toggleImageSize(imagehube2016virtual);"
                title="Click to show details"
            >
                Virtual UNREALity: Exploring Alternative Visualization Techniques for Virtual Reality
            </h3>  <a class="anchor" name="hube2016virtual"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Hannes Grusla, Mathias Müller, Ingmar S. Franke, Tobias Günther, Rainer Groh
            </div>
            <div>
                <span class="publication">xCoAX 2016</span>
                <span class="publication">Full Paper</span>
                <a href="http://2016.xcoax.org/pdf/xcoax2016-Hube.pdf" target="_blank">PDF</a>
                <a href="http://2016.xcoax.org/pdf/xcoax2016-Hube.pdf" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Virtual Reality (VR) offers new ways to perceive and interact with virtual content. Apart from photo-realism, VR can be used to explore new ways of visualization and interaction. In this contribution, we describe two student projects, which focused on creating innovative concepts for an artistic VR experience. We provide a review of sources of inspiration ranging from standard NPR-techniques through movies, interactive artworks and games to phenomena of human perception. Based on this wide collection of material we describe the prototypes, and discuss observations during implementation and from user feedback. Finally, possible future directions to use the potential of VR as a tool for novel, artful and unconventional experiences are discussed.
            </div>
            
            
        </div>
    </div>
    
            </article>
        </div>
    </main>
</body>
</html>