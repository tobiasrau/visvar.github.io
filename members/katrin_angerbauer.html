<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Katrin Angerbauer | VISVAR Research Group, University of Stuttgart</title>
  <link rel="stylesheet" href="../style.css">
  <script src="../script.js"></script>
  <link rel="shortcut icon" href="../img/favicon.png">
  <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
  <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
</head>
<body>
  <a class="anchor" name="top"></a>
  <main>
    <div>
      
<header>
  <div>
    <a href="https://visvar.github.io/">
      <h1 class="h1desktop"><div>VISVAR</div><div>Research</div><div>Group</div></h1>
      <h1 class="h1mobile">VISVAR</h1>
    </a>
  </div>
  <div>
    <nav>
      <ul>
        <li>
          <a href="https://visvar.github.io/#aboutus">about VISVAR</a>
        </li>
        <li>
          <a href="https://visvar.github.io/#publications">publications</a>
        </li>
        <li class="memberNav">
          <a href="https://visvar.github.io/#members">members</a>
        </li>
        <ul class="memberNav">
          
            <li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>
          
            <li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>
          
            <li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>
          
            <li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>
          
            <li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>
          
            <li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>
          
            <li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>
          
            <li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>
          
            <li><a href="https://visvar.github.io/members/nina_doerr.html">Nina Dörr</a></li>
          
            <li><a href="https://visvar.github.io/members/quynh_ngo.html">Quynh Ngo</a></li>
          
            <li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>
          
            <li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>
          
            <li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>
          
            <li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>
          
            <li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>
          
        </ul>
      </ul>
    </nav>
  </div>
</header>
    </div>
    <div>
      <article> <a class="anchor" name="aboutus"></a>
        <h1>
  Katrin Angerbauer, M.Sc.
</h1>

<div class="aboutMember">

  <div class="avatarAndBio">
    <img class="avatar" src="../img/people/katrin_angerbauer.jpg" />

    <div class="bio">
      <p>
        After my Bachelor's and Master's in Software Engineering here in Stuttgart, I started my PhD at VISUS in March
        2019.
        I'm passionate about all things HCI. During my Ph.D., I research how to make visual computing more accessible
        and try to
        develop ways to assist people with all kinds of impairments.
      </p>
    </div>
  </div>

  <p>
    <a href="https://www.visus.uni-stuttgart.de/en/institute/team/Angerbauer/" target="_blank">Institute website</a>
  </p>

  <h2>Research Interests</h2>
  <ul>
    <li>HCI</li>
    <li>Accessibility</li>
  </ul>

  <h2>More</h2>
  <ul>
    <li>
      <a href="https://orcid.org/0000-0002-1126-5288" target="_blank">ORCID</a>
    </li>
  </ul>
</div>

      </article>
      <article> <a class="anchor" name="publications"></a>
        <h1>Publications</h1>
        
  <h2>2022</h2>
  <div class="paper small" id="paperangerbauer2022accessibility">
    
      <img
        id="imageangerbauer2022accessibility"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperangerbauer2022accessibility', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/angerbauer2022accessibility.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperangerbauer2022accessibility', 'small'); toggleImageSize(imageangerbauer2022accessibility)"
        title="Click to show details"
      >
        Accessibility for Color Vision Deficiencies: Challenges and Findings of a Large Scale Study on Paper Figures<a class="anchor" name="angerbauer2022accessibility"></a>
      </h3>
      <div>
        Katrin Angerbauer, Nils Rodrigues, Rene Cutura, Seyda Öney, Nelusa Pathmanathan, Cristina Morariu, Daniel Weiskopf, Michael Sedlmair
      </div>
      <div>
        CHI (2022) Full Paper
        <a href="https://doi.org/10.1145/3491102.3502133" target="_blank">website</a>
        <a href="../pdf/angerbauer2022accessibility.pdf" target="_blank">PDF</a>
        <a href="https://www.youtube.com/watch?v=fhhg0k2LLkk" target="_blank">video</a>
        <a href="../suppl/angerbauer2022accessibility.zip" target="_blank">supplemental</a>
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We present an exploratory study on the accessibility of images in publications when viewed with color vision deficiencies (CVDs). The study is based on 1,710 images sampled from a visualization dataset (VIS30K) over five years. We simulated four CVDs on each image. First, four researchers (one with a CVD) identified existing issues and helpful aspects in a subset of the images. Based on the resulting labels, 200 crowdworkers provided  30,000 ratings on present CVD issues in the simulated images. We analyzed this data for correlations, clusters, trends, and free text comments to gain a first overview of paper figure accessibility. Overall, about 60 % of the images were rated accessible. Furthermore, our study indicates that accessibility issues are subjective and hard to detect. On a meta-level, we reflect on our study experience to point out challenges and opportunities of large-scale accessibility studies for future research directions.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3491102.3502133,
author = {Angerbauer, Katrin and Rodrigues, Nils and Cutura, Rene and \"{O}ney, Seyda and Pathmanathan, Nelusa and Morariu, Cristina and Weiskopf, Daniel and Sedlmair, Michael},
title = {Accessibility for Color Vision Deficiencies: Challenges and Findings of a Large Scale Study on Paper Figures},
year = {2022},
isbn = {9781450391573},
publisher = {ACM},
url = {https://doi.org/10.1145/3491102.3502133},
doi = {10.1145/3491102.3502133},
abstract = {We present an exploratory study on the accessibility of images in publications when viewed with color vision deficiencies (CVDs). The study is based on 1,710 images sampled from a visualization dataset (VIS30K) over five years. We simulated four CVDs on each image. First, four researchers (one with a CVD) identified existing issues and helpful aspects in a subset of the images. Based on the resulting labels, 200 crowdworkers provided  30,000 ratings on present CVD issues in the simulated images. We analyzed this data for correlations, clusters, trends, and free text comments to gain a first overview of paper figure accessibility. Overall, about 60 % of the images were rated accessible. Furthermore, our study indicates that accessibility issues are subjective and hard to detect. On a meta-level, we reflect on our study experience to point out challenges and opportunities of large-scale accessibility studies for future research directions. },
booktitle = {CHI Conference on Human Factors in Computing Systems},
articleno = {134},
numpages = {23},
keywords = {color vision deficiency, crowdsourcing, visualization, accessibility},
series = {CHI '22}
}</textarea></div>
      <h4>Acknowledgements</h4><div class="abstract">Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (projects A08 and B01). We thank all our study participants and in particular Sajid Baloch for his valuable input.</div>
      <div>
        <a href="https://visvar.github.io/pub/angerbauer2022accessibility.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  <h2>2021</h2>
  <div class="paper small" id="paperhube2021ismar">
    
      <img
        id="imagehube2021ismar"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperhube2021ismar', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/hube2021ismar.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperhube2021ismar', 'small'); toggleImageSize(imagehube2021ismar)"
        title="Click to show details"
      >
        VR Collaboration in Large Companies: An Interview Study on the Role of Avatars<a class="anchor" name="hube2021ismar"></a>
      </h3>
      <div>
        Natalie Hube, Katrin Angerbauer, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair
      </div>
      <div>
        ISMAR (2021) Short Paper
        <a href="https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037" target="_blank">website</a>
        <a href="../pdf/hube2021ismar.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Collaboration is essential in companies and often physical presence is required, thus, more and more Virtual Reality (VR) systems are used to work together remotely. To support social interaction, human representations in form of avatars are used in collaborative virtual environment (CVE) tools. However, up to now, the avatar representations often are limited in their design and functionality, which may hinder effective collaboration. In our interview study, we explored the status quo of VR collaboration in a large automotive company setting with a special focus on the role of avatars. We collected inter-view data from 21 participants, from which we identified challenges of current avatar representations used in our setting. Based on these findings, we discuss design suggestions for avatars in a company setting, which aim to improve social interaction. As opposed to state-of-the-art research, we found that users within the context of a large automotive company have an altered need with respect to avatar representations.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{hube2021ismar,
	Author = {Natalie Hube and Katrin Angerbauer and Daniel Pohlandt and Kresimir Vidackovic and Michael Sedlmair},
	Title = {VR Collaboration in Large Companies: An Interview Study on the Role of Avatars},
	Booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct, Short paper)},
	pages = {139--144},
	url = {https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037},
  	doi = {10.1109/ISMAR-Adjunct54149.2021.00037},
	Year = {2021}
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/hube2021ismar.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  
  <div class="paper small" id="papercutura2021visap">
    
      <img
        id="imagecutura2021visap"
        title="Click to enlarge and show details"
        onclick="toggleClass('papercutura2021visap', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/cutura2021visap.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('papercutura2021visap', 'small'); toggleImageSize(imagecutura2021visap)"
        title="Click to show details"
      >
        DaRt: Generative Art using Dimensionality Reduction Algorithms<a class="anchor" name="cutura2021visap"></a>
      </h3>
      <div>
        Rene Cutura, Katrin Angerbauer, Frank Heyen, Natalie Hube, Michael Sedlmair
      </div>
      <div>
        VIS (2021) Pictorial
        <a href="https://doi.org/10.1109/VISAP52981.2021.00013" target="_blank">website</a>
        <a href="../pdf/cutura2021visap.pdf" target="_blank">PDF</a>
        <a href="https://youtu.be/pOcksJOiAPw" target="_blank">video</a>
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Dimensionality Reduction (DR) is a popular technique that is often used in Machine Learning and Visualization communities to analyze high-dimensional data. The approach is empirically proven to be powerful for uncovering previously unseen structures in the data. While observing the results of the intermediate optimization steps of DR algorithms, we coincidently discovered the artistic beauty of the DR process. With enthusiasm for the beauty, we decided to look at DR from a generative art lens rather than their technical application aspects and use DR techniques to create artwork. Particularly, we use the optimization process to generate images, by drawing each intermediate step of the optimization process with some opacity over the previous intermediate result. As another alternative input, we used a neural-network model for face-landmark detection, to apply DR to portraits, while maintaining some facial properties, resulting in abstracted facial avatars. In this work, we provide such a collection of such artwork.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{cutura2021dart,
  title={{DaRt}: Generative Art using Dimensionality Reduction Algorithms},
  author={Cutura, Rene and Angerbauer, Katrin and Heyen, Frank and Hube, Natalie and Sedlmair, Michael},
  booktitle={2021 IEEE VIS Arts Program (VISAP)},
  pages={59--72},
  year={2021},
  organization={IEEE},
}</textarea></div>
      <h4>Acknowledgements</h4><div class="abstract">Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 251654672 - TRR 161</div>
      <div>
        <a href="https://visvar.github.io/pub/cutura2021visap.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  
  <div class="paper small" id="paperkrauter2021muc">
    
      <img
        id="imagekrauter2021muc"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperkrauter2021muc', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/krauter2021muc.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperkrauter2021muc', 'small'); toggleImageSize(imagekrauter2021muc)"
        title="Click to show details"
      >
        Don’t Catch It: An Interactive Virtual-Reality Environment to Learn About COVID-19 Measures Using Gamification Elements<a class="anchor" name="krauter2021muc"></a>
      </h3>
      <div>
        Christian Krauter, Jonas Vogelsang, Aimee Sousa Calepso, Katrin Angerbauer, Michael Sedlmair
      </div>
      <div>
        MuC (2021) Full Paper
        <a href="https://doi.org/10.1145/3473856.3474031" target="_blank">website</a>
        <a href="../pdf/krauter2021muc.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">The world is still under the influence of the COVID-19 pandemic. Even though vaccines are deployed as rapidly as possible, it is still necessary to use other measures to reduce the spread of the virus. Measures such as social distancing or wearing a mask receive a lot of criticism. Therefore, we want to demonstrate a serious game to help the players understand these measures better and show them why they are still necessary. The player of the game has to avoid other agents to keep their risk of a COVID-19 infection low. The game uses Virtual Reality through a Head-Mounted-Display to deliver an immersive and enjoyable experience. Gamification elements are used to engage the user with the game while they explore various environments. We also implemented visualizations that help the user with social distancing.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{krauter2021muc,
	Author = {Christian Krauter and Jonas Vogelsang and Aimee Sousa Calepso and Katrin Angerbauer and Michael Sedlmair},
	Title = {Don’t Catch It: An Interactive Virtual-Reality Environment to Learn About {COVID-19} Measures Using Gamification Elements},
	Booktitle = {Mensch und Computer},
	publisher = {ACM},
	pages = {593--596},
	url = {https://doi.org/10.1145/3473856.3474031},
  	doi = {10.1145/3473856.3474031},
	Year = {2021}
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/krauter2021muc.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  <h2>2020</h2>
  <div class="paper small" id="paperyu2020perspective">
    
      <img
        id="imageyu2020perspective"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperyu2020perspective', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/yu2020perspective.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperyu2020perspective', 'small'); toggleImageSize(imageyu2020perspective)"
        title="Click to show details"
      >
        Perspective Matters: Design Implications for Motion Guidance in Mixed Reality<a class="anchor" name="yu2020perspective"></a>
      </h3>
      <div>
        Xingyao Yu, Katrin Angerbauer, Peter Mohr, Denis Kalkofen, Michael Sedlmair
      </div>
      <div>
        ISMAR (2020) Full Paper
        <a href="https://doi.org/10.1109/ISMAR50242.2020.00085" target="_blank">website</a>
        <a href="../pdf/yu2020perspective.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We investigate how Mixed Reality (MR) can be used to guide human body motions, such as in physiotherapy, dancing, or workout applications. While first MR prototypes have shown promising results, many dimensions of the design space behind such applications remain largely unexplored. To better understand this design space, we approach the topic from different angles by contributing three user studies. In particular, we take a closer look at the influence of the perspective, the characteristics of motions, and visual guidance on different user performance measures. Our results indicate that a first-person perspective performs best for all visible motions, whereas the type of visual instruction plays a minor role. From our results we compile a set of considerations that can guide future work on the design of instructions, evaluations, and the technical setup of MR motion guidance systems.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@INPROCEEDINGS{9284729,
  author={Yu, Xingyao and Angerbauer, Katrin and Mohr, Peter and Kalkofen, Denis and Sedlmair, Michael},
  booktitle={2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Perspective Matters: Design Implications for Motion Guidance in Mixed Reality}, 
  year={2020},
  pages={577-587},
  doi={10.1109/ISMAR50242.2020.00085}}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/yu2020perspective.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  
  <div class="paper small" id="paperbalestrucci2020beliv">
    
      <img
        id="imagebalestrucci2020beliv"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperbalestrucci2020beliv', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/balestrucci2020beliv.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperbalestrucci2020beliv', 'small'); toggleImageSize(imagebalestrucci2020beliv)"
        title="Click to show details"
      >
        Pipelines Bent, Pipelines Broken: Interdisciplinary Self-Reflection on the Impact of COVID-19 on Current and Future Research (Position Paper)<a class="anchor" name="balestrucci2020beliv"></a>
      </h3>
      <div>
        Priscilla Balestrucci, Katrin Angerbauer, Cristina Morariu, Robin Welsch, Lewis L Chuang, Daniel Weiskopf, Marc O Ernst, Michael Sedlmair
      </div>
      <div>
        BELIV (2020) Workshop
        <a href="https://doi.org/10.1109/BELIV51497.2020.00009" target="_blank">website</a>
        <a href="../pdf/balestrucci2020beliv.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Among the many changes brought about by the COVID-19 pandemic, one of the most pressing for scientific research concerns user testing. For the researchers who conduct studies with human participants, the requirements for social distancing have created a need for reflecting on methodologies that previously seemed relatively straightforward. It has become clear from the emerging literature on the topic and from first-hand experiences of researchers that the restrictions due to the pandemic affect every aspect of the research pipeline. The current paper offers an initial reflection on user-based research, drawing on the authors' own experiences and on the results of a survey that was conducted among researchers in different disciplines, primarily psychology, human-computer interaction (HCI), and visualization communities. While this sampling of researchers is by no means comprehensive, the multi-disciplinary approach and the consideration of different aspects of the research pipeline allow us to examine current and future challenges for user-based research. Through an exploration of these issues, this paper also invites others in the VIS-as well as in the wider-research community, to reflect on and discuss the ways in which the current crisis might also present new and previously unexplored opportunities.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@INPROCEEDINGS{9307759,
  author={Balestrucci, Priscilla and Angerbauer, Katrin and Morariu, Cristina and Welsch, Robin and Chuang, Lewis L. and Weiskopf, Daniel and Ernst, Marc O. and Sedlmair, Michael},
  booktitle={2020 IEEE Workshop on Evaluation and Beyond - Methodological Approaches to Visualization (BELIV)}, 
  title={Pipelines Bent, Pipelines Broken: Interdisciplinary Self-Reflection on the Impact of COVID-19 on Current and Future Research (Position Paper)}, 
  year={2020},
  pages={11-18},
  doi={10.1109/BELIV51497.2020.00009}}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/balestrucci2020beliv.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  
  <div class="paper small" id="paperweiss2020revisited">
    
      <img
        id="imageweiss2020revisited"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperweiss2020revisited', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/weiss2020revisited.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperweiss2020revisited', 'small'); toggleImageSize(imageweiss2020revisited)"
        title="Click to show details"
      >
        Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes<a class="anchor" name="weiss2020revisited"></a>
      </h3>
      <div>
        Maximilian Weiß, Katrin Angerbauer, Alexandra Voit, Magdalena Schwarzl, Michael Sedlmair, Sven Mayer
      </div>
      <div>
        VIS (2020) Full Paper
        <a href="https://doi.org/10.1109/TVCG.2020.3030400" target="_blank">website</a>
        <a href="../pdf/weiss2020revisited.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@ARTICLE{9225008,
  author={Weiß, Maximilian and Angerbauer, Katrin and Voit, Alexandra and Schwarzl, Magdalena and Sedlmair, Michael and Mayer, Sven},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes}, 
  year={2021},
  volume={27},
  number={2},
  pages={1204-1213},
  doi={10.1109/TVCG.2020.3030400}}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/weiss2020revisited.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  
  <div class="paper small" id="paperstreichert2020etra">
    
      <img
        id="imagestreichert2020etra"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperstreichert2020etra', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/streichert2020etra.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperstreichert2020etra', 'small'); toggleImageSize(imagestreichert2020etra)"
        title="Click to show details"
      >
        Comparing Input Modalities for Shape Drawing Tasks<a class="anchor" name="streichert2020etra"></a>
      </h3>
      <div>
        Annalena Streichert, Katrin Angerbauer, Magdalena Schwarzl, Michael Sedlmair
      </div>
      <div>
        ETVIS (2020) Workshop / Short Paper
        <a href="https://doi.org/10.1145/3379156.3391830" target="_blank">website</a>
        <a href="../pdf/streichert2020etra.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N= 20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3379156.3391830,
author = {Streichert, Annalena and Angerbauer, Katrin and Schwarzl, Magdalena and Sedlmair, Michael},
title = {Comparing Input Modalities for Shape Drawing Tasks},
year = {2020},
isbn = {9781450371346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379156.3391830},
doi = {10.1145/3379156.3391830},
abstract = {With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N=20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods.},
booktitle = {ACM Symposium on Eye Tracking Research and Applications},
articleno = {51},
numpages = {5},
keywords = {Immersive analytics, input modalities, interaction},
location = {Stuttgart, Germany},
series = {ETRA '20 Short Papers}
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/streichert2020etra.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  
  <div class="paper small" id="paperkraus2020chi">
    
      <img
        id="imagekraus2020chi"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperkraus2020chi', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/kraus2020chi.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperkraus2020chi', 'small'); toggleImageSize(imagekraus2020chi)"
        title="Click to show details"
      >
        Assessing 2D and 3D Heatmaps for Comparative Analysis: An Empirical Study<a class="anchor" name="kraus2020chi"></a>
      </h3>
      <div>
        Matthias Kraus, Katrin Angerbauer, Juri Buchmüller, Daniel Schweitzer, Daniel A Keim, Michael Sedlmair, Johannes Fuchs
      </div>
      <div>
        CHI (2020) Full Paper
        <a href="https://doi.org/10.1145/3313831.3376675" target="_blank">website</a>
        <a href="../pdf/kraus2020chi.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Heatmaps are a popular visualization technique that encode 2D density distributions using color or brightness. Experimental studies have shown though that both of these visual variables are inaccurate when reading and comparing numeric data values. A potential remedy might be to use 3D heatmaps by introducing height as a third dimension to encode the data. Encoding abstract data in 3D, however, poses many problems, too. To better understand this tradeoff, we conducted an empirical study (N=48) to evaluate the user performance of 2D and 3D heatmaps for comparative analysis tasks. We test our conditions on a conventional 2D screen, but also in a virtual reality environment to allow for real stereoscopic vision. Our main results show that 3D heatmaps are superior in terms of error rate when reading and comparing single data items. However, for overview tasks, the well-established 2D heatmap performs better.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inbook{10.1145/3313831.3376675,
author = {Kraus, Matthias and Angerbauer, Katrin and Buchm\"{u}ller, Juri and Schweitzer, Daniel and Keim, Daniel A. and Sedlmair, Michael and Fuchs, Johannes},
title = {Assessing 2D and 3D Heatmaps for Comparative Analysis: An Empirical Study},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376675},
abstract = {Heatmaps are a popular visualization technique that encode 2D density distributions using color or brightness. Experimental studies have shown though that both of these visual variables are inaccurate when reading and comparing numeric data values. A potential remedy might be to use 3D heatmaps by introducing height as a third dimension to encode the data. Encoding abstract data in 3D, however, poses many problems, too. To better understand this tradeoff, we conducted an empirical study (N=48) to evaluate the user performance of 2D and 3D heatmaps for comparative analysis tasks. We test our conditions on a conventional 2D screen, but also in a virtual reality environment to allow for real stereoscopic vision. Our main results show that 3D heatmaps are superior in terms of error rate when reading and comparing single data items. However, for overview tasks, the well-established 2D heatmap performs better.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–14},
numpages = {14}
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/kraus2020chi.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  
  <div class="paper small" id="paperkurzhals2020chi">
    
      <img
        id="imagekurzhals2020chi"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperkurzhals2020chi', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/kurzhals2020chi.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperkurzhals2020chi', 'small'); toggleImageSize(imagekurzhals2020chi)"
        title="Click to show details"
      >
        A View on the Viewer: Gaze-Adaptive Captions for Videos<a class="anchor" name="kurzhals2020chi"></a>
      </h3>
      <div>
        Kuno Kurzhals, Fabian Göbel, Katrin Angerbauer, Michael Sedlmair, Martin Raubal
      </div>
      <div>
        CHI (2020) Full Paper
        <a href="https://doi.org/10.1145/3313831.3376266" target="_blank">website</a>
        <a href="../pdf/kurzhals2020chi.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inbook{10.1145/3313831.3376266,
author = {Kurzhals, Kuno and G\"{o}bel, Fabian and Angerbauer, Katrin and Sedlmair, Michael and Raubal, Martin},
title = {A View on the Viewer: Gaze-Adaptive Captions for Videos},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376266},
abstract = {Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12}
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/kurzhals2020chi.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  <h2>2019</h2>
  <div class="paper small" id="paperangerbauer2019interspeech">
    
      <img
        id="imageangerbauer2019interspeech"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperangerbauer2019interspeech', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/angerbauer2019interspeech.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperangerbauer2019interspeech', 'small'); toggleImageSize(imageangerbauer2019interspeech)"
        title="Click to show details"
      >
        Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience<a class="anchor" name="angerbauer2019interspeech"></a>
      </h3>
      <div>
        Katrin Angerbauer, Heike Adel, Ngoc Thang Vu
      </div>
      <div>
        INTERSPEECH (2019) Poster
        <a href="https://doi.org/10.21437/Interspeech.2019-1750" target="_blank">website</a>
        <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2019/angerbauer19_interspeech.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Understanding spoken language can be impeded through factors like noisy environments, hearing impairments or lack of proficiency. Subtitles can help in those cases. However, for fast speech or limited screen size, it might be advantageous to compress the subtitles to their most relevant content. Therefore, we address automatic sentence compression in this paper. We propose a neural network model based on an encoder-decoder approach with the possibility of integrating the desired compression ratio. Using this model, we conduct a user study to investigate the effects of compressed subtitles on user experience. Our results show that compressed subtitles can suffice for comprehension but may pose additional cognitive load.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{angerbauer19_interspeech,
  author={Katrin Angerbauer and Heike Adel and Ngoc Thang Vu},
  title={{Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience}},
  year=2019,
  booktitle={Proc. Interspeech 2019},
  pages={594--598},
  doi={10.21437/Interspeech.2019-1750}
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/angerbauer2019interspeech.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  
  <div class="paper small" id="paperkern2019lessons">
    
      <img
        id="imagekern2019lessons"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperkern2019lessons', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/kern2019lessons.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperkern2019lessons', 'small'); toggleImageSize(imagekern2019lessons)"
        title="Click to show details"
      >
        Lessons Learned from Users Reading Highlighted Abstracts in a Digital Library<a class="anchor" name="kern2019lessons"></a>
      </h3>
      <div>
        Dagmar Kern, Daniel Hienert, Katrin Angerbauer, Tilman Dingler, Pia Borlund
      </div>
      <div>
        CHIIR (2019) Short Paper
        <a href="https://doi.org/10.1145/3295750.3298950" target="_blank">website</a>
        <a href="../pdf/kern2019lessons.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Finding relevant documents is essential for researchers of all disciplines. We investigated an approach for supporting searchers in their relevance decision in a digital library by automatically highlighting the most important keywords in abstracts. We conducted an eye-tracking study with 25 subjects and observed very different search and reading behavior which lead to diverse results. Some of the participants liked that highlighted abstracts accelerate their relevance decision, while others found that they disturb the reading flow. What many agree on is that the quality of highlighting is crucial for trust and system credibility.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3295750.3298950,
author = {Kern, Dagmar and Hienert, Daniel and Angerbauer, Katrin and Dingler, Tilman and Borlund, Pia},
title = {Lessons Learned from Users Reading Highlighted Abstracts in a Digital Library},
year = {2019},
isbn = {9781450360258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295750.3298950},
doi = {10.1145/3295750.3298950},
abstract = {Finding relevant documents is essential for researchers of all disciplines. We investigated an approach for supporting searchers in their relevance decision in a digital library by automatically highlighting the most important keywords in abstracts. We conducted an eye-tracking study with 25 subjects and observed very different search and reading behavior which lead to diverse results. Some of the participants liked that highlighted abstracts accelerate their relevance decision, while others found that they disturb the reading flow. What many agree on is that the quality of highlighting is crucial for trust and system credibility.},
booktitle = {Proceedings of the 2019 Conference on Human Information Interaction and Retrieval},
pages = {271–275},
numpages = {5},
keywords = {user study, highlighting, reading behavior, relevance judgment},
location = {Glasgow, Scotland UK},
series = {CHIIR '19}
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/kern2019lessons.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  <h2>2017</h2>
  <div class="paper small" id="paperangerbauer2017valuetools">
    
      <img
        id="imageangerbauer2017valuetools"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperangerbauer2017valuetools', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/angerbauer2017valuetools.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperangerbauer2017valuetools', 'small'); toggleImageSize(imageangerbauer2017valuetools)"
        title="Click to show details"
      >
        The Back End is Only One Part of the Picture: Mobile-Aware Application Performance Monitoring and Problem Diagnosis<a class="anchor" name="angerbauer2017valuetools"></a>
      </h3>
      <div>
        Katrin Angerbauer, Dušan Okanović,  André van Hoorn, Christoph Heger
      </div>
      <div>
        VALUETOOLS (2017) Full Paper
        <a href="https://doi.org/10.1145/3150928.3150939" target="_blank">website</a>
        <a href="../pdf/angerbauer2017valuetools.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">The success of modern businesses relies on the quality of their supporting application systems. Continuous application performance management is mandatory to enable efficient problem detection, diagnosis, and resolution during production. In today's age of ubiquitous computing, large fractions of users access application systems from mobile devices, such as phones and tablets. For detecting, diagnosing, and resolving performance and availability problems, an end-to-end view, i.e., traceability of requests starting on the (mobile) clients' devices, is becoming increasingly important. In this paper, we propose an approach for end-to-end monitoring of applications from the users' mobile devices to the back end, and diagnosing root-causes of detected performance problems. We extend our previous work on diagnosing performance anti-patterns from execution traces by new metrics and rules. The evaluation of this work shows that our approach successfully detects and diagnoses performance anti-patterns in applications with iOS-based mobile clients. While there are threats to validity to our experiment, our research is a promising starting point for future work.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3150928.3150939,
author = {Angerbauer, Katrin and Okanovi\'{c}, Du\v{s}an and van Hoorn, Andr\'{e} and Heger, Christoph},
title = {The Back End is Only One Part of the Picture: Mobile-Aware Application Performance Monitoring and Problem Diagnosis},
year = {2017},
isbn = {9781450363464},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3150928.3150939},
doi = {10.1145/3150928.3150939},
abstract = {The success of modern businesses relies on the quality of their supporting application systems. Continuous application performance management is mandatory to enable efficient problem detection, diagnosis, and resolution during production. In today's age of ubiquitous computing, large fractions of users access application systems from mobile devices, such as phones and tablets. For detecting, diagnosing, and resolving performance and availability problems, an end-to-end view, i.e., traceability of requests starting on the (mobile) clients' devices, is becoming increasingly important. In this paper, we propose an approach for end-to-end monitoring of applications from the users' mobile devices to the back end, and diagnosing root-causes of detected performance problems. We extend our previous work on diagnosing performance anti-patterns from execution traces by new metrics and rules. The evaluation of this work shows that our approach successfully detects and diagnoses performance anti-patterns in applications with iOS-based mobile clients. While there are threats to validity to our experiment, our research is a promising starting point for future work.},
booktitle = {Proceedings of the 11th EAI International Conference on Performance Evaluation Methodologies and Tools},
pages = {82–89},
numpages = {8},
keywords = {performance anti-patterns, application performance monitoring, iOS},
location = {Venice, Italy},
series = {VALUETOOLS 2017}
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/angerbauer2017valuetools.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  
  <div class="paper small" id="paperdingler2017text">
    
      <img
        id="imagedingler2017text"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperdingler2017text', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/dingler2017text.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperdingler2017text', 'small'); toggleImageSize(imagedingler2017text)"
        title="Click to show details"
      >
        Text Priming-Effects of Text Visualizations on Readers Prior to Reading<a class="anchor" name="dingler2017text"></a>
      </h3>
      <div>
        Tilman Dingler, Dagmar Kern, Katrin Angerbauer, Albrecht Schmidt
      </div>
      <div>
        INTERACT (2017) Full Paper
        <a href="https://doi.org/10.1007/978-3-319-67687-6_23" target="_blank">website</a>
        <a href="https://hal.inria.fr/hal-01717225/document" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Living in our information society poses the challenge of having to deal with a plethora of information. While most content is represented through text, keyword extraction and visualization techniques allow the processing and adjustment of text presentation to the readers’ individual requirements and preferences. In this paper, we investigate four types of text visualizations and their feasibility to give readers an overview before they actually engage with a text: word clouds, highlighting, mind maps, and image collages. In a user study with 50 participants, we assessed the effects of such visualizations on reading comprehension, reading time, and subjective impressions. Results show that (1) mind maps best support readers in getting the gist of a text, (2) they also give better subjective impressions on text content and structure, and (3) highlighting keywords in a text before reading helps to reduce reading time. We discuss a set of guidelines to inform the design of automated systems for creating text visualizations for reader support.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@InProceedings{10.1007/978-3-319-67687-6_23,
author="Dingler, Tilman
and Kern, Dagmar
and Angerbauer, Katrin
and Schmidt, Albrecht",
editor="Bernhaupt, Regina
and Dalvi, Girish
and Joshi, Anirudha
and K. Balkrishan, Devanuj
and O'Neill, Jacki
and Winckler, Marco",
title="Text Priming - Effects of Text Visualizations on Readers Prior to Reading",
booktitle="Human-Computer Interaction -- INTERACT 2017",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="345--365",
abstract="Living in our information society poses the challenge of having to deal with a plethora of information. While most content is represented through text, keyword extraction and visualization techniques allow the processing and adjustment of text presentation to the readers' individual requirements and preferences. In this paper, we investigate four types of text visualizations and their feasibility to give readers an overview before they actually engage with a text: word clouds, highlighting, mind maps, and image collages. In a user study with 50 participants, we assessed the effects of such visualizations on reading comprehension, reading time, and subjective impressions. Results show that (1) mind maps best support readers in getting the gist of a text, (2) they also give better subjective impressions on text content and structure, and (3) highlighting keywords in a text before reading helps to reduce reading time. We discuss a set of guidelines to inform the design of automated systems for creating text visualizations for reader support.",
isbn="978-3-319-67687-6"
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/dingler2017text.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
  <h2>2015</h2>
  <div class="paper small" id="paperangerbauer2015chi">
    
      <img
        id="imageangerbauer2015chi"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperangerbauer2015chi', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/angerbauer2015chi.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperangerbauer2015chi', 'small'); toggleImageSize(imageangerbauer2015chi)"
        title="Click to show details"
      >
        Utilizing the Effects of Priming to Facilitate Text Comprehension<a class="anchor" name="angerbauer2015chi"></a>
      </h3>
      <div>
        Katrin Angerbauer, Tilman Dingler, Dagmar Kern, Albrecht Schmidt
      </div>
      <div>
        CHI (2015) Extended Abstract
        <a href="https://doi.org/10.1145/2702613.2732914" target="_blank">website</a>
        <a href="../pdf/angerbauer2015chi.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Due to the ever-growing amount of textual information we face in our everyday life, the skill of scanning and absorbing the essence of a piece of text is crucial. We cannot afford to read every text in detail, hence we need to acquire strategies to quickly decide on the importance of a text and how to grasp its content. Additionally, the sheer amount of daily reading makes it hard to remember the gist of every text encountered. Research in psychology has proposed priming as an implicit memory effect where exposure to one stimulus influences the response to a subsequent stimulus. Hence, exposure to contextual information can influence comprehension and recall. In our work we investigate the feasibility of using such an effect to visually present text summaries that are quick to understand and deliver the essence of a text in order to help readers not only make informed decisions about whether to read the text or not, but also to build out more cognitive associations that help to remember the content of the text afterward. In two focus groups we discussed our approach by providing four different visualizations representing the gist and important details of the text. In this paper we introduce the visualizations as well as results of the focus groups.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/2702613.2732914,
author = {Angerbauer, Katrin and Dingler, Tilman and Kern, Dagmar and Schmidt, Albrecht},
title = {Utilizing the Effects of Priming to Facilitate Text Comprehension},
year = {2015},
isbn = {9781450331463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702613.2732914},
doi = {10.1145/2702613.2732914},
abstract = {Due to the ever-growing amount of textual information we face in our everyday life, the skill of scanning and absorbing the essence of a piece of text is crucial. We cannot afford to read every text in detail, hence we need to acquire strategies to quickly decide on the importance of a text and how to grasp its content. Additionally, the sheer amount of daily reading makes it hard to remember the gist of every text encountered. Research in psychology has proposed priming as an implicit memory effect where exposure to one stimulus influences the response to a subsequent stimulus. Hence, exposure to contextual information can influence comprehension and recall. In our work we investigate the feasibility of using such an effect to visually present text summaries that are quick to understand and deliver the essence of a text in order to help readers not only make informed decisions about whether to read the text or not, but also to build out more cognitive associations that help to remember the content of the text afterward. In two focus groups we discussed our approach by providing four different visualizations representing the gist and important details of the text. In this paper we introduce the visualizations as well as results of the focus groups.},
booktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
pages = {1043–1048},
numpages = {6},
keywords = {comprehension, reading interfaces, priming},
location = {Seoul, Republic of Korea},
series = {CHI EA '15}
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/angerbauer2015chi.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
      </article>
    </div>
  </main>
</body>
</html>