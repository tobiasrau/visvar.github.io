<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Alexander Achberger, M.Sc. | VISVAR Research Group, University of Stuttgart</title>
  <link rel="stylesheet" href="../style.css">
  <script src="../script.js"></script>
  <link rel="shortcut icon" href="../img/favicon.png">
  <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
  <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
</head>
<body>
  <a class="anchor" name="top"></a>
  <main>
    <div>
      
<header>
  <div>
    <a href="https://visvar.github.io/">
      <h1 class="h1desktop"><div>VISVAR</div><div>Research</div><div>Group</div></h1>
      <h1 class="h1mobile">VISVAR</h1>
    </a>
  </div>
  <div>
    <nav>
      <ul>
        <li>
          <a href="https://visvar.github.io/#aboutus">about VISVAR</a>
        </li>
        <li>
          <a href="https://visvar.github.io/#publications">publications</a>
        </li>
        <li class="memberNav">
          <a href="https://visvar.github.io/#members">members</a>
        </li>
        <ul class="memberNav">
          
            <li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>
          
            <li><a href="https://visvar.github.io/members/quynh_quang_ngo.html">Quynh Quang Ngo</a></li>
          
            <li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>
          
            <li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>
          
            <li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>
          
            <li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>
          
            <li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>
          
            <li><a href="https://visvar.github.io/members/markus_wieland.html">Markus Wieland</a></li>
          
            <li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>
          
            <li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>
          
            <li><a href="https://visvar.github.io/members/nina_doerr.html">Nina Dörr</a></li>
          
            <li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>
          
            <li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>
          
            <li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>
          
            <li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>
          
            <li><a href="https://visvar.github.io/members/tobias_rau.html">Tobias Rau</a></li>
          
            <li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>
          
        </ul>
      </ul>
    </nav>
  </div>
</header>
    </div>
    <div>
      <article> <a class="anchor" name="aboutus"></a>
        <h1>Alexander Achberger, M.Sc.</h1>
        <div class="aboutMember">
          <div class="avatarAndBio">
            <img class="avatar" src="../img/people/alexander_achberger.jpg" />
            <div class="bio"></div>
          </div>
          <div class="furtherInfo">
            <div>
              <h2>Research Interests</h2>
              <ul>
                <li>HCI</li>
<li>Haptics</li>
              </ul>
            </div>
            <div>
              <h2>Links</h2>
              <ul>
                <li><a href="https://www.visus.uni-stuttgart.de/en/institute/team/Achberger/" target="_blank" rel="noreferrer">University of Stuttgart website</a></li>
<li><a href="https://scholar.google.com/citations?user=V3QAjFsAAAAJ" target="_blank" rel="noreferrer">Google Scholar</a></li>
              </ul>
            </div>
            
          </div>
        </div>
      </article>
      <article> <a class="anchor" name="publications"></a>
        <h1>Publications</h1>
        
  <h2 class="yearHeading">2022</h2>
  <div class="paper small" id="paperhube2022study">
    
    <div class="metaData noImage">
      <h3
        onclick="toggleClass('paperhube2022study', 'small'); toggleImageSize(imagehube2022study)"
        title="Click to show details"
      >
        Study on the Influence of Upper Limb Representations and Haptic Feedback in Virtual Reality<a class="anchor" name="hube2022study"></a>
      </h3>
      <div>
        Natalie Hube, Alexander Achberger, Philipp Liepert, Jonas Vogelsang, Kresimir Vidackovic, Michael Sedlmair
      </div>
      <div>
        ISMAR-adjunct (2022) 
        <a href="https://visvar.github.io/pub/hube2022study.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1109/ISMAR-Adjunct57072.2022.00172" target="_blank">DOI</a>
        
        
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Visual representations are used in various digitization processes to display objects in immersive environments. In industrial environments, the self-representation can be very helpful when reviewing 3D models to conform realistic proportions, particularly in combi-nation with haptic feedback that goes beyond vibrating controllers. For instance, haptic feedback in combination with a virtual representation supports working on use cases where collision is an important part to maintain data quality. To understand the dependency of both, we conducted a pilot study with 15 users from the automotive sector to examine the influence of upper limb representations on haptic feedback in Virtual Reality. Each participant was assigned with one of three upper limb representations which was used in different scenarios using haptic feedback devices. Overall, we found that the realistic arm representation was rated highest in terms of perceived realism and achieved the best task performance.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@INPROCEEDINGS{9974563,
  author={Hube, Natalie and Achberger, Alexander and Liepert, Philipp and Vogelsang, Jonas and Vidačković, Krešimir and Sedlmair, Michael},
  booktitle={2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Study on the Influence of Upper Limb Representations and Haptic Feedback in Virtual Reality}, 
  year={2022},
  pages={802-807},
  doi={10.1109/ISMAR-Adjunct57072.2022.00172}}</textarea></div>
      
    </div>
  </div>
  
  
  <div class="paper small" id="paperachberger2022touching">
    
      <img
        id="imageachberger2022touching"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperachberger2022touching', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/achberger2022touching.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperachberger2022touching', 'small'); toggleImageSize(imageachberger2022touching)"
        title="Click to show details"
      >
        Touching Data with PropellerHand<a class="anchor" name="achberger2022touching"></a>
      </h3>
      <div>
        Alexander Achberger, Frank Heyen, Kresimir Vidackovic, Michael Sedlmair 
      </div>
      <div>
        Journal of Visualization (2022) Full Paper
        <a href="https://visvar.github.io/pub/achberger2022touching.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1007/s12650-022-00859-2" target="_blank">DOI</a>
        
        <a href="../pdf/achberger2022touching.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Immersive analytics often takes place in virtual environments which promise the users immersion. To fulfill this promise, sensory feedback, such as haptics, is an important component, which is however not well supported yet. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our prototype through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality. Additionally, we conducted a second user study in the field of immersive visualization to investigate the potential benefits of PropellerHand there.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@article{achberger2022touching,
    title={Touching data with PropellerHand},
    author={Achberger, Alexander and Heyen, Frank and Vidackovic, Kresimir and Sedlmair, Michael},
    journal={Journal of Visualization},
    year={2022},
    doi={10.1007/s12650-022-00859-2},
    url={https://doi.org/10.1007/s12650-022-00859-2}
}</textarea></div>
      <h4>Acknowledgements</h4><div class="abstract">Partially supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy – EXC 2120/1 – 390831618</div>
    </div>
  </div>
  
  
  <div class="paper small" id="paperachberger2022stroe">
    
      <img
        id="imageachberger2022stroe"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperachberger2022stroe', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/achberger2022stroe.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperachberger2022stroe', 'small'); toggleImageSize(imageachberger2022stroe)"
        title="Click to show details"
      >
        STROE: An Ungrounded String-Based Weight Simulation Device<a class="anchor" name="achberger2022stroe"></a>
      </h3>
      <div>
        Alexander Achberger, Pirathipan Arulrajah, Michael Sedlmair, Kresimir Vidackovic
      </div>
      <div>
        VR (2022) Full Paper
        <a href="https://visvar.github.io/pub/achberger2022stroe.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1109/VR51125.2022.00029" target="_blank">DOI</a>
        
        <a href="../pdf/achberger2022stroe.pdf" target="_blank">PDF</a>
        <a href="https://www.youtube.com/watch?v=9edaBf7VqNY" target="_blank">video</a>
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user’s hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users’ perceived realism and immersion of VR scenes.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@INPROCEEDINGS{9756818,
  author={Achberger, Alexander and Arulrajah, Pirathipan and Sedlmair, Michael and Vidackovic, Kresimir},
  booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={{STROE}: An Ungrounded String-Based Weight Simulation Device}, 
  year={2022},
  pages={112-120},
  abstract={We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user’s hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users’ perceived realism and immersion of VR scenes.},
  doi={10.1109/VR51125.2022.00029},
  ISSN={2642-5254}
}</textarea></div>
      
    </div>
  </div>
  
  <h2 class="yearHeading">2021</h2>
  <div class="paper small" id="paperachberger2021uist">
    
      <img
        id="imageachberger2021uist"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperachberger2021uist', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/achberger2021uist.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperachberger2021uist', 'small'); toggleImageSize(imageachberger2021uist)"
        title="Click to show details"
      >
        Strive: String-Based Force Feedback for Automotive Engineering<a class="anchor" name="achberger2021uist"></a>
      </h3>
      <div>
        Alexander Achberger, Fabian Aust, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair
      </div>
      <div>
        UIST (2021) Full Paper
        <a href="https://visvar.github.io/pub/achberger2021uist.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3472749.3474790" target="_blank">DOI</a>
        
        <a href="../pdf/achberger2021uist.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">The large potential of force feedback devices for interacting in Virtual Reality (VR) has been illustrated in a plethora of research prototypes. Yet, these devices are still rarely used in practice and it remains an open challenge how to move this research into practice. To that end, we contribute a participatory design study on the use of haptic feedback devices in the automotive industry. Based on a 10-month observing process with 13 engineers, we developed STRIVE, a string-based haptic feedback device. In addition to the design of STRIVE, this process led to a set of requirements for introducing haptic devices into industrial settings, which center around a need for flexibility regarding forces, comfort, and mobility. We evaluated STRIVE with 16 engineers in five different day-to-day automotive VR use cases. The main results show an increased level of trust and perceived safety as well as further challenges towards moving haptics research into practice. </div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{achberger2021uist,
	Author = {Alexander Achberger and Fabian Aust and Daniel Pohlandt and Kresimir Vidackovic and Michael Sedlmair},
	Title = {{STRIVE}: String-Based Force Feedback for Automotive Engineering},
	Booktitle = {ACM Symposium on User Interface Software and Technology (UIST)},
	pages = {841--853},
	url = {https://doi.org/10.1145/3472749.3474790},
  	doi = {10.1145/3472749.3474790},
	Year = {2021}
}</textarea></div>
      
    </div>
  </div>
  
  
  <div class="paper small" id="paperachberger2021vinci">
    
      <img
        id="imageachberger2021vinci"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperachberger2021vinci', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/achberger2021vinci.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperachberger2021vinci', 'small'); toggleImageSize(imageachberger2021vinci)"
        title="Click to show details"
      >
        PropellerHand: Hand-Mounted, Propeller-Based Force Feedback Device<a class="anchor" name="achberger2021vinci"></a>
      </h3>
      <div>
        Alexander Achberger, Frank Heyen, Kresimir Vidackovic, Michael Sedlmair
      </div>
      <div>
        VINCI (2021) Full Paper
        <a href="https://visvar.github.io/pub/achberger2021vinci.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3481549.3481563" target="_blank">DOI</a>
        
        <a href="../pdf/achberger2021vinci.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Immersive analytics is a fast growing field that is often applied in virtual reality (VR). VR environments often lack immersion due to missing sensory feedback when interacting with data. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our final version through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{achberger2021vinci,
  author = {Alexander Achberger and Frank Heyen and Kresimir Vidackovic and Michael Sedlmair},
  title = {PropellerHand: {A} Hand-Mounted, Propeller-Based Force Feedback Device},
  booktitle = {International Symposium on Visual Information Communication and Interaction (VINCI)},
  pages     = {4:1--4:8},
  publisher = {ACM},
  year      = {2021},
  url       = {https://doi.org/10.1145/3481549.3481563},
  doi       = {10.1145/3481549.3481563}
}</textarea></div>
      
    </div>
  </div>
  
  <h2 class="yearHeading">2020</h2>
  <div class="paper small" id="paperachberger2020caarvida">
    
      <img
        id="imageachberger2020caarvida"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperachberger2020caarvida', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/achberger2020caarvida.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperachberger2020caarvida', 'small'); toggleImageSize(imageachberger2020caarvida)"
        title="Click to show details"
      >
        Caarvida: Visual Analytics for Test Drive Videos<a class="anchor" name="achberger2020caarvida"></a>
      </h3>
      <div>
        Alexander Achberger, Rene Cutura, Oguzhan Türksoy, Michael Sedlmair
      </div>
      <div>
        AVI  (2020) Full Paper
        <a href="https://visvar.github.io/pub/achberger2020caarvida.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3399715.3399862" target="_blank">DOI</a>
        
        <a href="../pdf/achberger2020caarvida.pdf" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3399715.3399862,
author = {Achberger, Alexander and Cutura, Ren\'{e} and T\"{u}rksoy, Oguzhan and Sedlmair, Michael},
title = {Caarvida: Visual Analytics for Test Drive Videos},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399862},
doi = {10.1145/3399715.3399862},
abstract = {We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {6},
numpages = {9},
keywords = {visual analytics, object detection, automotive, information visualization, human computer interaction},
location = {Salerno, Italy},
series = {AVI '20}
}</textarea></div>
      
    </div>
  </div>
  
      </article>
    </div>
  </main>
</body>
</html>