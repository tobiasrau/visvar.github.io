<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Benjamin Lee, Ph.D. | VISVAR Research Group, University of Stuttgart</title>
  <link rel="stylesheet" href="../style.css">
  <script src="../script.js"></script>
  <link rel="shortcut icon" href="../img/favicon.png">
  <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
  <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
</head>
<body>
  <a class="anchor" name="top"></a>
  <main>
    
<div>
<header>
<div>
<a href="https://visvar.github.io/">
<h1 class="h1desktop"><div>VISVAR</div><div>Research</div><div>Group</div></h1>
<h1 class="h1mobile">VISVAR</h1>
</a>
</div>
<div>
<nav>
<ul>
<li><a href="https://visvar.github.io/#aboutus">about VISVAR</a></li>
<li><a href="https://visvar.github.io/#publications">publications</a></li>
<li class="memberNav"><a href="https://visvar.github.io/#members">members</a></li>
<ul class="memberNav">

<li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>

<li><a href="https://visvar.github.io/members/quynh_quang_ngo.html">Quynh Quang Ngo</a></li>

<li><a href="https://visvar.github.io/members/benjamin_lee.html">Benjamin Lee</a></li>

<li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>

<li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>

<li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>

<li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>

<li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>

<li><a href="https://visvar.github.io/members/markus_wieland.html">Markus Wieland</a></li>

<li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>

<li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>

<li><a href="https://visvar.github.io/members/nina_doerr.html">Nina Dörr</a></li>

<li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>

<li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>

<li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>

<li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>

<li><a href="https://visvar.github.io/members/tobias_rau.html">Tobias Rau</a></li>

<li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>

</ul>
</ul>
</nav>
</div>
</header>
</div>
    <div>
      <article><a class="anchor" name="aboutus"></a>
        <h1>Benjamin Lee, Ph.D.</h1>
        <div class="aboutMember">
          <div class="avatarAndBio">
            <img class="avatar" src="../img/people/benjamin_lee.jpg" />
            <div class="bio">
    <p>
      My research revolves around the field of Immersive Analytics: the use of virtual and/or augmented reality technologies to support visual data exploration and presentation.
      In particular, my work has investigated how 2D surfaces can be used inside of an immersive 3D space to facilitate (collaborative) visual analysis and data understanding.
    </p>
    <p>
      I submitted my PhD at the <a href="https://www.monash.edu/it/hcc/dvia-lab" target="_blank" rel="noreferrer">Data Visualisation and Immersive Analytics Lab</a>, Monash University.
      My supervisors were Prof. Tim Dwyer (main), A/Prof. Bernhard Jenny, Dr. Maxime Cordeil, and Dr. Arnaud Prouzeau.
      </p>
      </div>
          </div>
          <div class="furtherInfo">
            <div>
              <h2>Research Interests</h2>
              <ul>
                <li>Data visualization</li>
<li>Immersive analytics</li>
<li>Collaborative analysis</li>
<li>VR/AR</li>
              </ul>
            </div>
            <div>
              <h2>Links</h2>
              <ul>
                <li><a href="https://scholar.google.de/citations?user=oJUHhu4AAAAJ" target="_blank" rel="noreferrer">Google Scholar</a></li>
<li><a href="https://benjaminchlee.github.io/" target="_blank" rel="noreferrer">Personal website</a></li>
<li><a href="https://www.youtube.com/@benjaminchlee" target="_blank" rel="noreferrer">YouTube</a></li>
              </ul>
            </div>
            
          </div>
        </div>
      </article>
      <article> <a class="anchor" name="publications"></a>
        <h1>Publications</h1>
        
  <h2 class="yearHeading">2023</h2>
  <div class="paper small" id="paperlee2023deimos">
    
      <img
        id="imagelee2023deimos"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperlee2023deimos', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/lee2023deimos.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperlee2023deimos', 'small'); toggleImageSize(imagelee2023deimos)"
        title="Click to show details"
      >
        Deimos: A Grammar of Dynamic Embodied Immersive Visualisation Morphs and Transitions<a class="anchor" name="lee2023deimos"></a>
      </h3>
      <div>
        Benjamin Lee, Arvind Satyanarayan, Maxime Cordeil, Arnaud Prouzeau, Bernhard Jenny, Tim Dwyer
      </div>
      <div>
        CHI (2023) Full Paper
        <a href="https://visvar.github.io/pub/lee2023deimos.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3544548.3580754" target="_blank">DOI</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3580754" target="_blank">ACM</a>
        
        <a href="https://www.youtube.com/watch?v=L9Ngzh1w7nM" target="_blank">video</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3544548.3580754" target="_blank">supplemental</a>
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We present Deimos, a grammar for specifying dynamic embodied immersive visualisation morphs and transitions. A morph is a collection of animated transitions that are dynamically applied to immersive visualisations at runtime and is conceptually modelled as a state machine. It is comprised of state, transition, and signal specifications. States in a morph are used to generate animation keyframes, with transitions connecting two states together. A transition is controlled by signals, which are composable data streams that can be used to enable embodied interaction techniques. Morphs allow immersive representations of data to transform and change shape through user interaction, facilitating the embodied cognition process. We demonstrate the expressivity of Deimos in an example gallery and evaluate its usability in an expert user study of six immersive analytics researchers. Participants found the grammar to be powerful and expressive, and showed interest in drawing upon Deimos’ concepts and ideas in their own research.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3544548.3580754,
    title        = {Deimos: A Grammar of Dynamic Embodied Immersive Visualisation Morphs and Transitions},
    author       = {Lee, Benjamin and Satyanarayan, Arvind and Cordeil, Maxime and Prouzeau, Arnaud and Jenny, Bernhard and Dwyer, Tim},
    year         = {2023},
    booktitle    = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
    publisher    = {Association for Computing Machinery},
    series       = {CHI '23},
    doi          = {10.1145/3544548.3580754},
    url          = {https://doi.org/10.1145/3544548.3580754},
    abstract     = {We present Deimos, a grammar for specifying dynamic embodied immersive visualisation morphs and transitions. A morph is a collection of animated transitions that are dynamically applied to immersive visualisations at runtime and is conceptually modelled as a state machine. It is comprised of state, transition, and signal specifications. States in a morph are used to generate animation keyframes, with transitions connecting two states together. A transition is controlled by signals, which are composable data streams that can be used to enable embodied interaction techniques. Morphs allow immersive representations of data to transform and change shape through user interaction, facilitating the embodied cognition process. We demonstrate the expressivity of Deimos in an example gallery and evaluate its usability in an expert user study of six immersive analytics researchers. Participants found the grammar to be powerful and expressive, and showed interest in drawing upon Deimos' concepts and ideas in their own research.},
    articleno    = {810},
    numpages     = {18},
    keywords     = {grammar, user study, data visualisation, Immersive Analytics, embodied interaction, animated transitions}
}
</textarea></div>
      
    </div>
  </div>
  
  <h2 class="yearHeading">2022</h2>
  <div class="paper small" id="paperyang2022collaborative">
    
      <img
        id="imageyang2022collaborative"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperyang2022collaborative', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/yang2022collaborative.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperyang2022collaborative', 'small'); toggleImageSize(imageyang2022collaborative)"
        title="Click to show details"
      >
        Towards Immersive Collaborative Sensemaking<a class="anchor" name="yang2022collaborative"></a>
      </h3>
      <div>
        Ying Yang, Tim Dwyer, Michael Wybrow, Benjamin Lee, Maxime Cordeil, Mark Billinghurst, Bruce H Thomas
      </div>
      <div>
        ACM ISS (2022) 
        <a href="https://visvar.github.io/pub/yang2022collaborative.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3567741" target="_blank">DOI</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3567741" target="_blank">ACM</a>
        <a href="../pdf/yang2022collaborative.pdf" target="_blank">PDF</a>
        <a href="https://www.youtube.com/watch?v=8AxNxvPAdYk" target="_blank">video</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3567741#sec-supp" target="_blank">supplemental</a>
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">When collaborating face-to-face, people commonly use the surfaces and spaces around them to perform sensemaking tasks, such as spatially organising documents, notes or images. However, when people collaborate remotely using desktop interfaces they no longer feel like they are sharing the same space. This limitation may be overcome through collaboration in immersive environments, which simulate the physical in-person experience. In this paper, we report on a between-groups study comparing collaborations on image organisation tasks, in an immersive Virtual Reality (VR) environment to more conventional desktop conferencing. Collecting data from 40 subjects in groups of four, we measured task performance, user behaviours, collaboration engagement and awareness. Overall, the VR and desktop interface resulted in similar speed, accuracy and social presence rating, but we observed more conversations and interaction with objects, and more equal contributions to the interaction from participants within groups in VR. We also identified differences in coordination and collaborative awareness behaviours between VR and desktop platforms. We report on a set of systematic measures for assessing VR collaborative experience and a new analysis tool that we have developed to capture user behaviours in collaborative setting. Finally, we provide design considerations and directions for future work.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@article{10.1145/3567741,
    title        = {Towards Immersive Collaborative Sensemaking},
    author       = {Yang, Ying and Dwyer, Tim and Wybrow, Michael and Lee, Benjamin and Cordeil, Maxime and Billinghurst, Mark and Thomas, Bruce H.},
    year         = {2022},
    month        = {nov},
    journal      = {Proc. ACM Hum.-Comput. Interact.},
    publisher    = {Association for Computing Machinery},
    volume       = {6},
    number       = {ISS},
    doi          = {10.1145/3567741},
    url          = {https://doi.org/10.1145/3567741},
    issue_date   = {December 2022},
    abstract     = {When collaborating face-to-face, people commonly use the surfaces and spaces around them to perform sensemaking tasks, such as spatially organising documents, notes or images. However, when people collaborate remotely using desktop interfaces they no longer feel like they are sharing the same space. This limitation may be overcome through collaboration in immersive environments, which simulate the physical in-person experience. In this paper, we report on a between-groups study comparing collaborations on image organisation tasks, in an immersive Virtual Reality (VR) environment to more conventional desktop conferencing. Collecting data from 40 subjects in groups of four, we measured task performance, user behaviours, collaboration engagement and awareness. Overall, the VR and desktop interface resulted in similar speed, accuracy and social presence rating, but we observed more conversations and interaction with objects, and more equal contributions to the interaction from participants within groups in VR. We also identified differences in coordination and collaborative awareness behaviours between VR and desktop platforms. We report on a set of systematic measures for assessing VR collaborative experience and a new analysis tool that we have developed to capture user behaviours in collaborative setting. Finally, we provide design considerations and directions for future work.},
    articleno    = {588},
    numpages     = {25},
    keywords     = {Virtual Reality, Collaborative Sensemaking}
}
</textarea></div>
      
    </div>
  </div>
  
  
  <div class="paper small" id="papersatriadi2022tangible">
    
      <img
        id="imagesatriadi2022tangible"
        title="Click to enlarge and show details"
        onclick="toggleClass('papersatriadi2022tangible', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/satriadi2022tangible.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('papersatriadi2022tangible', 'small'); toggleImageSize(imagesatriadi2022tangible)"
        title="Click to show details"
      >
        Tangible Globes for Data Visualisation in Augmented Reality<a class="anchor" name="satriadi2022tangible"></a>
      </h3>
      <div>
        Kadek Satriadi, Jim Smiley, Barrett Ens, Maxime Cordeil, Tobias Czauderna, Benjamin Lee, Ying Yang, Tim Dwyer, Bernhard Jenny
      </div>
      <div>
        CHI (2022) Full Paper
        <a href="https://visvar.github.io/pub/satriadi2022tangible.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3491102.3517715" target="_blank">DOI</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3517715" target="_blank">ACM</a>
        <a href="../pdf/satriadi2022tangible.pdf" target="_blank">PDF</a>
        <a href="https://www.youtube.com/watch?v=QwlpML4D9lo" target="_blank">video</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3517715#sec-supp" target="_blank">supplemental</a>
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Head-mounted augmented reality (AR) displays allow for the seamless integration of virtual visualisation with contextual tangible references, such as physical (tangible) globes. We explore the design of immersive geospatial data visualisation with AR and tangible globes. We investigate the “tangible-virtual interplay” of tangible globes with virtual data visualisation, and propose a conceptual approach for designing immersive geospatial globes. We demonstrate a set of use cases, such as augmenting a tangible globe with virtual overlays, using a physical globe as a tangible input device for interacting with virtual globes and maps, and linking an augmented globe to an abstract data visualisation. We gathered qualitative feedback from experts about our use case visualisations, and compiled a summary of key takeaways as well as ideas for envisioned future improvements. The proposed design space, example visualisations and lessons learned aim to guide the design of tangible globes for data visualisation in AR. </div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3491102.3517715,
    title        = {Tangible Globes for Data Visualisation in Augmented Reality},
    author       = {Satriadi, Kadek Ananta and Smiley, Jim and Ens, Barrett and Cordeil, Maxime and Czauderna, Tobias and Lee, Benjamin and Yang, Ying and Dwyer, Tim and Jenny, Bernhard},
    year         = {2022},
    booktitle    = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
    publisher    = {Association for Computing Machinery},
    series       = {CHI '22},
    doi          = {10.1145/3491102.3517715},
    url          = {https://doi.org/10.1145/3491102.3517715},
    abstract     = {Head-mounted augmented reality (AR) displays allow for the seamless integration of virtual visualisation with contextual tangible references, such as physical (tangible) globes. We explore the design of immersive geospatial data visualisation with AR and tangible globes. We investigate the ``tangible-virtual interplay'' of tangible globes with virtual data visualisation, and propose a conceptual approach for designing immersive geospatial globes. We demonstrate a set of use cases, such as augmenting a tangible globe with virtual overlays, using a physical globe as a tangible input device for interacting with virtual globes and maps, and linking an augmented globe to an abstract data visualisation. We gathered qualitative feedback from experts about our use case visualisations, and compiled a summary of key takeaways as well as ideas for envisioned future improvements. The proposed design space, example visualisations and lessons learned aim to guide the design of tangible globes for data visualisation in AR.},
    articleno    = {505},
    numpages     = {16},
    keywords     = {immersive analytics, augmented reality, geographic visualisation, tangible user interface}
}
</textarea></div>
      
    </div>
  </div>
  
  
  <div class="paper small" id="paperlee2022design">
    
      <img
        id="imagelee2022design"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperlee2022design', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/lee2022design.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperlee2022design', 'small'); toggleImageSize(imagelee2022design)"
        title="Click to show details"
      >
        A Design Space For Data Visualisation Transformations Between 2D And 3D In Mixed-Reality Environments<a class="anchor" name="lee2022design"></a>
      </h3>
      <div>
        Benjamin Lee, Maxime Cordeil, Arnaud Prouzeau, Bernhard Jenny, Tim Dwyer
      </div>
      <div>
        CHI (2022) 
        <a href="https://visvar.github.io/pub/lee2022design.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3491102.3501859" target="_blank">DOI</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3501859" target="_blank">ACM</a>
        <a href="../pdf/lee2022design.pdf" target="_blank">PDF</a>
        <a href="https://www.youtube.com/watch?v=jjB99Ruc5gY" target="_blank">video</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3491102.3501859#sec-supp" target="_blank">supplemental</a>
        <span>Honourable Mention Award</span>
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">As mixed-reality (MR) technologies become more mainstream, the delineation between data visualisations displayed on screens or other surfaces and those floating in space becomes increasingly blurred. Rather than the choice of using either a 2D surface or the 3D space for visualising data being a dichotomy, we argue that users should have the freedom to transform visualisations seamlessly between the two as needed. However, the design space for such transformations is large, and practically uncharted. To explore this, we first establish an overview of the different states that a data visualisation can take in MR, followed by how transformations between these states can facilitate common visualisation tasks. We then describe a design space of how these transformations function, in terms of the different stages throughout the transformation, and the user interactions and input parameters that affect it. This design space is then demonstrated with multiple exemplary techniques based in MR.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3491102.3501859,
    title        = {A Design Space For Data Visualisation Transformations Between 2D And 3D In Mixed-Reality Environments},
    author       = {Lee, Benjamin and Cordeil, Maxime and Prouzeau, Arnaud and Jenny, Bernhard and Dwyer, Tim},
    year         = {2022},
    booktitle    = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
    publisher    = {Association for Computing Machinery},
    series       = {CHI '22},
    doi          = {10.1145/3491102.3501859},
    url          = {https://doi.org/10.1145/3491102.3501859},
    abstract     = {As mixed-reality (MR) technologies become more mainstream, the delineation between data visualisations displayed on screens or other surfaces and those floating in space becomes increasingly blurred. Rather than the choice of using either a 2D surface or the 3D space for visualising data being a dichotomy, we argue that users should have the freedom to transform visualisations seamlessly between the two as needed. However, the design space for such transformations is large, and practically uncharted. To explore this, we first establish an overview of the different states that a data visualisation can take in MR, followed by how transformations between these states can facilitate common visualisation tasks. We then describe a design space of how these transformations function, in terms of the different stages throughout the transformation, and the user interactions and input parameters that affect it. This design space is then demonstrated with multiple exemplary techniques based in MR.},
    articleno    = {25},
    numpages     = {14},
    keywords     = {animated transitions, mixed reality, Immersive Analytics, direct manipulation, visualisation}
}
</textarea></div>
      
    </div>
  </div>
  
  <h2 class="yearHeading">2021</h2>
  <div class="paper small" id="papersmiley2021madeaxis">
    
      <img
        id="imagesmiley2021madeaxis"
        title="Click to enlarge and show details"
        onclick="toggleClass('papersmiley2021madeaxis', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/smiley2021madeaxis.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('papersmiley2021madeaxis', 'small'); toggleImageSize(imagesmiley2021madeaxis)"
        title="Click to show details"
      >
        The MADE-Axis: A Modular Actuated Device to Embody the Axis of a Data Dimension<a class="anchor" name="smiley2021madeaxis"></a>
      </h3>
      <div>
        Jim Smiley, Benjamin Lee, Siddhant Tandon, Maxime Cordeil, Lonni Besançon, Jarrod Knibbe, Bernhard Jenny, Tim Dwyer
      </div>
      <div>
        ACM ISS (2021) 
        <a href="https://visvar.github.io/pub/smiley2021madeaxis.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3488546" target="_blank">DOI</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3488546" target="_blank">ACM</a>
        <a href="../pdf/smiley2021madeaxis.pdf" target="_blank">PDF</a>
        <a href="https://www.youtube.com/watch?v=ILZlecsvUbw" target="_blank">video</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3488546#sec-supp" target="_blank">supplemental</a>
        <span>Honourable Mention Award</span>
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Tangible controls-especially sliders and rotary knobs-have been explored in a wide range of interactive applications for desktop and immersive environments. Studies have shown that they support greater precision and provide proprioceptive benefits, such as support for eyes-free interaction. However, such controls tend to be expressly designed for specific applications. We draw inspiration from a bespoke controller for immersive data visualisation, but decompose this design into a simple, wireless, composable unit featuring two actuated sliders and a rotary encoder. Through these controller units, we explore the interaction opportunities around actuated sliders; supporting precise selection, infinite scrolling, adaptive data representations, and rich haptic feedback; all within a mode-less interaction space. We demonstrate the controllers' use for simple, ad hoc desktop interaction,before moving on to more complex, multi-dimensional interactions in VR and AR. We show that the flexibility and composability of these actuated controllers provides an emergent design space which covers the range of interactive dynamics for visual analysis. In a user study involving pairs performing collaborative visual analysis tasks in mixed-reality, our participants were able to easily compose rich visualisations, make insights and discuss their findings.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@article{10.1145/3488546,
    title        = {The MADE-Axis: A Modular Actuated Device to Embody the Axis of a Data Dimension},
    author       = {Smiley, Jim and Lee, Benjamin and Tandon, Siddhant and Cordeil, Maxime and Besan\c{c}on, Lonni and Knibbe, Jarrod and Jenny, Bernhard and Dwyer, Tim},
    year         = {2021},
    month        = {nov},
    journal      = {Proc. ACM Hum.-Comput. Interact.},
    publisher    = {Association for Computing Machinery},
    volume       = {5},
    number       = {ISS},
    doi          = {10.1145/3488546},
    url          = {https://doi.org/10.1145/3488546},
    issue_date   = {November 2021},
    abstract     = {Tangible controls-especially sliders and rotary knobs-have been explored in a wide range of interactive applications for desktop and immersive environments. Studies have shown that they support greater precision and provide proprioceptive benefits, such as support for eyes-free interaction. However, such controls tend to be expressly designed for specific applications. We draw inspiration from a bespoke controller for immersive data visualisation, but decompose this design into a simple, wireless, composable unit featuring two actuated sliders and a rotary encoder. Through these controller units, we explore the interaction opportunities around actuated sliders; supporting precise selection, infinite scrolling, adaptive data representations, and rich haptic feedback; all within a mode-less interaction space. We demonstrate the controllers' use for simple, ad hoc desktop interaction,before moving on to more complex, multi-dimensional interactions in VR and AR. We show that the flexibility and composability of these actuated controllers provides an emergent design space which covers the range of interactive dynamics for visual analysis. In a user study involving pairs performing collaborative visual analysis tasks in mixed-reality, our participants were able to easily compose rich visualisations, make insights and discuss their findings.},
    articleno    = {501},
    numpages     = {23},
    keywords     = {data visualization, embodied interfaces}
}
</textarea></div>
      
    </div>
  </div>
  
  
  <div class="paper small" id="paperspyrison2021is">
    
      <img
        id="imagespyrison2021is"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperspyrison2021is', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/spyrison2021is.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperspyrison2021is', 'small'); toggleImageSize(imagespyrison2021is)"
        title="Click to show details"
      >
        Is IEEE VIS *that* good? On key factors in the initial assessment of manuscript and venue quality<a class="anchor" name="spyrison2021is"></a>
      </h3>
      <div>
        Nicholas Spyrison, Benjamin Lee, Lonni Besançon
      </div>
      <div>
        alt.VIS (2021) 
        <a href="https://visvar.github.io/pub/spyrison2021is.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.31219/osf.io/65wm7" target="_blank">DOI</a>
        
        <a href="../pdf/spyrison2021is.pdf" target="_blank">PDF</a>
        
        <a href="https://osf.io/ch6p4/" target="_blank">supplemental</a>
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Background: Academic performance is at the heart of hiring decisions and funding applications. It is based on a combination of qualitative and quantitative metrics. One of those is the venue in which scholarly publications are published. Depending on the perceived (qualitative) or measured (quantitative) prestige associated with a venue, a specific publication will have more or less weight. 

Objectives: We want to understand how visualization researchers consider the prestige of a venue when looking for papers that they could use in their own manuscripts, and how they determine the prestige of any given venue.

Method: We ran an online survey open for 10 days that we sent out to visualization researchers.

Results: We gathered 46 responses from a sample of convenience. We found that publication venue plays the biggest part in how visualization researchers assess research articles. Interestingly, rating systems and metrics are least important criteria for researchers when assessing the quality of a venue. 

Conclusion: We highlight the potential risks around focusing on venue when assessing research articles. We further underline the necessity to discuss with the community on strategies to switch the focus to robustness and reliability to foster better practices and less stressful publishing expectations.

Reproducibility: Data, materials and preregistration available on osf.io/ch6p4/</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@misc{spyrison_lee_besancon_2021,
    title        = {"Is IEEE VIS \textasteriskcenteredthat\textasteriskcentered good?" On key factors in the initial assessment of manuscript and venue quality},
    author       = {Spyrison, Nicholas and Lee, Benjamin and Besan\c{c}on, Lonni},
    year         = {2021},
    month        = {Jul},
    publisher    = {OSF Preprints},
    doi          = {10.31219/osf.io/65wm7},
    url          = {osf.io/65wm7}
}
</textarea></div>
      
    </div>
  </div>
  
  <h2 class="yearHeading">2020</h2>
  <div class="paper small" id="paperlee2020data">
    
      <img
        id="imagelee2020data"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperlee2020data', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/lee2020data.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperlee2020data', 'small'); toggleImageSize(imagelee2020data)"
        title="Click to show details"
      >
        Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality<a class="anchor" name="lee2020data"></a>
      </h3>
      <div>
        Benjamin Lee, Dave Brown, Bongshin Lee, Christophe Hurter, Steven Drucker, Tim Dwyer
      </div>
      <div>
        VIS (2020) 
        <a href="https://visvar.github.io/pub/lee2020data.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1109/TVCG.2020.3030435" target="_blank">DOI</a>
        <a href="https://ieeexplore.ieee.org/abstract/document/9229242" target="_blank">IEEE</a>
        <a href="../pdf/lee2020data.pdf" target="_blank">PDF</a>
        <a href="https://www.youtube.com/watch?v=XmYNISBjL_Q" target="_blank">video</a>
        
        <span>Honourable Mention Award</span>
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@article{9229242,
    title        = {Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality},
    author       = {Lee, Benjamin and Brown, Dave and Lee, Bongshin and Hurter, Christophe and Drucker, Steven and Dwyer, Tim},
    year         = {2021},
    month        = {Feb},
    journal      = {IEEE Transactions on Visualization and Computer Graphics},
    volume       = {27},
    number       = {2},
    pages        = {1095--1105},
    doi          = {10.1109/TVCG.2020.3030435},
    issn         = {1941-0506},
    abstract     = {A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.}
}
</textarea></div>
      
    </div>
  </div>
  
  
  <div class="paper small" id="paperlee2020shared">
    
      <img
        id="imagelee2020shared"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperlee2020shared', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/lee2020shared.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperlee2020shared', 'small'); toggleImageSize(imagelee2020shared)"
        title="Click to show details"
      >
        Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment<a class="anchor" name="lee2020shared"></a>
      </h3>
      <div>
        Benjamin Lee, Xiaoyun Hu, Maxime Cordeil, Arnaud Prouzeau, Bernhard Jenny, Tim Dwyer
      </div>
      <div>
        VIS (2020) 
        <a href="https://visvar.github.io/pub/lee2020shared.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1109/TVCG.2020.3030450" target="_blank">DOI</a>
        <a href="https://ieeexplore.ieee.org/abstract/document/9222346" target="_blank">IEEE</a>
        <a href="../pdf/lee2020shared.pdf" target="_blank">PDF</a>
        <a href="https://www.youtube.com/watch?v=0ksaAnu9kog" target="_blank">video</a>
        <a href="https://sites.google.com/monash.edu/shared-surfaces-and-spaces" target="_blank">supplemental</a>
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@article{9222346,
    title        = {Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment},
    author       = {Lee, Benjamin and Hu, Xiaoyun and Cordeil, Maxime and Prouzeau, Arnaud and Jenny, Bernhard and Dwyer, Tim},
    year         = {2021},
    month        = {Feb},
    journal      = {IEEE Transactions on Visualization and Computer Graphics},
    volume       = {27},
    number       = {2},
    pages        = {1171--1181},
    doi          = {10.1109/TVCG.2020.3030450},
    issn         = {1941-0506},
    abstract     = {Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.}
}
</textarea></div>
      
    </div>
  </div>
  
  <h2 class="yearHeading">2019</h2>
  <div class="paper small" id="paperlee2019fiesta">
    
      <img
        id="imagelee2019fiesta"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperlee2019fiesta', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        loading="lazy"
        src="../img/small/lee2019fiesta.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperlee2019fiesta', 'small'); toggleImageSize(imagelee2019fiesta)"
        title="Click to show details"
      >
        FIESTA: A Free Roaming Collaborative Immersive Analytics System<a class="anchor" name="lee2019fiesta"></a>
      </h3>
      <div>
        Benjamin Lee, Maxime Cordeil, Arnaud Prouzeau, Tim Dwyer
      </div>
      <div>
        ACM ISS (2019) 
        <a href="https://visvar.github.io/pub/lee2019fiesta.html" target="_blank">direct link</a>
        <a href="https://doi.org/10.1145/3343055.3360746" target="_blank">DOI</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3343055.3360746" target="_blank">ACM</a>
        <a href="../pdf/lee2019fiesta.pdf" target="_blank">PDF</a>
        <a href="https://dl.acm.org/doi/abs/10.1145/3343055.3360746#sec-supp" target="_blank">video</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">We present FIESTA, a prototype system for collaborative immersive analytics (CIA). In contrast to many existing CIA prototypes, FIESTA allows users to collaboratively work together wherever and however they wish---untethered from mandatory physical display devices. Users can freely move around in a shared room-sized environment, author and generate immersive data visualisations, position them in the space around them, and share and communicate their insights to one another. Certain visualisation tasks are also supported to facilitate this process, such as details on demand and brushing and linking.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings{10.1145/3343055.3360746,
    title        = {FIESTA: A Free Roaming Collaborative Immersive Analytics System},
    author       = {Lee, Benjamin and Cordeil, Maxime and Prouzeau, Arnaud and Dwyer, Tim},
    year         = {2019},
    booktitle    = {Proceedings of the 2019 ACM International Conference on Interactive Surfaces and Spaces},
    publisher    = {Association for Computing Machinery},
    series       = {ISS '19},
    pages        = {335--338},
    doi          = {10.1145/3343055.3360746},
    url          = {https://doi.org/10.1145/3343055.3360746},
    abstract     = {We present FIESTA, a prototype system for collaborative immersive analytics (CIA). In contrast to many existing CIA prototypes, FIESTA allows users to collaboratively work together wherever and however they wish---untethered from mandatory physical display devices. Users can freely move around in a shared room-sized environment, author and generate immersive data visualisations, position them in the space around them, and share and communicate their insights to one another. Certain visualisation tasks are also supported to facilitate this process, such as details on demand and brushing and linking.},
    numpages     = {4},
    keywords     = {virtual reality, collaborative immersive analytics, immersive analytics, collaboration, data visualisation}
}
</textarea></div>
      
    </div>
  </div>
  
      </article>
    </div>
  </main>
</body>
</html>