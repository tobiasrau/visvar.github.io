<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ruben Bauer | VISVAR Research Group, University of Stuttgart</title>
  <link rel="stylesheet" href="../style.css">
  <script src="../script.js"></script>
  <link rel="shortcut icon" href="../img/favicon.png">
  <link rel="icon" type="image/png" href="../img/favicon.png" sizes="256x256">
  <link rel="apple-touch-icon" sizes="256x256" href="../img/favicon.png">
</head>
<body>
  <a class="anchor" name="top"></a>
  <main>
    <div>
      
<header>
  <div>
    <a href="https://visvar.github.io/">
      <h1 class="h1desktop"><div>VISVAR</div><div>Research</div><div>Group</div></h1>
      <h1 class="h1mobile">VISVAR</h1>
    </a>
  </div>
  <div>
    <nav>
      <ul>
        <li>
          <a href="https://visvar.github.io/#aboutus">about VISVAR</a>
        </li>
        <li>
          <a href="https://visvar.github.io/#publications">publications</a>
        </li>
        <li class="memberNav">
          <a href="https://visvar.github.io/#members">members</a>
        </li>
        <ul class="memberNav">
          
            <li><a href="https://visvar.github.io/members/michael_sedlmair.html">Michael Sedlmair</a></li>
          
            <li><a href="https://visvar.github.io/members/aimee_sousa_calepso.html">Aimee Sousa Calepso</a></li>
          
            <li><a href="https://visvar.github.io/members/alexander_achberger.html">Alexander Achberger</a></li>
          
            <li><a href="https://visvar.github.io/members/frank_heyen.html">Frank Heyen</a></li>
          
            <li><a href="https://visvar.github.io/members/jonas_haischt.html">Jonas Haischt</a></li>
          
            <li><a href="https://visvar.github.io/members/katrin_angerbauer.html">Katrin Angerbauer</a></li>
          
            <li><a href="https://visvar.github.io/members/melissa_reinelt.html">Melissa Reinelt</a></li>
          
            <li><a href="https://visvar.github.io/members/natalie_hube.html">Natalie Hube</a></li>
          
            <li><a href="https://visvar.github.io/members/nina_doerr.html">Nina DÃ¶rr</a></li>
          
            <li><a href="https://visvar.github.io/members/quynh_quang_ngo.html">Quynh Quang Ngo</a></li>
          
            <li><a href="https://visvar.github.io/members/rene_cutura.html">Rene Cutura</a></li>
          
            <li><a href="https://visvar.github.io/members/ruben_bauer.html">Ruben Bauer</a></li>
          
            <li><a href="https://visvar.github.io/members/sebastian_rigling.html">Sebastian Rigling</a></li>
          
            <li><a href="https://visvar.github.io/members/simeon_rau.html">Simeon Rau</a></li>
          
            <li><a href="https://visvar.github.io/members/markus_wieland.html">Markus Wieland</a></li>
          
            <li><a href="https://visvar.github.io/members/tobias_rau.html">Tobias Rau</a></li>
          
            <li><a href="https://visvar.github.io/members/xingyao_yu.html">Xingyao Yu</a></li>
          
        </ul>
      </ul>
    </nav>
  </div>
</header>
    </div>
    <div>
      <article> <a class="anchor" name="aboutus"></a>
        <h1>
  Ruben Bauer, M.Sc.
</h1>

<div class="aboutMember">

  <div class="avatarAndBio">
    <img class="avatar" src="../img/people/ruben_bauer.jpg" />

    <div class="bio">
      <p>
        I research machine learning- and VR/AR-supported interactive visualization techniques to aid domain
        experts, who work with ensemble simulation or experimental data, in analyzing their data and visualizing
        their findings.
      </p>
    </div>
  </div>

  <h2>Research Interests</h2>
  <ul>
    <li>Visual analytics</li>
    <li>Ensemble data</li>
    <li>Machine learning</li>
    <li>VR/AR</li>
    <li>HCI</li>
  </ul>

  <p>
    <a href="https://www.visus.uni-stuttgart.de/en/institute/team/Bauer-00029/" target="_blank">Institute website</a>
  </p>

  <h2>More</h2>
  <ul>
    <li>
      <a href="https://scholar.google.com/citations?user=vqiuvPMAAAAJ" target="_blank">Google Scholar</a>
    </li>
    <!-- <li>
      <a href="https://orcid.org/" target="_blank">ORCID</a>
    </li> -->
  </ul>

</div>

      </article>
      <article> <a class="anchor" name="publications"></a>
        <h1>Publications</h1>
        
  <h2>2019</h2>
  <div class="paper small" id="paperbruder2019voronoi-based">
    
      <img
        id="imagebruder2019voronoi-based"
        title="Click to enlarge and show details"
        onclick="toggleClass('paperbruder2019voronoi-based', 'small'); toggleImageSize(this)"
        class="publicationImage small"
        src="../img/small/bruder2019voronoi-based.png"
      />
    <div class="metaData ">
      <h3
        onclick="toggleClass('paperbruder2019voronoi-based', 'small'); toggleImageSize(imagebruder2019voronoi-based)"
        title="Click to show details"
      >
        Voronoi-Based Foveated Volume Rendering<a class="anchor" name="bruder2019voronoi-based"></a>
      </h3>
      <div>
        Valentin Bruder, Christoph Schulz, Ruben Bauer, Steffen Frey, Daniel Weiskopf, Thomas Ertl
      </div>
      <div>
        EuroVis (2019) Short Paper
        <a href="https://doi.org/10.2312/evs.20191172" target="_blank">website</a>
        <a href="https://diglib.eg.org/bitstream/handle/10.2312/evs20191172/067-071.pdf?sequence=1&isAllowed=y" target="_blank">PDF</a>
        
        
      </div>
    </div>
    <div class="info">
      <h4>Abstract</h4><div class="abstract">Foveal vision is located in the center of the field of view with a rich impression of detail and color, whereas peripheral vision occurs on the side with more fuzzy and colorless perception. This visual acuity fall-off can be used to achieve higher frame rates by adapting rendering quality to the human visual system. Volume raycasting has unique characteristics, preventing a direct transfer of many traditional foveated rendering techniques. We present an approach that utilizes the visual acuity fall-off to accelerate volume rendering based on Linde-Buzo-Gray sampling and natural neighbor interpolation. First, we measure gaze using a stationary 1200 Hz eye-tracking system. Then, we adapt our sampling and reconstruction strategy to that gaze. Finally, we apply a temporal smoothing filter to attenuate undersampling artifacts since peripheral vision is particularly sensitive to contrast changes and movement. Our approach substantially improves rendering performance with barely perceptible changes in visual quality. We demonstrate the usefulness of our approach through performance measurements on various data sets.</div>
      <h4>BibTex</h4><div class="bibtex"><textarea>@inproceedings {10.2312:evs.20191172,
booktitle = {EuroVis 2019 - Short Papers},
editor = {Johansson, Jimmy and Sadlo, Filip and Marai, G. Elisabeta},
title = {{Voronoi-Based Foveated Volume Rendering}},
author = {Bruder, Valentin and Schulz, Christoph and Bauer, Ruben and Frey, Steffen and Weiskopf, Daniel and Ertl, Thomas},
year = {2019},
publisher = {The Eurographics Association},
ISBN = {978-3-03868-090-1},
DOI = {10.2312/evs.20191172}
}</textarea></div>
      
      <div>
        <a href="https://visvar.github.io/pub/bruder2019voronoi-based.html" target="_blank">direct link</a>
      </div>
    </div>
  </div>
  
      </article>
    </div>
  </main>
</body>
</html>