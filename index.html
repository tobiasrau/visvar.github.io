<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VISVAR Research Group, University of Stuttgart</title>
    <link rel="stylesheet" href="./style.css">
    <script src="./script.js"></script>
    <link rel="shortcut icon" href="./img/favicon.png">
    <link rel="icon" type="image/png" href="./img/favicon.png" sizes="256x256">
    <link rel="apple-touch-icon" sizes="256x256" href="./img/favicon.png">
</head>
<body>
    <a class="anchor" name="top"></a>
    <main>
        <div>
            
<header>
    <div>
        <a href="https://visvar.github.io/">
            <h1 class="h1desktop">
                <div>
                    VISVAR
                </div>
                <div>
                    Research
                </div>
                <div>
                    Group
                </div>
            </h1>
            <h1 class="h1mobile">
                VISVAR
            </h1>
        </a>
    </div>
    <div>
        <nav>
            <ul>
                <li>
                    <a href="https://visvar.github.io/#aboutus">about VISVAR</a>
                </li>
                <li>
                    <a href="https://visvar.github.io/#publications">publications</a>
                </li>
                <li class="memberNav">
                    <a href="https://visvar.github.io/#members">members</a>
                </li>
                <ul class="memberNav">
                    
                        <li>
                            <a href="https://visvar.github.io/members/aimee_sousa_calepso.html">
                                Aimee Sousa Calepso
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/alexander_achberger.html">
                                Alexander Achberger
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/frank_heyen.html">
                                Frank Heyen
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/katrin_angerbauer.html">
                                Katrin Angerbauer
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/melissa_reinelt.html">
                                Melissa Reinelt
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/michael_sedlmair.html">
                                Michael Sedlmair
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/natalie_hube.html">
                                Natalie Hube
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/quynh_ngo.html">
                                Quynh Ngo
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/rene_cutura.html">
                                Rene Cutura
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/ruben_bauer.html">
                                Ruben Bauer
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/sebastian_rigling.html">
                                Sebastian Rigling
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/simeon_rau.html">
                                Simeon Rau
                            </a>
                        </li>
                    
                        <li>
                            <a href="https://visvar.github.io/members/xingyao_yu.html">
                                Xingyao Yu
                            </a>
                        </li>
                    
                </ul>
            </ul>
        </nav>
    </div>
</header>
        </div>
        <div>
            <article> <a class="anchor" name="aboutus"></a>
                <h1>About VISVAR</h1>

<div>
    <div class="aboutusTextAndLogo">
        <div>
            <p>
                Our research covers a diverse set of topics related to visualization, human-computer interaction (HCI),
                and virtual and augmented reality (VR/AR).
                The VISVAR group was founded in 2019 and is steadily growing in members and publications every since.
            </p>
            <p>
                <a href="https://www.visus.uni-stuttgart.de/en/institute/workinggroups/sedlmair-group/" target="_blank">
                    Official group website of the University of Stuttgart
                </a>
            </p>
            <p>
                <a href="https://observablehq.com/@fheyen/visvar-publications" target="_blank">
                    A visual overview of publications and co-authorships
                </a>
            </p>
            <p>
                Director:
                <a href="members/michael_sedlmair.html">
                    Prof. Dr. Michael Sedlmair
                </a>
                <br />
                Address:
                <a href="https://www.openstreetmap.org/way/97945756" target="_blank">
                    Visualization Research Center (VISUS), Allmandring 19, 70569 Stuttgart
                </a>
            </p>

        </div>
        <div>
            <img src="img/visvar_logo.svg" />
        </div>
    </div>

    <h2>Emphasis</h2>
    <ul>
        <li>Virtual and augmented reality (VR/AR)</li>
        <li>Human-machine and human-data interaction (HCI)</li>
        <li>Basics of perception and cognition</li>
        <li>Interactive visualization of data</li>
    </ul>

    <h2>Teaching</h2>
    <p>
        Together with the other groups of the institute, we contribute teaching to the bachelor and master programs in
        computer science modules related to socio-cognitive systems.
        See our
        <a href="https://www.vis.uni-stuttgart.de/lehre/index.en.html" target="_blank">
            university website
        </a>
        for more informaiton on teaching.
    </p>

    <h2>Projects</h2>
    <ul>
        <li>
            <a href="https://www.sfbtrr161.de/" target="_blank">
                SFB-TRR 161
            </a>
        </li>
        <li>
            <a href="https://www.simtech.uni-stuttgart.de/" target="_blank">
                EXC SimTech
            </a>
        </li>
        <li>
            <a href="https://www.intcdc.uni-stuttgart.de/" target="_blank">
                EXC IntCDC
            </a>
        </li>
        <li>
            <a href="http://www.viscipub.com/" target="_blank">
                FFG ViSciPub
            </a>
        </li>
        <li>
            <a href="https://www.visus.uni-stuttgart.de/en/projects/cvrf-instrudata/" target="_blank">
                CyberValley - InstruData
            </a>
        </li>
    </ul>

    <h2>Research topics</h2>
    <p>
        Our research in the area of Virtual Reality and Augmented Reality (VR/AR) focuses on:
    </p>
    <ol>
        <li>Immersive analytics</li>
        <li>Novel interaction methods for VR/AR.</li>
    </ol>
    <p>
        In terms of immersive analytics, we focus on the question as to when VR/AR is really needed for analyzing and
        visualizing data.
        For interaction, we specifically explore novel ways of how VR/AR might offer more natural ways to interact with
        data.
    </p>
    <p>
        There is a close cooperation with the working groups of the
        <a href="https://www.visus.uni-stuttgart.de/en/" target="_blank">Visualization Research Center (VISUS)</a>
        and the other departments of
        <a href="https://www.vis.uni-stuttgart.de/en/" target="_blank">VIS</a>.
    </p>

</div>

            </article>
            <article> <a class="anchor" name="members"></a>
                <h1>Members</h1>
                <div class="memberList">
                    
    <div>
        <a href="./members/aimee_sousa_calepso.html">
            <img
                class="avatar"
                src="./img/people/small/aimee_sousa_calepso.jpg"
            />
            <div>
                Aimee Sousa Calepso
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/alexander_achberger.html">
            <img
                class="avatar"
                src="./img/people/small/alexander_achberger.jpg"
            />
            <div>
                Alexander Achberger
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/frank_heyen.html">
            <img
                class="avatar"
                src="./img/people/small/frank_heyen.jpg"
            />
            <div>
                Frank Heyen
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/katrin_angerbauer.html">
            <img
                class="avatar"
                src="./img/people/small/katrin_angerbauer.jpg"
            />
            <div>
                Katrin Angerbauer
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/melissa_reinelt.html">
            <img
                class="avatar"
                src="./img/people/small/melissa_reinelt.jpg"
            />
            <div>
                Melissa Reinelt
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/michael_sedlmair.html">
            <img
                class="avatar"
                src="./img/people/small/michael_sedlmair.jpg"
            />
            <div>
                Michael Sedlmair
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/natalie_hube.html">
            <img
                class="avatar"
                src="./img/people/small/natalie_hube.jpg"
            />
            <div>
                Natalie Hube
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/quynh_ngo.html">
            <img
                class="avatar"
                src="./img/people/small/quynh_ngo.jpg"
            />
            <div>
                Quynh Ngo
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/rene_cutura.html">
            <img
                class="avatar"
                src="./img/people/small/rene_cutura.jpg"
            />
            <div>
                Rene Cutura
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/ruben_bauer.html">
            <img
                class="avatar"
                src="./img/people/small/ruben_bauer.jpg"
            />
            <div>
                Ruben Bauer
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/sebastian_rigling.html">
            <img
                class="avatar"
                src="./img/people/small/sebastian_rigling.jpg"
            />
            <div>
                Sebastian Rigling
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/simeon_rau.html">
            <img
                class="avatar"
                src="./img/people/small/simeon_rau.jpg"
            />
            <div>
                Simeon Rau
            </div>
        </a>
    </div>
    

    <div>
        <a href="./members/xingyao_yu.html">
            <img
                class="avatar"
                src="./img/people/small/xingyao_yu.jpg"
            />
            <div>
                Xingyao Yu
            </div>
        </a>
    </div>
    
                </div>
            </article>
            <article> <a class="anchor" name="publications"></a>
                <h1>Publications</h1>
                
    
    <h2>
        2021
    </h2>
    
    <div
        class="paper small"
        id="paperrau2021visual"
    >
        
            <img
                id="imagerau2021visual"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperrau2021visual', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/rau2021visual.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperrau2021visual', 'small'); toggleImageSize(imagerau2021visual);"
                title="Click to show details"
            >
                Visual Support for Human-AI Co-Composition
            </h3>  <a class="anchor" name="rau2021visual"></a>
            <div class="authors">
                <span class="firstAuthor">Simeon Rau</span>,
                Frank Heyen, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMIR 2021</span>
                <span class="publication">Late-Breaking Demo Poster</span>
                <a href="./pdf/rau2021visual.pdf" target="_blank">PDF</a>
                <a href="https://archives.ismir.net/ismir2021/latebreaking/000014.pdf" target="_blank">website</a>
                <a href="./video/rau2021visual.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose a visual approach for AI-assisted music composition, where the user interactively generates, selects, and adapts short melodies. Based on an entered start melody, we automatically generate multiple continuation samples. Repeating this step and in turn generating continuations for these samples results in a tree or graph of melodies. We visualize this structure with two visualizations, where nodes display the piano roll of the corresponding sample. By interacting with these visualizations, the user can quickly listen to, choose, and adapt melodies, to iteratively create a composition. A third visualization provides an overview over larger numbers of samples, allowing for insights into the AI's predictions and the sample space.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{rau2021visual,
  title={Visual Support for Human-{AI} Co-Composition},
  author={Rau, Simeon and Heyen, Frank and Sedlmair, Michael},
  year={2021},
  booktitle={Extended Abstracts for the Late-Breaking Demo Session of the 22nd Int. Society for Music Information Retrieval Conf. (ISMIR)},
  url={https://archives.ismir.net/ismir2021/latebreaking/000014.pdf}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was funded by the Cyber Valley Research Fund – Project InstruData.
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2021ismar"
    >
        
            <img
                id="imagehube2021ismar"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperhube2021ismar', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/hube2021ismar.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperhube2021ismar', 'small'); toggleImageSize(imagehube2021ismar);"
                title="Click to show details"
            >
                VR Collaboration in Large Companies: An Interview Study on the Role of Avatars
            </h3>  <a class="anchor" name="hube2021ismar"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Katrin Angerbauer, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2021</span>
                <span class="publication">Short Paper</span>
                <a href="./pdf/hube2021ismar.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Collaboration is essential in companies and often physical presence is required, thus, more and more Virtual Reality (VR) systems are used to work together remotely. To support social interaction, human representations in form of avatars are used in collaborative virtual environment (CVE) tools. However, up to now, the avatar representations often are limited in their design and functionality, which may hinder effective collaboration. In our interview study, we explored the status quo of VR collaboration in a large automotive company setting with a special focus on the role of avatars. We collected inter-view data from 21 participants, from which we identified challenges of current avatar representations used in our setting. Based on these findings, we discuss design suggestions for avatars in a company setting, which aim to improve social interaction. As opposed to state-of-the-art research, we found that users within the context of a large automotive company have an altered need with respect to avatar representations.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{hube2021ismar,
	Author = {Natalie Hube and Katrin Angerbauer and Daniel Pohlandt and Kresimir Vidackovic and Michael Sedlmair},
	Title = {VR Collaboration in Large Companies: An Interview Study on the Role of Avatars},
	Booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct, Short paper)},
	pages = {139--144},
	url = {https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00037},
  	doi = {10.1109/ISMAR-Adjunct54149.2021.00037},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2021visap"
    >
        
            <img
                id="imagecutura2021visap"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2021visap', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/cutura2021visap.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2021visap', 'small'); toggleImageSize(imagecutura2021visap);"
                title="Click to show details"
            >
                DaRt: Generative Art using Dimensionality Reduction Algorithms
            </h3>  <a class="anchor" name="cutura2021visap"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Kathrin Angerbauer, Frank Heyen, Natalie Hube, Michael Sedlmair
            </div>
            <div>
                <span class="publication">VIS 2021</span>
                <span class="publication">Pictorial</span>
                <a href="./pdf/cutura2021visap.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/VISAP52981.2021.00013" target="_blank">website</a>
                <a href="https://youtu.be/pOcksJOiAPw" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Dimensionality Reduction (DR) is a popular technique that is often used in Machine Learning and Visualization communities to analyze high-dimensional data. The approach is empirically proven to be powerful for uncovering previously unseen structures in the data. While observing the results of the intermediate optimization steps of DR algorithms, we coincidently discovered the artistic beauty of the DR process. With enthusiasm for the beauty, we decided to look at DR from a generative art lens rather than their technical application aspects and use DR techniques to create artwork. Particularly, we use the optimization process to generate images, by drawing each intermediate step of the optimization process with some opacity over the previous intermediate result. As another alternative input, we used a neural-network model for face-landmark detection, to apply DR to portraits, while maintaining some facial properties, resulting in abstracted facial avatars. In this work, we provide such a collection of such artwork.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2021dart,
  title={{DaRt}: Generative Art using Dimensionality Reduction Algorithms},
  author={Cutura, Rene and Angerbauer, Katrin and Heyen, Frank and Hube, Natalie and Sedlmair, Michael},
  booktitle={2021 IEEE VIS Arts Program (VISAP)},
  pages={59--72},
  year={2021},
  organization={IEEE},
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - Project-ID 251654672 - TRR 161
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papergrossmann2021vis"
    >
        
            <img
                id="imagegrossmann2021vis"
                title="Click to enlarge and show details"
                onclick="toggleClass('papergrossmann2021vis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/grossmann2021vis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papergrossmann2021vis', 'small'); toggleImageSize(imagegrossmann2021vis);"
                title="Click to show details"
            >
                Does the Layout Really Matter? A Study on Visual Model Accuracy Estimation
            </h3>  <a class="anchor" name="grossmann2021vis"></a>
            <div class="authors">
                <span class="firstAuthor">Nicolas Grossmann</span>,
                Jürgen Bernard, Michael Sedlmair, Manuela Waldner
            </div>
            <div>
                <span class="publication">VIS 2021</span>
                <span class="publication">Short Paper</span>
                <a href="https://arxiv.org/abs/2110.07188" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/VIS49827.2021.9623326" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In visual interactive labeling, users iteratively assign labels to data items until the machine model reaches an acceptable accuracy. A crucial step of this process is to inspect the model’s accuracy and decide whether it is necessary to label additional elements. In scenarios with no or very little labeled data, visual inspection of the predictions is required. Similarity-preserving scatterplots created through a dimensionality reduction algorithm are a common visualization that is used in these cases. Previous studies investigated the effects of layout and image complexity on tasks like labeling. However, model evaluation has not been studied systematically. We present the results of an experiment studying the influence of image complexity and visual grouping of images on model accuracy estimation. We found that users outperform traditional automated approaches when estimating a model’s accuracy. Furthermore, while the complexity of images impacts the overall performance, the layout of the items in the plot has little to no effect on estimations.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{grossmann2021vis,
	Author = {Nicolas Grossmann and J{\"u}rgen Bernard and Michael Sedlmair and Manuela Waldner},
	Title = {Does the Layout Really Matter? A Study on Visual Model Accuracy Estimation},
	Booktitle = {IEEE Visualization Conference  (VIS, Short Paper)},
	pages = {61--65},
	url = {https://arxiv.org/abs/2110.07188},
  	doi = {10.1109/VIS49827.2021.9623326},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperachberger2021uist"
    >
        
            <img
                id="imageachberger2021uist"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperachberger2021uist', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/achberger2021uist.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperachberger2021uist', 'small'); toggleImageSize(imageachberger2021uist);"
                title="Click to show details"
            >
                Strive: String-Based Force Feedback for Automotive Engineering
            </h3>  <a class="anchor" name="achberger2021uist"></a>
            <div class="authors">
                <span class="firstAuthor">Alexander Achberger</span>,
                Fabian Aust, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair
            </div>
            <div>
                <span class="publication">UIST 2021</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/achberger2021uist.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3472749.3474790" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The large potential of force feedback devices for interacting in Virtual Reality (VR) has been illustrated in a plethora of research prototypes. Yet, these devices are still rarely used in practice and it remains an open challenge how to move this research into practice. To that end, we contribute a participatory design study on the use of haptic feedback devices in the automotive industry. Based on a 10-month observing process with 13 engineers, we developed STRIVE, a string-based haptic feedback device. In addition to the design of STRIVE, this process led to a set of requirements for introducing haptic devices into industrial settings, which center around a need for flexibility regarding forces, comfort, and mobility. We evaluated STRIVE with 16 engineers in five different day-to-day automotive VR use cases. The main results show an increased level of trust and perceived safety as well as further challenges towards moving haptics research into practice. 
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{achberger2021uist,
	Author = {Alexander Achberger and Fabian Aust and Daniel Pohlandt and Kresimir Vidackovic and Michael Sedlmair},
	Title = {{STRIVE}: String-Based Force Feedback for Automotive Engineering},
	Booktitle = {ACM Symposium on User Interface Software and Technology (UIST)},
	pages = {841--853},
	url = {https://doi.org/10.1145/3472749.3474790},
  	doi = {10.1145/3472749.3474790},
	Year = {2021}
} </textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2021vr"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperhube2021vr', 'small'); toggleImageSize(imagehube2021vr);"
                title="Click to show details"
            >
                VR Collaboration in Large Companies: An Interview Study on the Role of Avatars
            </h3>  <a class="anchor" name="hube2021vr"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Katrin Angerbauer, Daniel Pohlandt, Kresimir Vidackovic, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2021</span>
                <span class="publication">Poster / Short Paper</span>
                
                <a href="" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Collaboration is essential in companies and often physical presence is required, thus, more and more Virtual Reality (VR) systems are used to work together remotely. To support social interaction, human representations in form of avatars are used in collaborative virtual environment (CVE) tools. However, up to now, the avatar representations often are limited in their design and functionality, which may hinder effective collaboration. In our interview study, we explored the status quo of VR collaboration in a large automotive company setting with a special focus on the role of avatars. We collected interview data from 21 participants, from which we identified challenges of current avatar representations used in our setting. Based on these findings, we discuss design suggestions for avatars in a company setting, which aim to improve social interaction. As opposed to state-of-the-art research, we found that users within the context of a large automotive company have an altered need with respect to avatar representations.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papereirich2021vis"
    >
        
            <img
                id="imageeirich2021vis"
                title="Click to enlarge and show details"
                onclick="toggleClass('papereirich2021vis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/eirich2021vis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papereirich2021vis', 'small'); toggleImageSize(imageeirich2021vis);"
                title="Click to show details"
            >
                IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines
            </h3>  <a class="anchor" name="eirich2021vis"></a>
            <div class="authors">
                <span class="firstAuthor">Joscha Eirich</span>,
                Jakob Bonart, Dominik Jäckle, Michael Sedlmair, Ute Schmid, Kai Fischbach, Tobias Schreck, Jürgen Bernard
            </div>
            <div>
                <span class="publication">TVCG 2021</span>
                <span class="publication">Full paper</span>
                <a href="./pdf/eirich2021vis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2021.3114797" target="_blank">website</a>
                <a href="./video/eirich2021vis.mp4" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this design study, we present IRVINE, a Visual Analytics (VA) system, which facilitates the analysis of acoustic data to detect and understand previously unknown errors in the manufacturing of electrical engines. In serial manufacturing processes, signatures from acoustic data provide valuable information on how the relationship between multiple produced engines serves to detect and understand previously unknown errors. To analyze such signatures, IRVINE leverages interactive clustering and data labeling techniques, allowing users to analyze clusters of engines with similar signatures, drill down to groups of engines, and select an engine of interest. Furthermore, IRVINE allows to assign labels to engines and clusters and annotate the cause of an error in the acoustic raw measurement of an engine. Since labels and annotations represent valuable knowledge, they are conserved in a knowledge database to be available for other stakeholders. We contribute a design study, where we developed IRVINE in four main iterations with engineers from a company in the automotive sector. To validate IRVINE, we conducted a field study with six domain experts. Our results suggest a high usability and usefulness of IRVINE as part of the improvement of a real-world manufacturing process. Specifically, with IRVINE domain experts were able to label and annotate produced electrical engines more than 30% faster.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{eirich2021vis,
	author = {Joscha Eirich and Jakob Bonart and Dominik J{\"a}ckle and Michael Sedlmair and Ute Schmid and Kai Fischbach and Tobias Schreck and J{\"u}rgen Bernard},
	title = {{IRVINE}: A Design Study on Analyzing Correlation Patterns of Electrical Engines},
	journal = {IEEE Trans. Visualization and Computer Graphics (TVCG, Proc. VIS 2021)},
	note = {To appear. Best paper award},
	url = {https://doi.org/10.1109/TVCG.2021.3114797},
  	doi = {10.1109/TVCG.2021.3114797},
	year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2021vinci"
    >
        
            <img
                id="imagecutura2021vinci"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2021vinci', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/cutura2021vinci.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2021vinci', 'small'); toggleImageSize(imagecutura2021vinci);"
                title="Click to show details"
            >
                Hagrid — Gridify Scatterplots with Hilbert and Gosper Curves
            </h3>  <a class="anchor" name="cutura2021vinci"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                
Cristina Morariu, Zhanglin Cheng, Yunhai Wang, Daniel Weiskopf, Michael Sedlmair
            </div>
            <div>
                <span class="publication">VINCI 2021</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/cutura2021vinci.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3481549.3481569" target="_blank">website</a>
                <a href="https://youtu.be/E_XP31_JzGY" target="_blank">video</a>
                <a href="https://renecutura.eu/pdfs/hagrid_supplemental.pdf" target="_blank">more...</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                A common enhancement of scatterplots represents points as small multiples, glyphs, or thumbnail images. As this encoding often results in overlaps, a general strategy is to alter the position of the data points, for instance, to a grid-like structure. Previous approaches rely on solving expensive optimization problems or on dividing the space that alter the global structure of the scatterplot. To find a good balance between efficiency and neighborhood and layout preservation, we propose Hagrid, a technique that uses space-filling curves (SFCs) to “gridify” a scatterplot without employing expensive collision detection and handling mechanisms. Using SFCs ensures that the points are plotted close to their original position, retaining approximately the same global structure. The resulting scatterplot is mapped onto a rectangular or hexagonal grid, using Hilbert and Gosper curves. We discuss and evaluate the theoretic runtime of our approach and quantitatively compare our approach to three state-of-the-art gridifying approaches, DGrid, Small multiples with gaps SMWG, and CorrelatedMultiples CMDS, in an evaluation comprising 339 scatterplots. Here, we compute several quality measures for neighborhood preservation together with an analysis of the actual runtimes. The main results show that, compared to the best other technique, Hagrid is faster by a factor of four, while achieving similar or even better quality of the gridified layout. Due to its computational efficiency, our approach also allows novel applications of gridifying approaches in interactive settings, such as removing local overlap upon hovering over a scatterplot.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2021hagrid,
	author = {Cutura, Rene and Morariu, Cristina and Cheng, Zhanglin and Wang, Yunhai and Weiskopf, Daniel and Sedlmair, Michael},
	title = {{Hagrid -- Gridify Scatterplots with Hilbert and Gosper Curves}},
	year = {2021},
	isbn = {9781450386470},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3481549.3481569},
	doi = {10.1145/3481549.3481569},
	booktitle = {The 14th International Symposium on Visual Information Communication and Interaction},
	articleno = {1},
	numpages = {8},
	keywords = {Grid layout, Neighborhood-preserving., Space-filling curve},
	location = {Potsdam, Germany},
	series = {VINCI 2021}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was supported by the BMK FFG ICT of the Future program via the ViSciPub project (no. 867378), and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161.
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperachberger2021vinci"
    >
        
            <img
                id="imageachberger2021vinci"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperachberger2021vinci', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/achberger2021vinci.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperachberger2021vinci', 'small'); toggleImageSize(imageachberger2021vinci);"
                title="Click to show details"
            >
                PropellerHand: Hand-Mounted, Propeller-Based Force Feedback Device
            </h3>  <a class="anchor" name="achberger2021vinci"></a>
            <div class="authors">
                <span class="firstAuthor">Alexander Achberger</span>,
                Frank Heyen, Kresimir Vidackovic, Michael Sedlmair
            </div>
            <div>
                <span class="publication">VINCI 2021</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/achberger2021vinci.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3481549.3481563" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Immersive analytics is a fast growing field that is often applied in virtual reality (VR). VR environments often lack immersion due to missing sensory feedback when interacting with data. Existing haptic devices are often expensive, stationary, or occupy the user’s hand, preventing them from grasping objects or using a controller. We propose PropellerHand, an ungrounded hand-mounted haptic device with two rotatable propellers, that allows exerting forces on the hand without obstructing hand use. PropellerHand is able to simulate feedback such as weight and torque by generating thrust up to 11 N in 2-DOF and a torque of 1.87 Nm in 2-DOF. Its design builds on our experience from quantitative and qualitative experiments with different form factors and parts. We evaluated our final version through a qualitative user study in various VR scenarios that required participants to manipulate virtual objects in different ways, while changing between torques and directional forces. Results show that PropellerHand improves users’ immersion in virtual reality.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{achberger2021vinci,
  author = {Alexander Achberger and Frank Heyen and Kresimir Vidackovic and Michael Sedlmair},
  title = {PropellerHand: {A} Hand-Mounted, Propeller-Based Force Feedback Device},
  booktitle = {International Symposium on Visual Information Communication and Interaction (VINCI)},
  pages     = {4:1--4:8},
  publisher = {ACM},
  year      = {2021},
  url       = {https://doi.org/10.1145/3481549.3481563},
  doi       = {10.1145/3481549.3481563}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkrauter2021muc"
    >
        
            <img
                id="imagekrauter2021muc"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperkrauter2021muc', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/krauter2021muc.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperkrauter2021muc', 'small'); toggleImageSize(imagekrauter2021muc);"
                title="Click to show details"
            >
                Don’t Catch It: An Interactive Virtual-Reality Environment to Learn About COVID-19 Measures Using Gamification Elements
            </h3>  <a class="anchor" name="krauter2021muc"></a>
            <div class="authors">
                <span class="firstAuthor">Christian Krauter</span>,
                Jonas Vogelsang, Aimee Sousa Calepso, Katrin Angerbauer, Michael Sedlmair
            </div>
            <div>
                <span class="publication">MuC 2021</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/krauter2021muc.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3473856.3474031" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The world is still under the influence of the COVID-19 pandemic. Even though vaccines are deployed as rapidly as possible, it is still necessary to use other measures to reduce the spread of the virus. Measures such as social distancing or wearing a mask receive a lot of criticism. Therefore, we want to demonstrate a serious game to help the players understand these measures better and show them why they are still necessary. The player of the game has to avoid other agents to keep their risk of a COVID-19 infection low. The game uses Virtual Reality through a Head-Mounted-Display to deliver an immersive and enjoyable experience. Gamification elements are used to engage the user with the game while they explore various environments. We also implemented visualizations that help the user with social distancing.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{krauter2021muc,
	Author = {Christian Krauter and Jonas Vogelsang and Aimee Sousa Calepso and Katrin Angerbauer and Michael Sedlmair},
	Title = {Don’t Catch It: An Interactive Virtual-Reality Environment to Learn About {COVID-19} Measures Using Gamification Elements},
	Booktitle = {Mensch und Computer},
	publisher = {ACM},
	pages = {593--596},
	url = {https://doi.org/10.1145/3473856.3474031},
  	doi = {10.1145/3473856.3474031},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperrijken2021illegible"
    >
        
            <img
                id="imagerijken2021illegible"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperrijken2021illegible', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/rijken2021illegible.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperrijken2021illegible', 'small'); toggleImageSize(imagerijken2021illegible);"
                title="Click to show details"
            >
                Illegible Semantics: Exploring the Design Space of Metal Logos
            </h3>  <a class="anchor" name="rijken2021illegible"></a>
            <div class="authors">
                <span class="firstAuthor">Gerrit J. Rijken</span>,
                Rene Cutura, Frank Heyen, Michael Sedlmair, Michael Correll, Jason Dykes, Noeska Smit
            </div>
            <div>
                <span class="publication">alt.VIS 2021</span>
                <span class="publication">Workshop Paper</span>
                <a href="https://arxiv.org/ftp/arxiv/papers/2109/2109.01688.pdf" target="_blank">PDF</a>
                <a href="https://arxiv.org/abs/2109.01688" target="_blank">website</a>
                <a href="https://www.youtube.com/watch?v=BZOdIhU-mrA" target="_blank">video</a>
                <a href="http://illegiblesemantics.com" target="_blank">more...</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The logos of metal bands can be by turns gaudy, uncouth, or nearly illegible. Yet, these logos work: they communicate sophisticated notions of genre and emotional affect. In this paper we use the design considerations of metal logos to explore the space of “illegible semantics”: the ways that text can communicate information at the cost of readability, which is not always the most important objective. In this work, drawing on formative visualization theory, professional design expertise, and empirical assessments of a corpus ofmetal band logos, we describe a design space of metal logos and present a tool through which logo characteristics can be explored through visualization. We investigate ways in which logo designers imbue their text with meaning and consider opportunities and implications for visualization more widely.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{rijken2021altvis,
	Author = {Gerrit J Rijken and Rene Cutura and Frank Heyen and Michael Sedlmair and Michael Correll and Jason Dykes and Noeska Smit},
	Title = {Illegible Semantics: Exploring the Design Space of Metal Logos},
	Booktitle = {{IEEE VIS} alt.VIS Workshop},
	url = {https://arxiv.org/abs/2109.01688},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2021tiis"
    >
        
            <img
                id="imagebernard2021tiis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2021tiis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/bernard2021tiis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2021tiis', 'small'); toggleImageSize(imagebernard2021tiis);"
                title="Click to show details"
            >
                A Taxonomy of Property Measures to Unify Active Learning and Human-centered Approaches to Data Labeling
            </h3>  <a class="anchor" name="bernard2021tiis"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                 Marco Hutter, Michael Sedlmair, Matthias Zeppelzauer, Tamara Munzner
            </div>
            <div>
                <span class="publication">TiiS 2021</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/bernard2021tiis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3439333" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Strategies for selecting the next data instance to label, in service of generating labeled data for machine learning, have been considered separately in the machine learning literature on active learning and in the visual analytics literature on human-centered approaches. We propose a unified design space for instance selection strategies to support detailed and fine-grained analysis covering both of these perspectives. We identify a concise set of 15 properties, namely measureable characteristics of datasets or of machine learning models applied to them, that cover most of the strategies in these literatures. To quantify these properties, we introduce Property Measures (PM) as fine-grained building blocks that can be used to formalize instance selection strategies. In addition, we present a taxonomy of PMs to support the description, evaluation, and generation of PMs across four dimensions: machine learning (ML) Model Output, Instance Relations, Measure Functionality, and Measure Valence. We also create computational infrastructure to support qualitative visual data analysis: a visual analytics explainer for PMs built around an implementation of PMs using cascades of eight atomic functions. It supports eight analysis tasks, covering the analysis of datasets and ML models using visual comparison within and between PMs and groups of PMs, and over time during the interactive labeling process. We iteratively refined the PM taxonomy, the explainer, and the task abstraction in parallel with each other during a two-year formative process, and show evidence of their utility through a summative evaluation with the same infrastructure. This research builds a formal baseline for the better understanding of the commonalities and differences of instance selection strategies, which can serve as the stepping stone for the synthesis of novel strategies in future work.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{bernard2021tiis,
	author = {J{\"u}rgen Bernard and Marco Hutter and Michael Sedlmair and Matthias Zeppelzauer and Tamara Munzner},
	title = {A Taxonomy of Property Measures to Unify Active Learning and Human-centered Approaches to Data Labeling},
	journal = {ACM Transactions on Interactive Intelligent Systems (TiiS)},
	volume={11},
    number={3-4},
    pages={1--42},
	url = {https://doi.org/10.1145/3439333},
  	doi = {10.1145/3439333},
	year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkraus2021cga"
    >
        
            <img
                id="imagekraus2021cga"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperkraus2021cga', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/kraus2021cga.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperkraus2021cga', 'small'); toggleImageSize(imagekraus2021cga);"
                title="Click to show details"
            >
                The Value of Immersive Visualization
            </h3>  <a class="anchor" name="kraus2021cga"></a>
            <div class="authors">
                <span class="firstAuthor">Matthias Kraus</span>,
                Karsten Klein, Johannes Fuchs, Daniel A Keim, Falk Schreiber, Michael Sedlmair
            </div>
            <div>
                <span class="publication">CG&A 2021</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/kraus2021cga.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/MCG.2021.3075258" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In recent years, research on immersive environments has experienced a new wave of interest, and immersive analytics has been established as a new research field. Every year, a vast amount of different techniques, applications, and user studies are published that focus on employing immersive environments for visualizing and analyzing data. Nevertheless, immersive analytics is still a relatively unexplored field that needs more basic research in many aspects and is still viewed with skepticism. Rightly so, because in our opinion, many researchers do not fully exploit the possibilities offered by immersive environments and, on the contrary, sometimes even overestimate the power of immersive visualizations. Although a growing body of papers has demonstrated individual advantages of immersive analytics for specific tasks and problems, the general benefit of using immersive environments for effective analytic tasks remains controversial. In this article, we reflect on when and how immersion may be appropriate for the analysis and present four guiding scenarios. We report on our experiences, discuss the landscape of assessment strategies, and point out the directions where we believe immersive visualizations have the greatest potential.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{kraus2021cga,
	author = {Matthias Kraus and Karsten Klein and Johannes Fuchs and Daniel A Keim and Falk Schreiber and Michael Sedlmair},
	title = {The Value of Immersive Visualization},
    journal={IEEE Computer Graphics and Applications (CG\&A)},
	volume={41},
    number={4},
    pages={125-132},
	url = {https://doi.org/10.1109/MCG.2021.3075258},
  	doi = {10.1109/MCG.2021.3075258},
	year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperabbas2021arxiv"
    >
        
            <img
                id="imageabbas2021arxiv"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperabbas2021arxiv', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/abbas2021arxiv.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperabbas2021arxiv', 'small'); toggleImageSize(imageabbas2021arxiv);"
                title="Click to show details"
            >
                ClustRank: A Visual Quality Measure Trained on Perceptual Data for Sorting Scatterplots by Cluster Patterns
            </h3>  <a class="anchor" name="abbas2021arxiv"></a>
            <div class="authors">
                <span class="firstAuthor">Mostafa Abbas</span>,
                Ehsan Ullah, Abdelkader Baggag, Halima Bensmail, Michael Sedlmair, Michael Aupetit
            </div>
            <div>
                <span class="publication"> 2021</span>
                <span class="publication">Technical Report</span>
                <a href="https://arxiv.org/pdf/2106.00599.pdf" target="_blank">PDF</a>
                <a href="https://arxiv.org/abs/2106.00599" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Visual quality measures (VQMs) are designed to support analysts by automatically detecting and quantifying patterns in visualizations. We propose a new data-driven technique called ClustRank that allows to rank scatterplots according to visible grouping patterns. Our model first encodes scatterplots in the parametric space of a Gaussian Mixture Model, and then uses a classifier trained on human judgment data to estimate the perceptual complexity of grouping patterns. The numbers of initial mixture components and final combined groups determine the rank of the scatterplot. ClustRank improves on existing VQM techniques by mimicking human judgments on two-Gaussian cluster patterns and gives more accuracy when ranking general cluster patterns in scatterplots. We demonstrate its benefit by analyzing kinship data for genome-wide association studies, a domain in which experts rely on the visual analysis of large sets of scatterplots. We make the three benchmark datasets and the ClustRank VQM available for practical use and further improvements. 
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@techreport{abbas2021arxiv,
	title = {{ClustRank}: A Visual Quality Measure Trained on Perceptual Data for Sorting Scatterplots by Cluster Patterns},
	author = {Mostafa Abbas and Ehsan Ullah and Abdelkader Baggag and Halima Bensmail and Michael Sedlmair and Michael Aupetit},
	Institution = {{arXiv} preprint},
	Number = {arXiv:2106.00599},
	Type = {Technical Report},
	url = {https://arxiv.org/pdf/2106.00599.pdf},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperling2021icdar"
    >
        
            <img
                id="imageling2021icdar"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperling2021icdar', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/ling2021icdar.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperling2021icdar', 'small'); toggleImageSize(imageling2021icdar);"
                title="Click to show details"
            >
                Document Domain Randomization for Deep Learning Document Layout Extraction
            </h3>  <a class="anchor" name="ling2021icdar"></a>
            <div class="authors">
                <span class="firstAuthor">Meng Ling</span>,
                Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert S Laramee, Han-Wei Shen, Jian Wu, C Lee Giles
            </div>
            <div>
                <span class="publication">ICDAR 2021</span>
                <span class="publication">Full Paper</span>
                <a href="https://arxiv.org/pdf/2105.14931" target="_blank">PDF</a>
                <a href="https://arxiv.org/abs/2105.14931" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present document domain randomization (DDR), the first successful transfer of convolutional neural networks (CNNs) trained only on graphically rendered pseudo-paper pages to real-world document segmentation. DDR renders pseudo-document pages by modeling randomized textual and non-textual contents of interest, with user-defined layout and font styles to support joint learning of fine-grained classes. We demonstrate competitive results using our DDR approach to extract nine document classes from the benchmark CS-150 and papers published in two domains, namely annual meetings of Association for Computational Linguistics (ACL) and IEEE Visualization (VIS). We compare DDR to conditions of style mismatch, fewer or more noisy samples that are more easily obtained in the real world. We show that high-fidelity semantic information is not necessary to label semantic classes but style mismatch between train and test can lower model accuracy. Using smaller training samples had a slightly detrimental effect. Finally, network models still achieved high test accuracy when correct labels are diluted towards confusing labels; this behavior hold across several classes. 
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{ling2021icdar,
	Author = {Meng Ling and Jian Chen and Torsten M{\"o}ller and Petra Isenberg and Tobias Isenberg and Michael Sedlmair and Robert S Laramee and Han-Wei Shen and Jian Wu and C Lee Giles},
	Title = {Document Domain Randomization for Deep Learning Document Layout Extraction},
	Booktitle = {Document Analysis and Recognition (ICDAR)},
	publisher= {Springer International Publishing},
	pages = {497--513},
	url = {https://arxiv.org/abs/2105.14931},
  	doi = {10.1007/978-3-030-86549-8_32},
  	isbn = {978-3-030-86549-8},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperling2021visimages"
    >
        
            <img
                id="imageling2021visimages"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperling2021visimages', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/ling2021visimages.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperling2021visimages', 'small'); toggleImageSize(imageling2021visimages);"
                title="Click to show details"
            >
                Three Benchmark Datasets for Scholarly Article Layout Analysis
            </h3>  <a class="anchor" name="ling2021visimages"></a>
            <div class="authors">
                <span class="firstAuthor">Meng Ling</span>,
                Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert Laramee, Han-Wei Shen, Jian Wu, Clyde Lee Giles
            </div>
            <div>
                <span class="publication"> 2021</span>
                <span class="publication">Dataset</span>
                
                <a href="https://doi.org/10.21227/326q-bf39" target="_blank">website</a>
                
                <a href="https://ieee-dataport.org/open-access/three-benchmark-datasets-scholarly-article-layout-analysis" target="_blank">more...</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                This dataset contains three benchmark datasets as part of the scholarly output of an ICDAR 2021 paper: 

Meng Ling, Jian Chen, Torsten Möller, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Robert S. Laramee, Han-Wei Shen, Jian Wu, and C. Lee Giles, Document Domain Randomization for Deep Learning Document Layout Extraction, 16th International Conference on Document Analysis and Recognition (ICDAR) 2021. September 5-10, Lausanne, Switzerland. 

This dataset contains nine class lables: abstract, algorithm, author, body text, caption, equation, figure, table, and title.

* Dataset 1: CS-150x, an extension of the classical benchmark dataset CS-150 from three classes (figure, table, and caption) to nine classes, 1176 pages, Clark, C., Divvala, S.: Looking beyond text: Extracting figures, tables and captions from com- puter science papers. In: Workshops at the 29th AAAI Conference on Artificial Intelligence (2015), https://aaai.org/ocs/index.php/WS/AAAIW15/paper/view/10092.

* Dataset 2: ACL300, 300 randomly sampled articles (or 2508 pages) from the 55,759 papers scraped from the ACL anthology website; https://www.aclweb.org/anthology/.

* Dataset 3: VIS300, about 10% (or 2619 pages) of the document pages in randomly partitioned articles from 26,350 VIS paper pages published in  Chen, J., Ling, M., Li, R., Isenberg, P., Isenberg, T., Sedlmair, M., Möller, T., Laramee, R.S., Shen, H.W., Wünsche, K., Wang, Q.: VIS30K: A collection of figures and tables from IEEE visualization conference publications. IEEE Trans. Vis. Comput. Graph. 27 (2021), to appear doi: 10.1109/TVCG.2021.3054916.

This dataset is also available online at https://web.cse.ohio-state.edu/~chen.8028/ICDAR2021Benchmark/.

            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@data{ling2021visimages,
	author = {Meng Ling and Jian Chen and Torsten M{\"o}ller and Petra Isenberg and Tobias Isenberg and Michael Sedlmair and Robert Laramee and Han-Wei Shen and Jian Wu and Clyde Lee Giles},
	title = {Three Benchmark Datasets for Scholarly Article Layout Analysis},
	publisher = {IEEE Dataport},
	url = {https://ieee-dataport.org/open-access/three-benchmark-datasets-scholarly-article-layout-analysis},
	doi = {10.21227/326q-bf39},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papermorariu2021dumbledr"
    >
        
            <img
                id="imagemorariu2021dumbledr"
                title="Click to enlarge and show details"
                onclick="toggleClass('papermorariu2021dumbledr', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/morariu2021dumbledr.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papermorariu2021dumbledr', 'small'); toggleImageSize(imagemorariu2021dumbledr);"
                title="Click to show details"
            >
                DumbleDR: Predicting User Preferences of Dimensionality Reduction Projection Quality
            </h3>  <a class="anchor" name="morariu2021dumbledr"></a>
            <div class="authors">
                <span class="firstAuthor">Cristina Morariu</span>,
                Adrien Bibal, Rene Cutura, Benoît Frénay, Michael Sedlmair
            </div>
            <div>
                <span class="publication">arXiv 2021</span>
                
                <a href="https://arxiv.org/pdf/2105.09275.pdf" target="_blank">PDF</a>
                <a href="https://arxiv.org/abs/2105.09275" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                A plethora of dimensionality reduction techniques have emerged over the past decades, leaving researchers and analysts with a wide variety of choices for reducing their data, all the more so given some techniques come with additional parametrization (e.g. t-SNE, UMAP, etc.). Recent studies are showing that people often use dimensionality reduction as a black-box regardless of the specific properties the method itself preserves. Hence, evaluating and comparing 2D projections is usually qualitatively decided, by setting projections side-by-side and letting human judgment decide which projection is the best. In this work, we propose a quantitative way of evaluating projections, that nonetheless places human perception at the center. We run a comparative study, where we ask people to select 'good' and 'misleading' views between scatterplots of low-level projections of image datasets, simulating the way people usually select projections. We use the study data as labels for a set of quality metrics whose purpose is to discover and quantify what exactly people are looking for when deciding between projections. With this proxy for human judgments, we use it to rank projections on new datasets, explain why they are relevant, and quantify the degree of subjectivity in projections selected.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@techreport{morariu2021arxiv,
	title = {{DumbleDR}: Predicting User Preferences of Dimensionality Reduction Projection Quality},
	author = {Cristina Morariu and Adrien Bibal and Rene Cutura and Benoit Frenay and Michael Sedlmair},
	Institution = {{arXiv} preprint},
	Number = {arXiv:2105.09275},
	Type = {Technical Report},
	url = {https://arxiv.org/abs/2105.09275},
	Year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbernard2021proseco"
    >
        
            <img
                id="imagebernard2021proseco"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbernard2021proseco', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/bernard2021proseco.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbernard2021proseco', 'small'); toggleImageSize(imagebernard2021proseco);"
                title="Click to show details"
            >
                ProSeCo: Visual analysis of class separation measures and dataset characteristics
            </h3>  <a class="anchor" name="bernard2021proseco"></a>
            <div class="authors">
                <span class="firstAuthor">Jürgen Bernard</span>,
                Marco Hutter, Matthias Zeppelzauer, Michael Sedlmair, Tamara Munzner
            </div>
            <div>
                <span class="publication">C&G 2021</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://doi.org/10.1016/j.cag.2021.03.004" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Class separation is an important concept in machine learning and visual analytics. We address the visual analysis of class separation measures for both high-dimensional data and its corresponding projections into 2D through dimensionality reduction (DR) methods. Although a plethora of separation measures have been proposed, it is difficult to compare class separation between multiple datasets with different characteristics, multiple separation measures, and multiple DR methods. We present ProSeCo, an interactive visualization approach to support comparison between up to 20 class separation measures and up to 4 DR methods, with respect to any of 7 dataset characteristics: dataset size, dataset dimensions, class counts, class size variability, class size skewness, outlieriness, and real-world vs. synthetically generated data. ProSeCo supports (1) comparing across measures, (2) comparing high-dimensional to dimensionally-reduced 2D data across measures, (3) comparing between different DR methods across measures, (4) partitioning with respect to a dataset characteristic, (5) comparing partitions for a selected characteristic across measures, and (6) inspecting individual datasets in detail. We demonstrate the utility of ProSeCo in two usage scenarios, using datasets [1] posted at https://osf.io/epcf9/.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{bernard2021proseco,
	author = {J{\"u}rgen Bernard and Marco Hutter and Matthias Zeppelzauer and Michael Sedlmair and Tamara Munzner},
	title = {{ProSeCo}: Visual analysis of class separation measures and dataset characteristics},
	journal = {Computers \& Graphics},
	publisher = {Elsevier},
  	volume = {96},
  	pages = {48--60},
  	year = {2021},
	url = {https://doi.org/10.1016/j.cag.2021.03.004},
  	doi = {10.1016/j.cag.2021.03.004}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papermachicao2021a"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('papermachicao2021a', 'small'); toggleImageSize(imagemachicao2021a);"
                title="Click to show details"
            >
                A visual analysis method of randomness for classifying and ranking pseudo-random number generators
            </h3>  <a class="anchor" name="machicao2021a"></a>
            <div class="authors">
                <span class="firstAuthor">Jeaneth Machicao</span>,
                Quynh Ngo, Vladimir Molchanov, Lars Linsen, Odemir M Bruno
            </div>
            <div>
                <span class="publication">IS 2021</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://doi.org/10.1016/j.ins.2020.10.041" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The development of new pseudo-random number generators (PRNGs) has steadily increased over the years. Commonly, PRNGs’ randomness is “measured” by using statistical pass/fail suite tests, but the question remains, which PRNG is the best when compared to others. Existing randomness tests lack means for comparisons between PRNGs, since they are not quantitatively analysing. It is, therefore, an important task to analyze the quality of randomness for each PRNG, or, in general, comparing the randomness property among PRNGs. In this paper, we propose a novel visual approach to analyze PRNGs randomness allowing for a ranking comparison concerning the PRNGs’ quality. Our analysis approach is applied to ensembles of time series which are outcomes of different PRNG runs. The ensembles are generated by using a single PRNG method with different parameter settings or by using different PRNG methods. We propose a similarity metric for PRNG time series for randomness and apply it within an interactive visual approach for analyzing similarities of PRNG time series and relating them to an optimal result of perfect randomness. The interactive analysis leads to an unsupervised classification, from which respective conclusions about the impact of the PRNGs’ parameters or rankings of PRNGs on randomness are derived. We report new findings using our approach in a study of randomness for state-of-the-art numerical PRNGs such as LCG, PCG, SplitMix, Mersenne Twister, and RANDU as well as chaos-based PRNG families such as K-Logistic map and K-Tent map with varying parameter K.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{DBLP:journals/isci/MachicaoNMLB21,
  author    = {Jeaneth Machicao and
               Quynh Quang Ngo and
               Vladimir Molchanov and
               Lars Linsen and
               Odemir M. Bruno},
  title     = {A visual analysis method of randomness for classifying and ranking
               pseudo-random number generators},
  journal   = {Inf. Sci.},
  volume    = {558},
  pages     = {1--20},
  year      = {2021},
  url       = {https://doi.org/10.1016/j.ins.2020.10.041},
  doi       = {10.1016/j.ins.2020.10.041},
  timestamp = {Thu, 29 Apr 2021 15:12:58 +0200},
  biburl    = {https://dblp.org/rec/journals/isci/MachicaoNMLB21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperchen2021tvcg"
    >
        
            <img
                id="imagechen2021tvcg"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperchen2021tvcg', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/chen2021tvcg.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperchen2021tvcg', 'small'); toggleImageSize(imagechen2021tvcg);"
                title="Click to show details"
            >
                VIS30K: A collection of figures and tables from IEEE visualization conference publications
            </h3>  <a class="anchor" name="chen2021tvcg"></a>
            <div class="authors">
                <span class="firstAuthor">Jian Chen</span>,
                Meng Ling, Rui Li, Petra Isenberg, Tobias Isenberg, Michael Sedlmair, Torsten Möller, Robert S Laramee, Han-Wei Shen, Katharina Wünsche, Qiru Wang
            </div>
            <div>
                <span class="publication">TVCG 2021</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/chen2021tvcg.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1109/TVCG.2021.3054916" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present the VIS30K dataset, a collection of 29,689 images that represents 30 years of figures and tables from each track of the IEEE Visualization conference series (Vis, SciVis, InfoVis, VAST). VIS30K's comprehensive coverage of the scientific literature in visualization not only reflects the progress of the field but also enables researchers to study the evolution of the state-of-the-art and to find relevant work based on graphical content. We describe the dataset and our semi-automatic collection process, which couples convolutional neural networks (CNN) with curation. Extracting figures and tables semi-automatically allows us to verify that no images are overlooked or extracted erroneously. To improve quality further, we engaged in a peer-search process for high-quality figures from early IEEE Visualization papers. With the resulting data, we also contribute VISImageNavigator (VIN, visimagenavigator.github.io ), a web-based tool that facilitates searching and exploring VIS30K by author names, paper keywords, title and abstract, and years.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{chen2021tvcg,
	author = {Jian Chen and Meng Ling and Rui Li and Petra Isenberg and Tobias Isenberg and Michael Sedlmair and Torsten M{\"o}ller and Robert S Laramee and Han-Wei Shen and Katharina W{\"u}nsche and Qiru Wang},
	title = {{VIS30K}: A collection of figures and tables from {IEEE} visualization conference publications},
	journal = {IEEE Transactions on Visualization and Computer Graphics (TVCG)},
  	year = {2021},
  	volume = {27},
  	number = {9},
  	pages = {3826--3833},
  	url = {https://doi.org/10.1109/TVCG.2021.3054916},
  	doi = {10.1109/TVCG.2021.3054916}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperwaldner2021iv"
    >
        
            <img
                id="imagewaldner2021iv"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperwaldner2021iv', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/waldner2021iv.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperwaldner2021iv', 'small'); toggleImageSize(imagewaldner2021iv);"
                title="Click to show details"
            >
                Linking unstructured evidence to structured observations
            </h3>  <a class="anchor" name="waldner2021iv"></a>
            <div class="authors">
                <span class="firstAuthor">Manuela Waldner</span>,
                Thomas Geymayer, Dieter Schmalstieg, Michael Sedlmair
            </div>
            <div>
                <span class="publication">IV 2021</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/waldner2021iv.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1177/1473871620986249" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Many professionals, like journalists, writers, or consultants, need to acquire information from various sources, make sense of this unstructured evidence, structure their observations, and finally create and deliver their product, such as a report or a presentation. In formative interviews, we found that tools allowing structuring of observations are often disconnected from the corresponding evidence. Therefore, we designed a sensemaking environment with a flexible observation graph that visually ties together evidence in unstructured documents with the user’s structured knowledge. This is achieved through bi-directional deep links between highlighted document portions and nodes in the observation graph. In a controlled study, we compared users’ sensemaking strategies using either the observation graph or a simple text editor on a large display. Results show that the observation graph represents a holistic, compact representation of users’ observations, which can be linked to unstructured evidence on demand. In contrast, users taking textual notes required much more display space to spatially organize source documents containing unstructured evidence. This implies that spatial organization is a powerful strategy to structure observations even if the available space is limited.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{waldner2021iv,
	author = {Manuela Waldner and Thomas Geymayer and Dieter Schmalstieg and Michael Sedlmair},
	title = {Linking unstructured evidence to structured observations},
	journal = {Information Visualization},
	publisher = {{SAGE} Publications},
	volume = {20},
    number = {1},
    pages = {47--65},
    url = {https://doi.org/10.1177/1473871620986249},
    doi = {10.1177/1473871620986249},
	year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2020
    </h2>
    
    <div
        class="paper small"
        id="paperyu2020perspective"
    >
        
            <img
                id="imageyu2020perspective"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperyu2020perspective', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/yu2020perspective.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperyu2020perspective', 'small'); toggleImageSize(imageyu2020perspective);"
                title="Click to show details"
            >
                Perspective Matters: Design Implications for Motion Guidance in Mixed Reality
            </h3>  <a class="anchor" name="yu2020perspective"></a>
            <div class="authors">
                <span class="firstAuthor">Xingyao Yu</span>,
                Katrin Angerbauer, Peter Mohr, Denis Kalkofen, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2020</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/yu2020perspective.pdf" target="_blank">PDF</a>
                <a href="https://ieeexplore.ieee.org/abstract/document/9284729" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We investigate how Mixed Reality (MR) can be used to guide human body motions, such as in physiotherapy, dancing, or workout applications. While first MR prototypes have shown promising results, many dimensions of the design space behind such applications remain largely unexplored. To better understand this design space, we approach the topic from different angles by contributing three user studies. In particular, we take a closer look at the influence of the perspective, the characteristics of motions, and visual guidance on different user performance measures. Our results indicate that a first-person perspective performs best for all visible motions, whereas the type of visual instruction plays a minor role. From our results we compile a set of considerations that can guide future work on the design of instructions, evaluations, and the technical setup of MR motion guidance systems.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2020comparing"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperhube2020comparing', 'small'); toggleImageSize(imagehube2020comparing);"
                title="Click to show details"
            >
                Comparing Methods for Mapping Facial Expressions to Enhance Immersive Collaboration with Signs of Emotion
            </h3>  <a class="anchor" name="hube2020comparing"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Oliver Lenz, Lars Engeln, Rainer Groh, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ISMAR 2020</span>
                <span class="publication">Poster / Short Paper</span>
                
                <a href="https://ieeexplore.ieee.org/abstract/document/9288476/" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We present a user study comparing a pre-evaluated mapping approach with a state-of-the-art direct mapping method of facial expressions for emotion judgment in an immersive setting. At its heart, the pre-evaluated approach leverages semiotics, a theory used in linguistic. In doing so, we want to compare pre-evaluation with an approach that seeks to directly map real facial expressions onto their virtual counterparts. To evaluate both approaches, we conduct a controlled lab study with 22 participants. The results show that users are significantly more accurate in judging virtual facial expressions with pre-evaluated mapping. Additionally, participants were slightly more confident when deciding on a presented emotion. We could not find any differences regarding potential Uncanny Valley effects. However, the pre-evaluated mapping shows potential to be more convenient in a conversational scenario.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperheyen2020supporting"
    >
        
            <img
                id="imageheyen2020supporting"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperheyen2020supporting', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/heyen2020supporting.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperheyen2020supporting', 'small'); toggleImageSize(imageheyen2020supporting);"
                title="Click to show details"
            >
                Supporting Music Education through Visualizations of MIDI Recordings
            </h3>  <a class="anchor" name="heyen2020supporting"></a>
            <div class="authors">
                <span class="firstAuthor">Frank Heyen</span>,
                Michael Sedlmair
            </div>
            <div>
                <span class="publication">VIS 2020</span>
                <span class="publication">Poster</span>
                <a href="./pdf/heyen2020supporting.pdf" target="_blank">PDF</a>
                <a href="https://vis2020-ieee.ipostersessions.com/default.aspx?s=82-F0-FF-F9-29-B9-B4-7F-FE-F3-A9-1D-4A-B7-4F-32" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Musicians mostly have to rely on their ears when they want to analyze what they play, for example to detect errors. Since hearing is sequential, it is not possible to quickly grasp an overview over one or multiple recordings of a whole piece of music at once. We therefore propose various visualizations that allow analyzing errors and stylistic variance. Our current approach focuses on rhythm and uses MIDI data for simplicity.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbalestrucci2020pipelines"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperbalestrucci2020pipelines', 'small'); toggleImageSize(imagebalestrucci2020pipelines);"
                title="Click to show details"
            >
                Pipelines Bent, Pipelines Broken: Interdisciplinary Self-Reflection on the Impact of COVID-19 on Current and Future Research (Position Paper)
            </h3>  <a class="anchor" name="balestrucci2020pipelines"></a>
            <div class="authors">
                <span class="firstAuthor">Priscilla Balestrucci</span>,
                Katrin Angerbauer, Cristina Morariu, Robin Welsch, Lewis L Chuang, Daniel Weiskopf, Marc O Ernst, Michael Sedlmair
            </div>
            <div>
                <span class="publication">BELIV 2020</span>
                <span class="publication">Workshop</span>
                
                <a href="https://ieeexplore.ieee.org/abstract/document/9307759" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Among the many changes brought about by the COVID-19 pandemic, one of the most pressing for scientific research concerns user testing. For the researchers who conduct studies with human participants, the requirements for social distancing have created a need for reflecting on methodologies that previously seemed relatively straightforward. It has become clear from the emerging literature on the topic and from first-hand experiences of researchers that the restrictions due to the pandemic affect every aspect of the research pipeline. The current paper offers an initial reflection on user-based research, drawing on the authors' own experiences and on the results of a survey that was conducted among researchers in different disciplines, primarily psychology, human-computer interaction (HCI), and visualization communities. While this sampling of researchers is by no means comprehensive, the multi-disciplinary approach and the consideration of different aspects of the research pipeline allow us to examine current and future challenges for user-based research. Through an exploration of these issues, this paper also invites others in the VIS-as well as in the wider-research community, to reflect on and discuss the ways in which the current crisis might also present new and previously unexplored opportunities.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2020druidjs"
    >
        
            <img
                id="imagecutura2020druidjs"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2020druidjs', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/cutura2020druidjs.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2020druidjs', 'small'); toggleImageSize(imagecutura2020druidjs);"
                title="Click to show details"
            >
                DRUIDJS — A JavaScript Library for Dimensionality Reduction
            </h3>  <a class="anchor" name="cutura2020druidjs"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Christoph Kralj, Michael Sedlmair
            </div>
            <div>
                <span class="publication">VIS 2020</span>
                <span class="publication">Short Paper</span>
                <a href="./pdf/cutura2020druidjs.pdf" target="_blank">PDF</a>
                <a href="https://ieeexplore.ieee.org/abstract/document/9331283" target="_blank">website</a>
                <a href="https://youtu.be/LyiqHl4rq34" target="_blank">video</a>
                <a href="https://renecutura.eu/pdfs/Druid_Supp.pdf" target="_blank">more...</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Dimensionality reduction (DR) is a widely used technique for visualization. Nowadays, many of these visualizations are developed for the web, most commonly using JavaScript as the underlying programming language. So far, only few DR methods have a JavaScript implementation though, necessitating developers to write wrappers around implementations in other languages. In addition, those DR methods that exist in JavaScript libraries, such as PCA, t-SNE, and UMAP, do not offer consistent programming interfaces, hampering the quick integration of different methods. Toward a coherent and comprehensive DR programming framework, we developed an open source JavaScript library named DruidJS. Our library contains implementations of ten different DR algorithms, as well as the required linear algebra techniques, tools, and utilities.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2020druid,
  title={{DRUIDJS — A JavaScript Library for Dimensionality Reduction}},
  author={Cutura, Rene and Kralj, Christoph and Sedlmair, Michael},
  booktitle={2020 IEEE Visualization Conference (VIS)},
  pages={111--115},
  year={2020},
  organization={IEEE}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was supported by the BMVIT ICT of the Future program via the ViSciPub project (no. 867378) and handled by the FFG.
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperweiß2020revisited"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperweiß2020revisited', 'small'); toggleImageSize(imageweiß2020revisited);"
                title="Click to show details"
            >
                Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes
            </h3>  <a class="anchor" name="weiß2020revisited"></a>
            <div class="authors">
                <span class="firstAuthor">Maximilian Weiß</span>,
                Katrin Angerbauer, Alexandra Voit, Magdalena Schwarzl, Michael Sedlmair, Sven Mayer
            </div>
            <div>
                <span class="publication">VIS 2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://ieeexplore.ieee.org/abstract/document/9225008" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperngo2020interactive"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperngo2020interactive', 'small'); toggleImageSize(imagengo2020interactive);"
                title="Click to show details"
            >
                Interactive Generation of 1D Emeddings from 2D Multi-dimensional Data Projections
            </h3>  <a class="anchor" name="ngo2020interactive"></a>
            <div class="authors">
                <span class="firstAuthor">Quynh Ngo</span>,
                Lars Linsen
            </div>
            <div>
                <span class="publication">VMV 2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://doi.org/10.2312/vmv.20201190" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Visual analysis of multi-dimensional data is commonly supported by mapping the data to a 2D embedding. When analyzing a sequence of multi-dimensional data, e.g., in case of temporal data, the usage of 1D embeddings allows for plotting the entire sequence in a 2D layout. Despite the good performance in generating 2D embeddings, 1D embeddings often exhibit a much lower quality for pattern recognition tasks. We propose to overcome the issue by involving the user to generate 1D embeddings of multi-dimensional data in a two-step procedure: We first generate a 2D embedding and then leave the task of reducing the 2D to a 1D embedding to the user. We demonstrate that an interactive generation of 1D embeddings from 2D projected views can be performed efficiently, effectively, and targeted towards an analysis task. We compare the performance of our approach against automatically generated 1D and 2D embeddings involving a user study for our interactive approach. We test the 1D approaches when being applied to time-varying multi-dimensional data.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{DBLP:conf/vmv/NgoL20,
  author    = {Quynh Quang Ngo and
               Lars Linsen},
  editor    = {Jens H. Kr{\"{u}}ger and
               Matthias Nie{\ss}ner and
               J{\"{o}}rg St{\"{u}}ckler},
  title     = {Interactive Generation of 1D Embeddings from 2D Multi-dimensional
               Data Projections},
  booktitle = {25th International Symposium on Vision, Modeling and Visualization,
               {VMV} 2020, T{\"{u}}bingen, Germany, September 28 - October 1,
               2020},
  pages     = {79--87},
  publisher = {Eurographics Association},
  year      = {2020},
  url       = {https://doi.org/10.2312/vmv.20201190},
  doi       = {10.2312/vmv.20201190},
  timestamp = {Tue, 06 Oct 2020 16:51:11 +0200},
  biburl    = {https://dblp.org/rec/conf/vmv/NgoL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperheyen2020clavis"
    >
        
            <img
                id="imageheyen2020clavis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperheyen2020clavis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/heyen2020clavis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperheyen2020clavis', 'small'); toggleImageSize(imageheyen2020clavis);"
                title="Click to show details"
            >
                ClaVis: An Interactive Visual Comparison System for Classifiers
            </h3>  <a class="anchor" name="heyen2020clavis"></a>
            <div class="authors">
                <span class="firstAuthor">Frank Heyen</span>,
                Tanja Munz, Michael Neumann, Daniel Ortega, Ngoc Thang Vu, Daniel Weiskopf, Michael Sedlmair
            </div>
            <div>
                <span class="publication">AVI 2020</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/heyen2020clavis.pdf" target="_blank">PDF</a>
                <a href="https://doi.org/10.1145/3399715.3399814" target="_blank">website</a>
                
                <a href="https://github.com/fheyen/clavis" target="_blank">more...</a>
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose ClaVis, a visual analytics system for comparative analysis of classification models. ClaVis allows users to visually compare the performance and behavior of tens to hundreds of classifiers trained with different hyperparameter configurations. Our approach is plugin-based and classifier-agnostic and allows users to add their own datasets and classifier implementations. It provides multiple visualizations, including a multivariate ranking, a similarity map, a scatterplot that reveals correlations between parameters and scores, and a training history chart. We demonstrate the effectivity of our approach in multiple case studies for training classification models in the domain of natural language processing.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.1145/3399715.3399814,
author = {Heyen, Frank and Munz, Tanja and Neumann, Michael and Ortega, Daniel and Vu, Ngoc Thang and Weiskopf, Daniel and Sedlmair, Michael},
title = {ClaVis: An Interactive Visual Comparison System for Classifiers},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399814},
doi = {10.1145/3399715.3399814},
abstract = {We propose ClaVis, a visual analytics system for comparative analysis of classification
models. ClaVis allows users to visually compare the performance and behavior of tens
to hundreds of classifiers trained with different hyperparameter configurations. Our
approach is plugin-based and classifier-agnostic and allows users to add their own
datasets and classifier implementations. It provides multiple visualizations, including
a multivariate ranking, a similarity map, a scatterplot that reveals correlations
between parameters and scores, and a training history chart. We demonstrate the effectivity
of our approach in multiple case studies for training classification models in the
domain of natural language processing.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {9},
numpages = {9},
keywords = {visual analytics, Visualization, machine learning, classifier comparison},
location = {Salerno, Italy},
series = {AVI '20}
}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – Project-ID 251654672 – TRR 161 (A08) and under Germany’s Excellence Strategy – EXC-2075 – 39074001
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2020comparing"
    >
        
            <img
                id="imagecutura2020comparing"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2020comparing', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/cutura2020comparing.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2020comparing', 'small'); toggleImageSize(imagecutura2020comparing);"
                title="Click to show details"
            >
                Comparing and Exploring High-Dimensional Data with Dimensionality Reduction Algorithms and Matrix Visualizations
            </h3>  <a class="anchor" name="cutura2020comparing"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Michaël Aupetit, Jean-Daniel Fekete, Michael Sedlmair
            </div>
            <div>
                <span class="publication">AVI  2020</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/cutura2020comparing.pdf" target="_blank">PDF</a>
                <a href="https://dl.acm.org/doi/abs/10.1145/3399715.3399875" target="_blank">website</a>
                <a href="https://youtu.be/UPkH7rc0ulU" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose Compadre, a tool for visual analysis for comparing distances of high-dimensional (HD) data and their low-dimensional projections. At the heart is a matrix visualization to represent the discrepancy between distance matrices, linked side-by-side with 2D scatterplot projections of the data. Using different examples and datasets, we illustrate how this approach fosters (1) evaluating dimensionality reduction techniques w.r.t. how well they project the HD data, (2) comparing them to each other side-by-side, and (3) evaluate important data features through subspace comparison. We also present a case study, in which we analyze IEEE VIS authors from 1990 to 2018, and gain new insights on the relationships between coauthors, citations, and keywords. The coauthors are projected as accurately with UMAP as with t-SNE but the projections show different insights. The structure of the citation subspace is very different from the coauthor subspace. The keyword subspace is noisy yet consistent among the three IEEE VIS sub-conferences.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2020comparing,
  title={Comparing and exploring high-dimensional data with dimensionality reduction algorithms and matrix visualizations},
  author={Cutura, Rene and Aupetit, Micha{\"e}l and Fekete, Jean-Daniel and Sedlmair, Michael},
  booktitle={Proc. Intl. Conf. on Advanced Visual Interfaces (AVI)},
  pages={1--9},
  year={2020},
  doi={10.1145/3399715.3399875}}</textarea>
            </div>
            
            <h4>Acknowledgements</h4>
            <div class="abstract">
                This work was supported by the BMVIT ICT of the Future program via the ViSciPub project (no. 867378) and handled by the FFG.
            </div>
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperachberger2020caarvida"
    >
        
            <img
                id="imageachberger2020caarvida"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperachberger2020caarvida', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/achberger2020caarvida.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperachberger2020caarvida', 'small'); toggleImageSize(imageachberger2020caarvida);"
                title="Click to show details"
            >
                Caarvida: Visual Analytics for Test Drive Videos
            </h3>  <a class="anchor" name="achberger2020caarvida"></a>
            <div class="authors">
                <span class="firstAuthor">Alexander Achberger</span>,
                Rene Cutura, Oguzhan Türksoy, Michael Sedlmair
            </div>
            <div>
                <span class="publication">AVI  2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3399715.3399862" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{10.1145/3399715.3399862,
author = {Achberger, Alexander and Cutura, Ren\'{e} and T\"{u}rksoy, Oguzhan and Sedlmair, Michael},
title = {Caarvida: Visual Analytics for Test Drive Videos},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399862},
doi = {10.1145/3399715.3399862},
abstract = {We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {6},
numpages = {9},
keywords = {visual analytics, object detection, automotive, information visualization, human computer interaction},
location = {Salerno, Italy},
series = {AVI '20}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2020mixed"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperhube2020mixed', 'small'); toggleImageSize(imagehube2020mixed);"
                title="Click to show details"
            >
                Mixed Reality based Collaboration for Design Processes
            </h3>  <a class="anchor" name="hube2020mixed"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Mathias Müller, Esther Lapczyna, Jan Wojdziak
            </div>
            <div>
                <span class="publication">i-com 2020</span>
                <span class="publication">Journal Paper</span>
                
                <a href="https://doi.org/10.1515/icom-2020-0012" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Due to constantly and rapidly growing digitization, requirements for international cooperation are changing. Tools for collaborative work such as video telephony are already an integral part of today’s communication across companies. However, these tools are not sufficient to represent the full physical presence of an employee or a product as well as its components in another location, since the representation of information in a two-dimensional way and the resulting limited communication loses concrete objectivity. Thus, we present a novel object-centered approach that compromises of Augmented and Virtual Reality technology as well as design suggestions for remote collaboration. Furthermore, we identify current key areas for future research and specify a design space for the use of Augmented and Virtual Reality remote collaboration in the manufacturing process in the automotive industry.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperschatz2020scivis"
    >
        
            <img
                id="imageschatz2020scivis"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperschatz2020scivis', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/schatz2020scivis.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperschatz2020scivis', 'small'); toggleImageSize(imageschatz2020scivis);"
                title="Click to show details"
            >
                2019 IEEE Scientific Visualization Contest Winner: Visual Analysis of Structure Formation in Cosmic Evolution
            </h3>  <a class="anchor" name="schatz2020scivis"></a>
            <div class="authors">
                <span class="firstAuthor">Kartsen Schatz</span>,
                Christoph Müller, Patrick Gralka, Moritz Heinemann, Alexander Straub, Christoph Schulz, Matthias Braun, Tobias Rau, Michael Becher, Steffen Frey,  Guido Reina, Michael Sedlmair, others
            </div>
            <div>
                <span class="publication">CG&A 2020</span>
                
                
                <a href="https://doi.org/10.1109/MCG.2020.3004613" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Simulations of cosmic evolution are a means to explain the formation of the universe as we see it today. The resulting data of such simulations comprise numerous physical quantities, which turns their analysis into a complex task. Here, we analyze such high-dimensional and time-varying particle data using various visualization techniques from the fields of particle visualization, flow visualization, volume visualization, and information visualization. Our approach employs specialized filters to extract and highlight the development of so-called active galactic nuclei and filament structures formed by the particles. Additionally, we calculate X-ray emission of the evolving structures in a preprocessing step to complement visual analysis. Our approach is integrated into a single visual analytics framework to allow for analysis of star formation at interactive frame rates. Finally, we lay out the methodological aspects of our work that led to success at the 2019 IEEE SciVis Contest.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{schatz2020scivis,
   title={2019 {IEEE} {S}cientific {V}isualization Contest Winner: Visual Analysis of Structure Formation in Cosmic Evolution},
   author={Schatz, Karsten and M{\"u}ller, Christoph and Gralka, Patrick and Heinemann, Moritz and Straub, Alexander and Schulz, Christoph and Braun, Matthias and Rau, Tobias and Becher, Michael and Frey, Steffen and Reina, Guido and Sedlmair, Michael and others},
   journal={IEEE Computer Graphics and Applications (CG\&A)},
   volume={41},
    number={6},
    pages={101--110},
doi={10.1109/MCG.2020.3004613},
year = {2021}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperstreichert2020comparing"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperstreichert2020comparing', 'small'); toggleImageSize(imagestreichert2020comparing);"
                title="Click to show details"
            >
                Comparing Input Modalities for Shape Drawing Tasks
            </h3>  <a class="anchor" name="streichert2020comparing"></a>
            <div class="authors">
                <span class="firstAuthor">Annalena Streichert</span>,
                Katrin Angerbauer, Magdalena Schwarzl, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ETVIS 2020</span>
                <span class="publication">Workshop / Short Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3379156.3391830" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N= 20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papermerino2020toward"
    >
        
            <img
                id="imagemerino2020toward"
                title="Click to enlarge and show details"
                onclick="toggleClass('papermerino2020toward', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/merino2020toward.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papermerino2020toward', 'small'); toggleImageSize(imagemerino2020toward);"
                title="Click to show details"
            >
                Toward Agile Situated Visualization: An Exploratory User Study
            </h3>  <a class="anchor" name="merino2020toward"></a>
            <div class="authors">
                <span class="firstAuthor">Leonel Merino</span>,
                Boris Sotomayor-Gómez, Xingyao Yu, Ronie Salgado, Alexandre Bergel, Michael Sedlmair, Daniel Weiskopf
            </div>
            <div>
                <span class="publication">CHI 2020</span>
                <span class="publication">Extended Abstract</span>
                <a href="./pdf/merino2020toward.pdf" target="_blank">PDF</a>
                <a href="https://dl.acm.org/doi/abs/10.1145/3334480.3383017" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We introduce AVAR, a prototypical implementation of an agile situated visualization (SV) toolkit targeting liveness, integration, and expressiveness. We report on results of an exploratory study with AVAR and seven expert users. In it, participants wore a Microsoft HoloLens device and used a Bluetooth keyboard to program a visualization script for a given dataset. To support our analysis, we (i) video recorded sessions, (ii) tracked users' interactions, and (iii) collected data of participants' impressions. Our prototype confirms that agile SV is feasible. That is, liveness boosted participants' engagement when programming an SV, and so, the sessions were highly interactive and participants were willing to spend much time using our toolkit (i.e., median ≥ 1.5 hours). Participants used our integrated toolkit to deal with data transformations, visual mappings, and view transformations without leaving the immersive environment. Finally, participants benefited from our expressive toolkit and employed multiple of the available features when programming an SV.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkurzhals2020a"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperkurzhals2020a', 'small'); toggleImageSize(imagekurzhals2020a);"
                title="Click to show details"
            >
                A View on the Viewer: Gaze-Adaptive Captions for Videos
            </h3>  <a class="anchor" name="kurzhals2020a"></a>
            <div class="authors">
                <span class="firstAuthor">Kuno Kurzhals</span>,
                Fabian Göbel, Katrin Angerbauer, Michael Sedlmair, Martin Raubal
            </div>
            <div>
                <span class="publication">CHI 2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376266" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkraus2020assessing"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperkraus2020assessing', 'small'); toggleImageSize(imagekraus2020assessing);"
                title="Click to show details"
            >
                Assessing 2D and 3D Heatmaps for Comparative Analysis: An Empirical Study
            </h3>  <a class="anchor" name="kraus2020assessing"></a>
            <div class="authors">
                <span class="firstAuthor">Matthias Kraus</span>,
                Katrin Angerbauer, Juri Buchmüller, Daniel Schweitzer, Daniel A Keim, Michael Sedlmair, Johannes Fuchs
            </div>
            <div>
                <span class="publication">CHI 2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376675" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Heatmaps are a popular visualization technique that encode 2D density distributions using color or brightness. Experimental studies have shown though that both of these visual variables are inaccurate when reading and comparing numeric data values. A potential remedy might be to use 3D heatmaps by introducing height as a third dimension to encode the data. Encoding abstract data in 3D, however, poses many problems, too. To better understand this tradeoff, we conducted an empirical study (N=48) to evaluate the user performance of 2D and 3D heatmaps for comparative analysis tasks. We test our conditions on a conventional 2D screen, but also in a virtual reality environment to allow for real stereoscopic vision. Our main results show that 3D heatmaps are superior in terms of error rate when reading and comparing single data items. However, for overview tasks, the well-established 2D heatmap performs better.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperboshe-plois2020visual"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperboshe-plois2020visual', 'small'); toggleImageSize(imageboshe-plois2020visual);"
                title="Click to show details"
            >
                Visual Analysis of Billiard Dynamics Simulation Ensembles
            </h3>  <a class="anchor" name="boshe-plois2020visual"></a>
            <div class="authors">
                <span class="firstAuthor">Stefan Boshe-Plois</span>,
                Quynh Ngo, Peter Albers, Lars Linsen
            </div>
            <div>
                <span class="publication">VISIGRAPP 2020</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://doi.org/10.5220/0008956201850192" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Mathematical billiards assume a table of a certain shape and dynamical rules for handling collisions. Some trajectories exhibit distinguished patterns. Detecting such trajectories manually for a given billiard is cumbersome, especially, when assuming an ensemble of billiards with different parameter settings. We propose a visual analysis approach for simulation ensembles of billiard dynamics based on phase-space visualizations and multi-dimensional scaling. We apply our methods to the well-studied approach of dynamical billiards for validation and to the novel approach of symplectic billiards for new observations.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{DBLP:conf/grapp/Boshe-PloisNAL20,
  author    = {Stefan Boshe{-}Plois and
               Quynh Quang Ngo and
               Peter Albers and
               Lars Linsen},
  editor    = {Andreas Kerren and
               Christophe Hurter and
               Jos{\'{e}} Braz},
  title     = {Visual Analysis of Billiard Dynamics Simulation Ensembles},
  booktitle = {Proceedings of the 15th International Joint Conference on Computer
               Vision, Imaging and Computer Graphics Theory and Applications, {VISIGRAPP}
               2020, Volume 3: IVAPP, Valletta, Malta, February 27-29, 2020},
  pages     = {185--192},
  publisher = {{SCITEPRESS}},
  year      = {2020},
  url       = {https://doi.org/10.5220/0008956201850192},
  doi       = {10.5220/0008956201850192},
  timestamp = {Thu, 16 Apr 2020 15:03:30 +0200},
  biburl    = {https://dblp.org/rec/conf/grapp/Boshe-PloisNAL20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2019
    </h2>
    
    <div
        class="paper small"
        id="paperngo2019visual"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperngo2019visual', 'small'); toggleImageSize(imagengo2019visual);"
                title="Click to show details"
            >
                Visual Analytics of Simulation Ensembles for Network Dynamics
            </h3>  <a class="anchor" name="ngo2019visual"></a>
            <div class="authors">
                <span class="firstAuthor">Quynh Ngo</span>,
                Marc-Thorsten Hütt, Lars Linsen
            </div>
            <div>
                <span class="publication">VMV 2019</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://doi.org/10.2312/vmv.20191322" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                A central question in the field of Network Science is to analyze the role of a given network topology on the dynamical behavior captured by time-varying simulations executed on the network. These dynamical systems are also influenced by global simulation parameters. We present a visual analytics approach that supports the investigation of the impact of the parameter settings, i.e., how parameter choices change the role of network topology on the simulations' dynamics. To answer this question, we are analyzing ensembles of simulation runs with different parameter settings executed on a given network topology. We relate the nodes' topological structures to their dynamical similarity in a 2D plot based on an interactively defined hierarchy of topological properties and a 1D embedding for the dynamical similarity. We evaluate interactively defined topological groups with respect to matching dynamical behavior, which we visually encode as graphs of the function of the considered simulation parameter. Interactive filtering and coordinated views allow for a detailed analysis of the parameter space with respect to topology-dynamics relations. Our visual analytics approach is applied to scenarios for excitable dynamics on synthetic and real brain connectome networks.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{DBLP:conf/vmv/NgoHL19,
  author    = {Quynh Quang Ngo and
               Marc{-}Thorsten H{\"{u}}tt and
               Lars Linsen},
  editor    = {Hans{-}J{\"{o}}rg Schulz and
               Matthias Teschner and
               Michael Wimmer},
  title     = {Visual Analytics of Simulation Ensembles for Network Dynamics},
  booktitle = {24th International Symposium on Vision, Modeling, and Visualization,
               {VMV} 2019, Rostock, Germany, September 30 - October 2, 2019},
  pages     = {89--97},
  publisher = {Eurographics Association},
  year      = {2019},
  url       = {https://doi.org/10.2312/vmv.20191322},
  doi       = {10.2312/vmv.20191322},
  timestamp = {Wed, 26 May 2021 11:51:06 +0200},
  biburl    = {https://dblp.org/rec/conf/vmv/NgoHL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperangerbauer2019automatic"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperangerbauer2019automatic', 'small'); toggleImageSize(imageangerbauer2019automatic);"
                title="Click to show details"
            >
                Automatic Compression of Subtitles with Neural Networks and its Effect on User Experience
            </h3>  <a class="anchor" name="angerbauer2019automatic"></a>
            <div class="authors">
                <span class="firstAuthor">Katrin Angerbauer</span>,
                Heike Adel, Ngoc Thang Vu
            </div>
            <div>
                <span class="publication">INTERSPEECH 2019</span>
                <span class="publication">Poster</span>
                
                <a href="https://www.isca-speech.org/archive/Interspeech_2019/abstracts/1750.html" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Understanding spoken language can be impeded through factors like noisy environments, hearing impairments or lack of proficiency. Subtitles can help in those cases. However, for fast speech or limited screen size, it might be advantageous to compress the subtitles to their most relevant content. Therefore, we address automatic sentence compression in this paper. We propose a neural network model based on an encoder-decoder approach with the possibility of integrating the desired compression ratio. Using this model, we conduct a user study to investigate the effects of compressed subtitles on user experience. Our results show that compressed subtitles can suffice for comprehension but may pose additional cognitive load.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperbruder2019voronoi-based"
    >
        
            <img
                id="imagebruder2019voronoi-based"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperbruder2019voronoi-based', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/bruder2019voronoi-based.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperbruder2019voronoi-based', 'small'); toggleImageSize(imagebruder2019voronoi-based);"
                title="Click to show details"
            >
                Voronoi-Based Foveated Volume Rendering
            </h3>  <a class="anchor" name="bruder2019voronoi-based"></a>
            <div class="authors">
                <span class="firstAuthor">Valentin Bruder</span>,
                Christoph Schulz, Ruben Bauer, Steffen Frey, Daniel Weiskopf, Thomas Ertl
            </div>
            <div>
                <span class="publication">EuroVis 2019</span>
                <span class="publication">Short Paper</span>
                <a href="https://diglib.eg.org/bitstream/handle/10.2312/evs20191172/067-071.pdf?sequence=1&isAllowed=y" target="_blank">PDF</a>
                <a href="https://doi.org/10.2312/evs.20191172" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Foveal vision is located in the center of the field of view with a rich impression of detail and color, whereas peripheral vision occurs on the side with more fuzzy and colorless perception. This visual acuity fall-off can be used to achieve higher frame rates by adapting rendering quality to the human visual system. Volume raycasting has unique characteristics, preventing a direct transfer of many traditional foveated rendering techniques. We present an approach that utilizes the visual acuity fall-off to accelerate volume rendering based on Linde-Buzo-Gray sampling and natural neighbor interpolation. First, we measure gaze using a stationary 1200 Hz eye-tracking system. Then, we adapt our sampling and reconstruction strategy to that gaze. Finally, we apply a temporal smoothing filter to attenuate undersampling artifacts since peripheral vision is particularly sensitive to contrast changes and movement. Our approach substantially improves rendering performance with barely perceptible changes in visual quality. We demonstrate the usefulness of our approach through performance measurements on various data sets.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings {10.2312:evs.20191172,
booktitle = {EuroVis 2019 - Short Papers},
editor = {Johansson, Jimmy and Sadlo, Filip and Marai, G. Elisabeta},
title = {{Voronoi-Based Foveated Volume Rendering}},
author = {Bruder, Valentin and Schulz, Christoph and Bauer, Ruben and Frey, Steffen and Weiskopf, Daniel and Ertl, Thomas},
year = {2019},
publisher = {The Eurographics Association},
ISBN = {978-3-03868-090-1},
DOI = {10.2312/evs.20191172}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhamid2019visual"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperhamid2019visual', 'small'); toggleImageSize(imagehamid2019visual);"
                title="Click to show details"
            >
                Visual Ensemble Analysis to Study the Influence of Hyper-parameters on Training Deep Neural Networks
            </h3>  <a class="anchor" name="hamid2019visual"></a>
            <div class="authors">
                <span class="firstAuthor">Sagad Hamid</span>,
                Adrian Derstroff, Sören Klemm, Quynh Ngo, Xiaoyi Jiang, Lars Linsen
            </div>
            <div>
                <span class="publication">EuroVis 2019</span>
                <span class="publication">Workshop Paper</span>
                
                <a href="https://doi.org/10.2312/mlvis.20191160" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                A good deep neural network design allows for efficient training and high accuracy. The training step requires a suitable choice of several hyper-parameters. Limited knowledge exists on how the hyper-parameters impact the training process, what is the interplay of multiple hyper-parameters, and what is the interrelation of hyper-parameters and network topology. In this paper, we present a structured analysis towards these goals by investigating an ensemble of training runs.We propose a visual ensemble analysis based on hyper-parameter space visualizations, performance visualizations, and visualizing correlations of topological structures. As a proof of concept, we apply our approach to deep convolutional neural networks.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{DBLP:conf/mlvis-ws/HamidDKNJL19,
  author    = {Sagad Hamid and
               Adrian Derstroff and
               S{\"{o}}ren Klemm and
               Quynh Quang Ngo and
               Xiaoyi Jiang and
               Lars Linsen},
  editor    = {Daniel Archambault and
               Ian T. Nabney and
               Jaakko Peltonen},
  title     = {Visual Ensemble Analysis to Study the Influence of Hyper-parameters
               on Training Deep Neural Networks},
  booktitle = {2nd Workshop on Machine Learning Methods in Visualisation for Big
               Data, MLVis@EuroVis 2019, Porto, Portugal, June 3, 2019},
  pages     = {19--23},
  publisher = {Eurographics Association},
  year      = {2019},
  url       = {https://doi.org/10.2312/mlvis.20191160},
  doi       = {10.2312/mlvis.20191160},
  timestamp = {Wed, 10 Feb 2021 15:14:52 +0100},
  biburl    = {https://dblp.org/rec/conf/mlvis-ws/HamidDKNJL19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperkern2019lessons"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperkern2019lessons', 'small'); toggleImageSize(imagekern2019lessons);"
                title="Click to show details"
            >
                Lessons Learned from Users Reading Highlighted Abstracts in a Digital Library
            </h3>  <a class="anchor" name="kern2019lessons"></a>
            <div class="authors">
                <span class="firstAuthor">Dagmar Kern</span>,
                Daniel Hienert, Katrin Angerbauer, Tilman Dingler, Pia Borlund
            </div>
            <div>
                <span class="publication">CHIIR 2019</span>
                <span class="publication">Short Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3295750.3298950" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Finding relevant documents is essential for researchers of all disciplines. We investigated an approach for supporting searchers in their relevance decision in a digital library by automatically highlighting the most important keywords in abstracts. We conducted an eye-tracking study with 25 subjects and observed very different search and reading behavior which lead to diverse results. Some of the participants liked that highlighted abstracts accelerate their relevance decision, while others found that they disturb the reading flow. What many agree on is that the quality of highlighting is crucial for trust and system credibility.
            </div>
            
            
        </div>
    </div>
    
    
    <h2>
        2018
    </h2>
    
    <div
        class="paper small"
        id="paperyu2018effect"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperyu2018effect', 'small'); toggleImageSize(imageyu2018effect);"
                title="Click to show details"
            >
                Effect of Using HMDs for One Hour on Preteens Visual Fatigue
            </h3>  <a class="anchor" name="yu2018effect"></a>
            <div class="authors">
                <span class="firstAuthor">Xingyao Yu</span>,
                Dongdong Weng, Jie Guo, Haiyan Jiang, Yihua Bao
            </div>
            <div>
                <span class="publication">ISMAR 2018</span>
                <span class="publication">Poster</span>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8699249" target="_blank">PDF</a>
                <a href="https://ieeexplore.ieee.org/abstract/document/8699249" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We designed a within-subject experiment to compare visual discomfort to preteen users caused by using head-mounted displays (HMD) and tablet computers for an hour. 18 participants younger than 13 years old were recruited to fulfill a series of similar painting tasks under both display conditions. Visual fatigue was measured with visual analog scale before and after experiment and during the break of experiment. The results indicated that HMD had a trend to bring higher visual fatigue than tablet computer during the exposure of 1 hour. Although the symptoms of visual discomfort disappeared after resting, there is need for preteen-specific head-mounted displays.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperengeln2018immersive"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperengeln2018immersive', 'small'); toggleImageSize(imageengeln2018immersive);"
                title="Click to show details"
            >
                Immersive VisualAudioDesign: Spectral Editing in VR
            </h3>  <a class="anchor" name="engeln2018immersive"></a>
            <div class="authors">
                <span class="firstAuthor">Lars Engeln</span>,
                Natalie Hube, Rainer Groh
            </div>
            <div>
                <span class="publication">Audio Mostly 2018</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3243274.3243279" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                VisualAudioDesign (VAD) is an attempt to design audio in a visual way. The frequency-domain visualized as a spectrogram construed as pixel data can be manipulated with image filters. Thereby, an approach is described to get away from direct DSP parameter manipulation to a more comprehensible sound design. Virtual Reality (VR) offers immersive insights into data and embodied interaction in the virtual environment. VAD and VR combined enrich spectral editing with a natural work-flow. Therefore, a design paper prototype for interaction with audio data in an virtual environment was used and examined.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2018towards"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperhube2018towards', 'small'); toggleImageSize(imagehube2018towards);"
                title="Click to show details"
            >
                Towards augmented reality in quality assurance processes
            </h3>  <a class="anchor" name="hube2018towards"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Mathias Müller, Jan Wojdziak, Franziska Hannß, Rainer Groh
            </div>
            <div>
                <span class="publication">MMSys 2018</span>
                <span class="publication">Workshop Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3210438.3210442" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Augmented reality (AR) has gained exceptional importance in supporting task performance. Particularly, in quality assurance (QA) processes in the automotive sector AR offers a diversity of use cases. In this paper we propose an interface design which projects information as a digital canvas on the surface of vehicle components. Based on a requirement analysis, we discuss design aspects and describe our application in applying the quality assurance process of a luxury automaker. The application includes a personal view on spatial information embedded in a guided interaction process as a design solution that can be applied to enhance QA processes.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2018the"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperhube2018the', 'small'); toggleImageSize(imagehube2018the);"
                title="Click to show details"
            >
                The Data in Your Hands: Exploring Novel Interaction Techniques and Data Visualization Approaches for Immersive Data Analytics
            </h3>  <a class="anchor" name="hube2018the"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Mathias Müller
            </div>
            <div>
                <span class="publication">AVI  2018</span>
                <span class="publication">Workshop Paper</span>
                <a href="http://ceur-ws.org/Vol-2108/paper2.pdf" target="_blank">PDF</a>
                <a href="" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we describe a concept for visualization and interaction with a large data set in an virtual environment. The core idea uses the traditional flat 2D representation as a base visualization but lets the user transform it into a spatial 3D visualizations on demand. Our visualization and interaction concept targets data analysts to use it for exploration and analysis, utilizing virtual reality to gain insight into complex data sets. The concept is based on the use of Parallel Sets for the representation of categorical data. By extending the conventional 2D Parallel Sets with a third dimension, correlations between path variables and the related number of items belonging to a specific node can be visualized. Furthermore, the concept uses virtual reality controllers in combination with a head-mounted display to control additional views. The purpose of the paper is to describe the core concepts and challenges for this type of spatial visualization and the related interaction design, including the use of gestures for direct manuipulation and a hand-attached menu for complex actions
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2018facilitating"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperhube2018facilitating', 'small'); toggleImageSize(imagehube2018facilitating);"
                title="Click to show details"
            >
                Facilitating exploration on exhibitions with augmented reality
            </h3>  <a class="anchor" name="hube2018facilitating"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Mathias Müller, Rainer Groh
            </div>
            <div>
                <span class="publication">AVI 2018</span>
                <span class="publication">Poster Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3206505.3206585" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                At exhibitions, visitors are usually in a completely unknown environment. Although visitors generally are informed about the topic before a visit, interests are still difficult to extract from the mass of exhibition stands and offers. In this paper we describe a concept using head-coupled AR together with recommender mechanisms for exhibitions. We present a conceptual development for a first prototype with focus on navigational aspects as well as explicit and implicit recommendations to generate input data for visually displayed recommendations.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papercutura2018viscoder"
    >
        
            <img
                id="imagecutura2018viscoder"
                title="Click to enlarge and show details"
                onclick="toggleClass('papercutura2018viscoder', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/cutura2018viscoder.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('papercutura2018viscoder', 'small'); toggleImageSize(imagecutura2018viscoder);"
                title="Click to show details"
            >
                VisCoDeR: A Tool for Visually Comparing Dimensionality Reduction Algorithms
            </h3>  <a class="anchor" name="cutura2018viscoder"></a>
            <div class="authors">
                <span class="firstAuthor">Rene Cutura</span>,
                Stefan Holzer, Michaël Aupetit, Michael Sedlmair
            </div>
            <div>
                <span class="publication">ESANN 2018</span>
                <span class="publication">Full Paper</span>
                <a href="./pdf/cutura2018viscoder.pdf" target="_blank">PDF</a>
                <a href="" target="_blank">website</a>
                <a href="https://youtu.be/gg2pgv0xwmc" target="_blank">video</a>
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                We propose VisCoDeR, a tool that leverages comparative visualization to support learning and analyzing different dimensionality reduction (DR) methods. VisCoDeR fosters two modes. The Discover mode allows qualitatively comparing several DR results by juxtaposing and linking the resulting scatterplots. The Explore mode allows for analyzing hundreds of differently parameterized DR results in a quantitative way. We present use cases that show that our approach helps to understand similarities and differences between DR algorithms.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{cutura2018viscoder,
  title={{VisCoDeR: A Tool for Visually Comparing Dimensionality Reduction Algorithms}},
  author={Cutura, Rene and Holzer, Stefan and Aupetit, Micha{\"e}l and Sedlmair, Michael},
  booktitle={Euro. Symp. on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN)},
  pages={641--646}
  year={2018}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2017
    </h2>
    
    <div
        class="paper small"
        id="paperangerbauer2017the"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperangerbauer2017the', 'small'); toggleImageSize(imageangerbauer2017the);"
                title="Click to show details"
            >
                The Back End is Only One Part of the Picture: Mobile-Aware Application Performance Monitoring and Problem Diagnosis
            </h3>  <a class="anchor" name="angerbauer2017the"></a>
            <div class="authors">
                <span class="firstAuthor">Katrin Angerbauer</span>,
                Dušan Okanović,  André van Hoorn, Christoph Heger
            </div>
            <div>
                <span class="publication">VALUETOOLS 2017</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/3150928.3150939" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The success of modern businesses relies on the quality of their supporting application systems. Continuous application performance management is mandatory to enable efficient problem detection, diagnosis, and resolution during production. In today's age of ubiquitous computing, large fractions of users access application systems from mobile devices, such as phones and tablets. For detecting, diagnosing, and resolving performance and availability problems, an end-to-end view, i.e., traceability of requests starting on the (mobile) clients' devices, is becoming increasingly important. In this paper, we propose an approach for end-to-end monitoring of applications from the users' mobile devices to the back end, and diagnosing root-causes of detected performance problems. We extend our previous work on diagnosing performance anti-patterns from execution traces by new metrics and rules. The evaluation of this work shows that our approach successfully detects and diagnoses performance anti-patterns in applications with iOS-based mobile clients. While there are threats to validity to our experiment, our research is a promising starting point for future work.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperdingler2017text"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperdingler2017text', 'small'); toggleImageSize(imagedingler2017text);"
                title="Click to show details"
            >
                Text Priming-Effects of Text Visualizations on Readers Prior to Reading
            </h3>  <a class="anchor" name="dingler2017text"></a>
            <div class="authors">
                <span class="firstAuthor">Tilman Dingler</span>,
                Dagmar Kern, Katrin Angerbauer, Albrecht Schmidt
            </div>
            <div>
                <span class="publication">INTERACT 2017</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://link.springer.com/chapter/10.1007/978-3-319-67687-6_23" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Living in our information society poses the challenge of having to deal with a plethora of information. While most content is represented through text, keyword extraction and visualization techniques allow the processing and adjustment of text presentation to the readers’ individual requirements and preferences. In this paper, we investigate four types of text visualizations and their feasibility to give readers an overview before they actually engage with a text: word clouds, highlighting, mind maps, and image collages. In a user study with 50 participants, we assessed the effects of such visualizations on reading comprehension, reading time, and subjective impressions. Results show that (1) mind maps best support readers in getting the gist of a text, (2) they also give better subjective impressions on text content and structure, and (3) highlighting keywords in a text before reading helps to reduce reading time. We discuss a set of guidelines to inform the design of automated systems for creating text visualizations for reader support.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@InProceedings{10.1007/978-3-319-67687-6_23,
author="Dingler, Tilman
and Kern, Dagmar
and Angerbauer, Katrin
and Schmidt, Albrecht",
editor="Bernhaupt, Regina
and Dalvi, Girish
and Joshi, Anirudha
and K. Balkrishan, Devanuj
and O'Neill, Jacki
and Winckler, Marco",
title="Text Priming - Effects of Text Visualizations on Readers Prior to Reading",
booktitle="Human-Computer Interaction -- INTERACT 2017",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="345--365",
abstract="Living in our information society poses the challenge of having to deal with a plethora of information. While most content is represented through text, keyword extraction and visualization techniques allow the processing and adjustment of text presentation to the readers' individual requirements and preferences. In this paper, we investigate four types of text visualizations and their feasibility to give readers an overview before they actually engage with a text: word clouds, highlighting, mind maps, and image collages. In a user study with 50 participants, we assessed the effects of such visualizations on reading comprehension, reading time, and subjective impressions. Results show that (1) mind maps best support readers in getting the gist of a text, (2) they also give better subjective impressions on text content and structure, and (3) highlighting keywords in a text before reading helps to reduce reading time. We discuss a set of guidelines to inform the design of automated systems for creating text visualizations for reader support.",
isbn="978-3-319-67687-6"
}

</textarea>
            </div>
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="papermüller2017a"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('papermüller2017a', 'small'); toggleImageSize(imagemüller2017a);"
                title="Click to show details"
            >
                A zoomable product browser for elastic displays
            </h3>  <a class="anchor" name="müller2017a"></a>
            <div class="authors">
                <span class="firstAuthor">Mathias Müller</span>,
                Mandy Keck, Thomas Gründer, Natalie Hube, Rainer Groh
            </div>
            <div>
                <span class="publication">xCoAX 2017</span>
                <span class="publication">Full Paper</span>
                <a href="http://2017.xcoax.org/pdf/xcoax2017-Muller.pdf" target="_blank">PDF</a>
                <a href="" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we present an interaction and visualization concept for elastic displays. The interaction concept was inspired by the search process of a rummage table to explore a large set of product data. The basic approach uses a similarity-based search pattern—based on a small set of items, the user refines the search result by examining similar items and exchanging them with items from the current result. A physically-based approach is used to interact with the data by deforming the surface of the elastic display. The presented visualization concept uses glyphs to directly compare items at a glance. Zoomable UI techniques controlled by the deformation of the elastic surface allow to display different levels of detail for each item.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2017additional"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperhube2017additional', 'small'); toggleImageSize(imagehube2017additional);"
                title="Click to show details"
            >
                Additional On-Demand Dimension for Data Visualization
            </h3>  <a class="anchor" name="hube2017additional"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Mathias Müller, Rainer Groh
            </div>
            <div>
                <span class="publication">EuroVis 2017</span>
                <span class="publication">Short Paper</span>
                <a href="https://diglib.eg.org/bitstream/handle/10.2312/eurovisshort20171151/163-167.pdf" target="_blank">PDF</a>
                <a href="https://dl.acm.org/doi/10.2312/eurovisshort.20171151" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                In this paper, we present a concept to interactively extend an 2d visualization by an additional on-demand dimension. We use categorical data in a multidimensional information space applied in a travel search scenario. Parallel sets are used as the basis for the visualization concept, since this is particularly suitable for the visualization of categorical data. The on-demand dimension expands the vertical axis of a parallel coordinate graph into depth axis and is intended to increase comparability of path variables with respect to the number of elements belonging to the respective parameter axis instead of direct comparability of individual paths and keep relations between the parallel sets. The presented implementation suits as foundation for further studies about the usefulness of a dynamic, on demand extension a of 2d visualizations into spatial visualizations. Furthermore, we present some additional approaches about the usage of the increased visualization space.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperngo2017visual"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperngo2017visual', 'small'); toggleImageSize(imagengo2017visual);"
                title="Click to show details"
            >
                Visual Analytics of Global Parameters in Simulation Ensembles of ODE-based Excitable Network Dynamics
            </h3>  <a class="anchor" name="ngo2017visual"></a>
            <div class="authors">
                <span class="firstAuthor">Quynh Ngo</span>,
                Marc-Thorsten Hütt, Lars Linsen
            </div>
            <div>
                <span class="publication">EuroVIS 2017</span>
                <span class="publication">Poster</span>
                
                <a href="https://doi.org/10.2312/eurp.20171179" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The role of network topology on the dynamics in simulations that are executed on the network is a central question in the field of network science. However, the influence of the topology is affected by the global dynamical simulation parameters. To investigate this impact of the parameter settings, multiple simulation runs are executed with different settings. Moreover, since the outcome of a single simulation run also depends on the randomly chosen start configurations, multiple runs with the same settings are carried out, as well. We present a visual approach to analyze the role of topology in such an ensemble of simulation ensembles. We use the dynamics of an excitable network implemented in the form of a coupled ordinary differential equation (ODE) following the FitzHugh-Nagumo (FHN) model and modular network topologies.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@inproceedings{DBLP:conf/vissym/NgoHL17,
  author    = {Quynh Quang Ngo and
               Marc{-}Thorsten H{\"{u}}tt and
               Lars Linsen},
  editor    = {Anna Puig and
               Tobias Isenberg},
  title     = {Visual Analytics of Global Parameters in Simulation Ensembles of ODE-based
               Excitable Network Dynamics},
  booktitle = {19th Eurographics Conference on Visualization, EuroVis 2017 - Posters,
               Barcelona, Spain, June 12-16, 2017},
  pages     = {101--103},
  publisher = {Eurographics Association},
  year      = {2017},
  url       = {https://doi.org/10.2312/eurp.20171179},
  doi       = {10.2312/eurp.20171179},
  timestamp = {Wed, 01 Jul 2020 13:35:20 +0200},
  biburl    = {https://dblp.org/rec/conf/vissym/NgoHL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2016
    </h2>
    
    <div
        class="paper small"
        id="paperyu2016reduce"
    >
        
            <img
                id="imageyu2016reduce"
                title="Click to enlarge and show details"
                onclick="toggleClass('paperyu2016reduce', 'small'); toggleImageSize(this);"
                class="publicationImage small"
                src="./img/small/yu2016reduce.png"
            />
        <div class="metaData ">
            <h3
                onclick="toggleClass('paperyu2016reduce', 'small'); toggleImageSize(imageyu2016reduce);"
                title="Click to show details"
            >
                Reduce Simulator Sickness by Overwritten Symbol in Smartphone-Based VR System
            </h3>  <a class="anchor" name="yu2016reduce"></a>
            <div class="authors">
                <span class="firstAuthor">Xingyao Yu</span>,
                Dongdong Weng, Li Cai
            </div>
            <div>
                <span class="publication">IEEE ICVRV 2016</span>
                <span class="publication">Workshop / Short Paper</span>
                <a href="./pdf/yu2016reduce.pdf" target="_blank">PDF</a>
                <a href="https://ieeexplore.ieee.org/abstract/document/7938233" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                The aim of this paper is to reduce simulator sickness caused by the low refresh rate of display in smartphone-based VR system. Without regard to the improvement of hardware, the method proposed in this paper reduces simulator sickness by adding static symbol on the screen of the smartphone. A series of user-participation experiments were done to validate the effectiveness of the method. Participants' responses to the symbol with different textures (cross or Minion logo) and in different positions (the center or near the corners) were assessed by Simulator Sickness Questionnaire (SSQ). The preliminary results demonstrate that the existence, the position and complexity of the symbols can be factors in relieving symptoms of simulator sickness.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperhube2016virtual"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperhube2016virtual', 'small'); toggleImageSize(imagehube2016virtual);"
                title="Click to show details"
            >
                Virtual UNREALity: Exploring Alternative Visualization Techniques for Virtual Reality
            </h3>  <a class="anchor" name="hube2016virtual"></a>
            <div class="authors">
                <span class="firstAuthor">Natalie Hube</span>,
                Hannes Grusla, Mathias Müller, Ingmar S. Franke, Tobias Günther, Rainer Groh
            </div>
            <div>
                <span class="publication">xCoAX 2016</span>
                <span class="publication">Full Paper</span>
                <a href="http://2016.xcoax.org/pdf/xcoax2016-Hube.pdf" target="_blank">PDF</a>
                <a href="" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Virtual Reality (VR) offers new ways to perceive and interact with virtual content. Apart from photo-realism, VR can be used to explore new ways of visualization and interaction. In this contribution, we describe two student projects, which focused on creating innovative concepts for an artistic VR experience. We provide a review of sources of inspiration ranging from standard NPR-techniques through movies, interactive artworks and games to phenomena of human perception. Based on this wide collection of material we describe the prototypes, and discuss observations during implementation and from user feedback. Finally, possible future directions to use the potential of VR as a tool for novel, artful and unconventional experiences are discussed.
            </div>
            
            
        </div>
    </div>
    
    
    <div
        class="paper small"
        id="paperngo2016visual"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperngo2016visual', 'small'); toggleImageSize(imagengo2016visual);"
                title="Click to show details"
            >
                Visual Analysis of Governing Topological Structures in Excitable Network Dynamics
            </h3>  <a class="anchor" name="ngo2016visual"></a>
            <div class="authors">
                <span class="firstAuthor">Quynh Ngo</span>,
                Marc-Thorsten Hütt, Lars Linsen
            </div>
            <div>
                <span class="publication">CGF 2016</span>
                <span class="publication">Full Paper</span>
                
                <a href="https://doi.org/10.1111/cgf.12906" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                To understand how topology shapes the dynamics in excitable networks is one of the fundamental problems in network science when applied to computational systems biology and neuroscience. Recent advances in the field discovered the influential role of two macroscopic topological structures, namely hubs and modules. We propose a visual analytics approach that allows for a systematic exploration of the role of those macroscopic topological structures on the dynamics in excitable networks. Dynamical patterns are discovered using the dynamical features of excitation ratio and co-activation. Our approach is based on the interactive analysis of the correlation of topological and dynamical features using coordinated views. We designed suitable visual encodings for both the topological and the dynamical features. A degree map and an adjacency matrix visualization allow for the interaction with hubs and modules, respectively. A barycentric-coordinates layout and a multi-dimensional scaling approach allow for the analysis of excitation ratio and co-activation, respectively. We demonstrate how the interplay of the visual encodings allows us to quickly reconstruct recent findings in the field within an interactive analysis and even discovered new patterns. We apply our approach to network models of commonly investigated topologies as well as to the structural networks representing the connectomes of different species. We evaluate our approach with domain experts in terms of its intuitiveness, expressiveness, and usefulness.
            </div>
            
            <h4>BibTex</h4>
            <div class="bibtex">
                <textarea>@article{DBLP:journals/cgf/NgoHL16,
  author    = {Quynh Quang Ngo and
               Marc{-}Thorsten H{\"{u}}tt and
               Lars Linsen},
  title     = {Visual Analysis of Governing Topological Structures in Excitable Network
               Dynamics},
  journal   = {Comput. Graph. Forum},
  volume    = {35},
  number    = {3},
  pages     = {301--310},
  year      = {2016},
  url       = {https://doi.org/10.1111/cgf.12906},
  doi       = {10.1111/cgf.12906},
  timestamp = {Fri, 26 May 2017 22:53:54 +0200},
  biburl    = {https://dblp.org/rec/journals/cgf/NgoHL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}</textarea>
            </div>
            
        </div>
    </div>
    
    
    <h2>
        2015
    </h2>
    
    <div
        class="paper small"
        id="paperangerbauer2015utilizing"
    >
        
        <div class="metaData noImage">
            <h3
                onclick="toggleClass('paperangerbauer2015utilizing', 'small'); toggleImageSize(imageangerbauer2015utilizing);"
                title="Click to show details"
            >
                Utilizing the effects of priming to facilitate text comprehension
            </h3>  <a class="anchor" name="angerbauer2015utilizing"></a>
            <div class="authors">
                <span class="firstAuthor">Katrin Angerbauer</span>,
                Tilman Dingler, Dagmar Kern, Albrecht Schmidt
            </div>
            <div>
                <span class="publication">CHI 2015</span>
                <span class="publication">Extended Abstract</span>
                
                <a href="https://dl.acm.org/doi/abs/10.1145/2702613.2732914" target="_blank">website</a>
                
                
            </div>
        </div>
        <div class="info">
            <h4>Abstract</h4>
            <div class="abstract">
                Due to the ever-growing amount of textual information we face in our everyday life, the skill of scanning and absorbing the essence of a piece of text is crucial. We cannot afford to read every text in detail, hence we need to acquire strategies to quickly decide on the importance of a text and how to grasp its content. Additionally, the sheer amount of daily reading makes it hard to remember the gist of every text encountered. Research in psychology has proposed priming as an implicit memory effect where exposure to one stimulus influences the response to a subsequent stimulus. Hence, exposure to contextual information can influence comprehension and recall. In our work we investigate the feasibility of using such an effect to visually present text summaries that are quick to understand and deliver the essence of a text in order to help readers not only make informed decisions about whether to read the text or not, but also to build out more cognitive associations that help to remember the content of the text afterward. In two focus groups we discussed our approach by providing four different visualizations representing the gist and important details of the text. In this paper we introduce the visualizations as well as results of the focus groups.
            </div>
            
            
        </div>
    </div>
    
            </article>
        </div>
    </main>
</body>
</html>